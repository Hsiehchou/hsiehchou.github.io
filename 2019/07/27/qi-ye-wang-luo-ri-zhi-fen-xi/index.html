<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="企业网络日志分析, 谢舟，博客，大数据，Java">
    <meta name="description" content="发布关于大数据,Java的一些信息和自己实践中的经验">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
	<meta name="baidu-site-verification" content="2VN5bX64Cz" />
    <title>企业网络日志分析 | 谢舟的博客</title>
    <link rel="icon" type="image/png" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/favicon.png">

    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/css/matery.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/css/my.css">

    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/jquery/jquery.min.js"></script>
    
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="谢舟的博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
					<div>
						
						<img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/logo.png" class="logo-img" alt="LOGO">
						
						<span class="logo-span">谢舟的博客</span>
					</div>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">谢舟的博客</div>
        <div class="logo-desc">
            
            发布关于大数据,Java的一些信息和自己实践中的经验
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    
<script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = 'fe154608ff5f3a34f6191987f32d750cb7ea2c0960b93c66def4d51b996ed3e7';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">企业网络日志分析</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/">
                                <span class="chip bg-color">大数据项目</span>
                            </a>
                        
                            <a href="/tags/%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/">
                                <span class="chip bg-color">网络日志分析</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/" class="post-category">
                                大数据项目
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-07-27
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    79.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    439 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>

        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h3 id="一、背景数据介绍"><a href="#一、背景数据介绍" class="headerlink" title="一、背景数据介绍"></a>一、背景数据介绍</h3><p><strong>1.    WiFi有哪些数据？</strong><br>手机号<br>机构<br>机构<br>机构<br>网页快照<br>论坛帖子<br>微博<br>邮件<br>IM聊天<br>表单数据<br>APP使用</p>
<p><strong>2.    WiFi价值</strong><br>客户体验：方便客户、基础设施<br>客户数据：精准营销、获取客户上网行为、获取客户信息、客户接触渠道</p>
<p><strong>3.    WiFi数据获取</strong><br>Wi-Fi 网络可以捕获附近智能手机的 IMSI 号码，无线跟踪并监控用户的根源在于智能手机（包括 Android 和 iOS 设备）连接 Wi-Fi 网络的方式。</p>
<p>在大多数现代移动操作系统中有两种广泛实现的协议：<br>可扩展认证协议（EAP）<br>认证和密钥协商（AKA）协议</p>
<p>这些协议允许智能手机通过自身设备的 IMSI 号码切换登录到已知的 Wi-Fi 网络，实现 WiFi 网络自动连接而无需所有者交互。</p>
<p><strong>4.    wifi数据应用</strong><br><img src="/medias/wifi%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8.PNG" alt="wifi数据应用"></p>
<p>画像系统<br><img src="/medias/%E7%94%BB%E5%83%8F%E7%B3%BB%E7%BB%9F.PNG" alt="画像系统"></p>
<p><strong>5.    数据架构</strong><br><img src="/medias/%E7%BD%91%E7%BB%9C%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E5%9B%BE.PNG" alt="网络用户行为数据架构图"></p>
<p><strong>6.    数据结构</strong><br>（1）    <strong>文件命名</strong><br>数据类型_来源_UUID.txt<br>如BASE_SOURCE_UUID.txt</p>
<p>定一套字段标准 ，类型标准<br>（2）    <strong>字段</strong><br>（3）  <strong>通用字段</strong></p>
<table>
<thead>
<tr>
<th align="center">参数1</th>
<th align="center">参数2</th>
<th align="center">参数3</th>
<th align="center">参数4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">imei</td>
<td align="center">imei号，手机唯一识别码</td>
<td align="center"></td>
<td align="center">手机IMEI码由15-17位数字组成</td>
</tr>
<tr>
<td align="center">imsi</td>
<td align="center">IMSI，SIM卡唯一识别码</td>
<td align="center">460011418603055</td>
<td align="center">14-15位数字</td>
</tr>
<tr>
<td align="center">longitude</td>
<td align="center">经度</td>
<td align="center"></td>
<td align="center">精确到小数点6位</td>
</tr>
<tr>
<td align="center">latitude</td>
<td align="center">纬度</td>
<td align="center"></td>
<td align="center">精确到小数点6位</td>
</tr>
<tr>
<td align="center">phone_mac</td>
<td align="center">手机MAC</td>
<td align="center"></td>
<td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围1-9，a-f）</td>
</tr>
<tr>
<td align="center">device_mac</td>
<td align="center">采集设备MAC</td>
<td align="center"></td>
<td align="center">格式需要统一（清洗）aa-aa-aa-aa-aa-aa（范围任意数字加字母）</td>
</tr>
<tr>
<td align="center">device_number</td>
<td align="center">采集设备号</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">collect_time</td>
<td align="center">collect_time</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p><strong>微信数据(wechat)</strong></p>
<table>
<thead>
<tr>
<th align="center">参数1</th>
<th align="center">参数2</th>
<th align="center">参数3</th>
<th align="center">参数4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">username</td>
<td align="center">微信昵称</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">phone</td>
<td align="center">手机号</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">object_username</td>
<td align="center">对方微信号</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">send_message</td>
<td align="center">发送内容（不能破解）</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">accept_message</td>
<td align="center">接收内容（不能破解）</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">message_time</td>
<td align="center">通信时间</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p><strong>邮箱数据(Mail)</strong></p>
<table>
<thead>
<tr>
<th align="center">参数1</th>
<th align="center">参数2</th>
<th align="center">参数3</th>
<th align="center">参数4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">send_mail</td>
<td align="center">发送邮箱</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">send_time</td>
<td align="center">发送时间</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">accept_mail</td>
<td align="center">接收邮箱</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">accept_time</td>
<td align="center">接收时间</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">mail_content</td>
<td align="center">发送内容</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">mail_type</td>
<td align="center">发送还是接收</td>
<td align="center"></td>
<td align="center">send  accept</td>
</tr>
</tbody></table>
<p><strong>搜索数据(Search)</strong></p>
<table>
<thead>
<tr>
<th align="center">参数1</th>
<th align="center">参数2</th>
<th align="center">参数3</th>
<th align="center">参数4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">search_content</td>
<td align="center">搜索内容</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">search_url</td>
<td align="center">搜索URL</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">search_type</td>
<td align="center">搜索引擎</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">search_time</td>
<td align="center">搜索时间</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p><strong>基础数据(Base)</strong></p>
<table>
<thead>
<tr>
<th align="center">参数1</th>
<th align="center">参数2</th>
<th align="center">参数3</th>
<th align="center">参数4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">name</td>
<td align="center">姓名</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">is_marry</td>
<td align="center">是否已婚</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">phone</td>
<td align="center">手机号</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">address</td>
<td align="center">户籍所在地</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">address_new</td>
<td align="center">现在居住地址</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">birthday</td>
<td align="center">出生日期</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">car_number</td>
<td align="center">车牌号</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">idcard</td>
<td align="center">身份证</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p>问题：数据结构，数据字段如何确定？<br>根据实际的需求自己确定。</p>
<h3 id="二．基础架构搭建"><a href="#二．基础架构搭建" class="headerlink" title="二．基础架构搭建"></a>二．基础架构搭建</h3><h4 id="1、创建Maven父项"><a href="#1、创建Maven父项" class="headerlink" title="1、创建Maven父项"></a>1、创建Maven父项</h4><p><strong>总的pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
  &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
  &lt;packaging&gt;pom&lt;/packaging&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;modules&gt;
    &lt;module&gt;xz_bigdata_common&lt;/module&gt;
    &lt;module&gt;xz_bigdata_es&lt;/module&gt;
    &lt;module&gt;xz_bigdata_flume&lt;/module&gt;
    &lt;module&gt;xz_bigdata_hbase&lt;/module&gt;
    &lt;module&gt;xz_bigdata_kafka&lt;/module&gt;
    &lt;module&gt;xz_bigdata_redis&lt;/module&gt;
    &lt;module&gt;xz_bigdata_resources&lt;/module&gt;
    &lt;module&gt;xz_bigdata_spark&lt;/module&gt;
  &lt;/modules&gt;

  &lt;name&gt;xz_bigdata2&lt;/name&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;cdh.version&gt;cdh5.14.0&lt;/cdh.version&gt;
    &lt;junit.version&gt;4.12&lt;/junit.version&gt;
    &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;
    &lt;zookeeper.version&gt;3.4.5&lt;/zookeeper.version&gt;
    &lt;scala.version&gt;2.10.5&lt;/scala.version&gt;
  &lt;/properties&gt;

  &lt;repositories&gt;
    &lt;repository&gt;
      &lt;id&gt;Akka repository&lt;/id&gt;
      &lt;url&gt;https://repo.akka.io/releases&lt;/url&gt;
    &lt;/repository&gt;
    &lt;!--cloudera依赖--&gt;
    &lt;repository&gt;
      &lt;id&gt;cloudera&lt;/id&gt;
      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;/repository&gt;
  &lt;/repositories&gt;

  &lt;!--日志依赖--&gt;
  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
      &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
      &lt;version&gt;${org.slf4j.version}&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.1&lt;/version&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.8&lt;/source&gt;
          &lt;target&gt;1.8&lt;/target&gt;
          &lt;encoding&gt;UTF-8&lt;/encoding&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
&lt;/project&gt;</code></pre><h4 id="2、项目整体结构"><a href="#2、项目整体结构" class="headerlink" title="2、项目整体结构"></a>2、项目整体结构</h4><p><img src="/medias/xz_bigdata2%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata2整体结构"></p>
<h4 id="3、创建子模块"><a href="#3、创建子模块" class="headerlink" title="3、创建子模块"></a>3、创建子模块</h4><p>选中xz_bigdata2，右键选择Module，新建maven子模块，上面图中的那些模块都是这样创建的。<br>注意：开发时使用jdk1.8以上版本，里面使用了jdk1.8特有的内容，低版本开发是报错的，使用jdk1.8方便开发。</p>
<p>ctrl+shift+alt+s：打开Project Structure里面可以进行操作。</p>
<p>ctrl+alt+s：打开Settings，可以配置本地Maven（在Build,Execution,Deployment下面的Build Tools下面的Maven配置自己的本地Maven仓库路径）。</p>
<p>Settings里面还可以看见之前说的Plugins，安装插件，Maven Helper以及后面的Scala插件都可以这里安装。</p>
<h3 id="三、Common开发"><a href="#三、Common开发" class="headerlink" title="三、Common开发"></a>三、Common开发</h3><p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_common&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;ant.version&gt;1.9.1&lt;/ant.version&gt;
        &lt;jaxen.version&gt;1.1.6&lt;/jaxen.version&gt;
        &lt;guava.version&gt;12.0.1&lt;/guava.version&gt;
        &lt;dom4j.version&gt;1.6.1&lt;/dom4j.version&gt;
        &lt;fastjson.version&gt;1.2.5&lt;/fastjson.version&gt;
        &lt;disruptor.version&gt;3.3.6&lt;/disruptor.version&gt;
        &lt;org.slf4j.version&gt;1.7.5&lt;/org.slf4j.version&gt;
        &lt;commons.io.version&gt;2.4&lt;/commons.io.version&gt;
        &lt;httpclient.version&gt;4.2.5&lt;/httpclient.version&gt;
        &lt;commons.exec.version&gt;1.3&lt;/commons.exec.version&gt;
        &lt;commons.lang.version&gt;2.4&lt;/commons.lang.version&gt;
        &lt;commons-vfs2.version&gt;2.1&lt;/commons-vfs2.version&gt;
        &lt;commons.math3.version&gt;3.4.1&lt;/commons.math3.version&gt;
        &lt;commons.logging.version&gt;1.2&lt;/commons.logging.version&gt;
        &lt;commons-httpclient.version&gt;3.1&lt;/commons-httpclient.version&gt;
        &lt;commons.collections4.version&gt;4.1&lt;/commons.collections4.version&gt;
        &lt;commons.configuration.version&gt;1.6&lt;/commons.configuration.version&gt;
        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;
        &lt;commons-dbutils.version&gt;1.6&lt;/commons-dbutils.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;
            &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;
            &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;5.1.46&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;${org.slf4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;${commons.io.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-lang&lt;/groupId&gt;
            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;
            &lt;version&gt;${commons.lang.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;
            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;
            &lt;version&gt;${commons.configuration.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;dom4j&lt;/groupId&gt;
            &lt;artifactId&gt;dom4j&lt;/artifactId&gt;
            &lt;version&gt;${dom4j.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
            &lt;version&gt;${fastjson.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- &lt;dependency&gt;
             &lt;groupId&gt;log4j&lt;/groupId&gt;
             &lt;artifactId&gt;log4j&lt;/artifactId&gt;
             &lt;version&gt;1.2.17&lt;/version&gt;
         &lt;/dependency&gt;--&gt;
    &lt;/dependencies&gt;

&lt;/project&gt;</code></pre><h4 id="1、config-ConfigUtil-java—配置文件读取"><a href="#1、config-ConfigUtil-java—配置文件读取" class="headerlink" title="1、config/ConfigUtil.java—配置文件读取"></a>1、config/ConfigUtil.java—配置文件读取</h4><pre><code>package com.hsiehchou.common.config;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

public class ConfigUtil {

    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);

    private static ConfigUtil configUtil;

    public static ConfigUtil getInstance(){

        if(configUtil == null){
            configUtil = new ConfigUtil();
        }
        return configUtil;
    }

    public Properties getProperties(String path){
        Properties properties = new Properties();
        try {
            LOG.info(&quot;开始加载配置文件&quot; + path);
            //流式读取配置文件
            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);
            properties = new Properties();
            properties.load(insss);
        } catch (IOException e) {
            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);
            LOG.error(null,e);
        }

        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);
        System.out.println(&quot;文件内容：&quot;+properties);
        return properties;
    }

    public static void main(String[] args) {
        ConfigUtil instance = ConfigUtil.getInstance();
        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);
        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);

       // properties.get(&quot;relationfield&quot;);
        System.out.println(properties);
    }
}</code></pre><h4 id="2、config-JsonReader-java"><a href="#2、config-JsonReader-java" class="headerlink" title="2、config/JsonReader.java"></a>2、config/JsonReader.java</h4><pre><code>package com.hsiehchou.common.config;

import org.apache.commons.io.FileUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;

public class JsonReader {
    private static Logger LOG = LoggerFactory.getLogger(JsonReader.class);

    public static String readJson(String json_path){
        JsonReader jsonReader = new JsonReader();
        return jsonReader.getJson(json_path);
    }

    private String getJson(String json_path){
        String jsonStr = &quot;&quot;;
        try {
            String path = getClass().getClassLoader().getResource(json_path).toString();
            path = path.replace(&quot;\\&quot;, &quot;/&quot;);
            if (path.contains(&quot;:&quot;)) {
                path = path.replace(&quot;file:/&quot;,&quot;&quot;);
            }
            jsonStr = FileUtils.readFileToString(new File(path), &quot;UTF-8&quot;);
            LOG.error(&quot;读取json文件{}成功&quot;,path);
        } catch (Exception e) {
            LOG.error(&quot;读取json文件失败&quot;,e);
        }
        return jsonStr;
    }
}</code></pre><h4 id="3、adjuster-Adjuster-java—数据调整接口"><a href="#3、adjuster-Adjuster-java—数据调整接口" class="headerlink" title="3、adjuster/Adjuster.java—数据调整接口"></a>3、adjuster/Adjuster.java—数据调整接口</h4><pre><code>package com.hsiehchou.common.adjuster;

/**
 * 数据调整接口
 */
public interface Adjuster&lt;T, E&gt; {
    E doAdjust(T data);
}</code></pre><h4 id="4、adjuster-StringAdjuster-java"><a href="#4、adjuster-StringAdjuster-java" class="headerlink" title="4、adjuster/StringAdjuster.java"></a>4、adjuster/StringAdjuster.java</h4><pre><code>package com.hsiehchou.common.adjuster;

public abstract class StringAdjuster&lt;E&gt; implements Adjuster&lt;String, E&gt; {}</code></pre><h4 id="5、file-FileCommon-java"><a href="#5、file-FileCommon-java" class="headerlink" title="5、file/FileCommon.java"></a>5、file/FileCommon.java</h4><pre><code>package com.hsiehchou.common.file;

import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.net.URL;
import java.util.List;

public class FileCommon {

    private FileCommon(){}

    /**
     * 判断文件是否存在
     * @param name
     * @return
     */
    public static boolean exist(String name){
        return exist(new File(name));
    }

    public static boolean exist(File file){
        return file.exists();
    }

    /**
     * 创建文件
     * @param file
     * @return
     * @throws IOException
     */
    public static boolean createFile(String file) throws IOException {
        return createFile(new File(file));
    }

    public static boolean createFile(File file) throws IOException {
        if(!file.exists()){
            if(file.isDirectory()){
                return file.mkdirs();
            }else{
                File parentDir = file.getParentFile();
                if(!parentDir.exists()) {
                    if (parentDir.mkdirs()) {
                        return file.createNewFile();
                    }
                }else{
                    return file.createNewFile();
                }
            }
        }
        return true;
    }

    /**
     * 读取文件内容 按行
     * @param file
     * @return
     * @throws IOException
     */
    public static List&lt;String&gt; readLines(String file) throws IOException{
        return readLines(new File(file), &quot;UTF-8&quot;);
    }

    public static List&lt;String&gt; readLines(String file, String encording) throws IOException{
        return readLines(new File(file), encording);
    }

    public static List&lt;String&gt; readLines(File file, String encording) throws IOException {

        List&lt;String&gt; lines = null;
        if(FileCommon.exist(file)) {
            FileInputStream fileInputStream = new FileInputStream(file);
            lines = IOUtils.readLines(fileInputStream, encording);
            fileInputStream.close();
        }
        return lines;
    }

    /**
     * 获取文件前缀
     * @param fileName
     * @return
     */
    public static String getPrefix(String fileName){
        String prefix = fileName;
        int pos = fileName.lastIndexOf(&quot;.&quot;);
        if (pos != -1){
            prefix = fileName.substring(0,pos);
        }
        return prefix;
    }

    /**
     * 获取文件名后缀
     * @param fileName
     * @return
     */
    public static String getFilePostfix(String fileName){
        String filePostfix = fileName.substring(fileName.lastIndexOf(&quot;.&quot;) + 1);
        return filePostfix.toLowerCase();
    }

    /**
     * 删除文件
     * @param filePath
     * @return
     */
    public static boolean delFile(String filePath) {
        boolean flag = false;
        File file = new File(filePath);
        if (file.isFile() &amp;&amp; file.exists()) {
            flag = file.delete();
        }
        return flag;
    }

    /**
     * 移动文件
     * @param oldPath
     * @param newPath
     * @return
     */
    public static boolean mvFile(String oldPath,String newPath){
        boolean flag = false;
        File oldfile = new File(oldPath);
        File newfile = new File(newPath);
        if(oldfile.isFile() &amp;&amp; oldfile.exists()){
            if(newfile.exists()){
                delFile(newfile.getAbsolutePath());
            }
            flag = oldfile.renameTo(newfile);
        }
        return flag;
    }

    /**
     * 删除目录
     * @param dir
     * @return
     */
    public static boolean deleteDir(File dir){
        if (dir.isDirectory()) {
            String[] children = dir.list();
            //递归删除目录中的子目录下
            if(children!=null){
                for (int i=0; i&lt;children.length; i++) {
                    boolean success = deleteDir(new File(dir, children[i]));
                    if (!success) {
                        return false;
                    }
                }
            }
        }
        // 目录此时为空，可以删除
        return dir.delete();
    }

    //递归建立目录，解压缩相关类中使用
    public static void mkdirs(File file) {
        File parent = file.getParentFile();
        if (parent != null &amp;&amp; (!parent.exists())) {
            parent.mkdirs();
        }
    }

    public static String getJarFilePathByClass(String clazz) throws ClassNotFoundException {
        return getJarFilePathByClass(Class.forName(clazz));
    }

    public static String getJarFileDirByClass(String clazz) throws ClassNotFoundException {
        return getJarFileDirByClass(Class.forName(clazz));
    }

    public static String getJarFilePathByClass(Class&lt;?&gt; clazz){
        return new File(clazz.getProtectionDomain().getCodeSource().getLocation().getFile()).getAbsolutePath();
    }

    public static String getJarFileDirByClass(Class&lt;?&gt; clazz){
        return new File(getJarFilePathByClass(clazz)).getParent();
    }

    public static String getAbstractPath(String abstractPath) throws Exception{
        URL url = FileCommon.class.getClassLoader().getResource(abstractPath);
        System.out.println(&quot;配置文件路径为&quot; + url);
        File file = new File(url.getFile());
        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);
        return content;
    }

    public static String getAbstractPath111(String abstractPath) throws Exception{
        File file = new File(abstractPath);
        String content= FileUtils.readFileToString(file,&quot;UTF-8&quot;);
        return content;
    }
}</code></pre><h4 id="6、filter—数据过滤顶层接口"><a href="#6、filter—数据过滤顶层接口" class="headerlink" title="6、filter—数据过滤顶层接口"></a>6、filter—数据过滤顶层接口</h4><pre><code>package com.hsiehchou.common.filter;

/**
 * 数据过滤顶层接口
 */
public interface Filter&lt;T&gt; {
    boolean filter(T obj);
}</code></pre><h4 id="7、net-HttpRequest-java"><a href="#7、net-HttpRequest-java" class="headerlink" title="7、net/HttpRequest.java"></a>7、net/HttpRequest.java</h4><pre><code>package com.hsiehchou.common.net;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLConnection;
import java.net.URLEncoder;
import java.util.Map;

public class HttpRequest {
    private static final Logger LOG = LoggerFactory.getLogger(HttpRequest.class);

    /**
     * 向指定URL发送GET方法的请求
     * @param url  发送请求的URL
     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。
     * @return URL  所代表远程资源的响应结果
     */
    public static String sendGet(String url, String param) {
        String result = &quot;&quot;;
        BufferedReader in = null;
        try {
            String urlNameString = url + &quot;?&quot; + param;
            URL realUrl = new URL(urlNameString);
            // 打开和URL之间的连接
            URLConnection connection = realUrl.openConnection();
            // 设置通用的请求属性
            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);
            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);
            connection.setRequestProperty(&quot;user-agent&quot;,
                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);
            // 建立实际的连接
            connection.connect();
            // 获取所有响应头字段
            //Map&lt;String, List&lt;String&gt;&gt; map = connection.getHeaderFields();
            // 遍历所有的响应头字段
            // 定义 BufferedReader输入流来读取URL的响应
            in = new BufferedReader(new InputStreamReader(connection.getInputStream(),&quot;UTF-8&quot;));
            String line;
            while ((line = in.readLine()) != null) {
                result += line;
            }
        } catch (Exception e) {
            LOG.info(&quot;发送GET请求出现异常！&quot; + (url+param));
            System.out.println(&quot;发送GET请求出现异常！&quot; + e);
            e.printStackTrace();
        }
        // 使用finally块来关闭输入流
        finally {
            try {
                if (in != null) {
                    in.close();
                }
            } catch (Exception e2) {
                e2.printStackTrace();
            }
        }
        return result;
    }

    /**
     * 向指定URL发送GET方法的请求
     * @param url  发送请求的URL
     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。
     * @return URL 所代表远程资源的响应结果
     */
    public static String sendGet(String url, String param,String authorization) {
        String result = &quot;&quot;;
        BufferedReader in = null;
        try {
            String urlNameString = url + &quot;?&quot; + param;
            URL realUrl = new URL(urlNameString);
            // 打开和URL之间的连接
            URLConnection connection = realUrl.openConnection();
            // 设置通用的请求属性
            connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);
            connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);
            connection.setRequestProperty(&quot;Authorization&quot;, authorization);
            connection.setRequestProperty(&quot;user-agent&quot;,
                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);

            // 建立实际的连接
            connection.connect();
            // 获取所有响应头字段
            connection.getHeaderFields();
            // 遍历所有的响应头字段
/*            for (String key : map.keySet()) {
                System.out.println(key + &quot;---&gt;&quot; + map.get(key));
            }*/
            // 定义 BufferedReader输入流来读取URL的响应
            in = new BufferedReader(new InputStreamReader(
                    connection.getInputStream(),&quot;UTF-8&quot;));
            String line;
            while ((line = in.readLine()) != null) {
                result += line;
            }
        } catch (Exception e) {
            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param));
            System.out.println(&quot;发送POST请求出现异常！&quot; + e);
            e.printStackTrace();
        }
        // 使用finally块来关闭输入流
        finally {
            try {
                if (in != null) {
                    in.close();
                }
            } catch (Exception e2) {
                e2.printStackTrace();
            }
        }
        return result;
    }

    public static void main(String[] args) throws Exception{

    }

    /**
     * 向指定 URL 发送POST方法的请求
     * @param url  发送请求的 URL
     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。
     * @return  所代表远程资源的响应结果
     */
    public static String sendPost(String url, String param) {
        PrintWriter out = null;
        BufferedReader in = null;
        String result = &quot;&quot;;
        try {
            URL realUrl = new URL(url);
            // 打开和URL之间的连接
            URLConnection conn = realUrl.openConnection();
            // 设置通用的请求属性
            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);
            //conn.setInstanceFollowRedirects(false);
            // conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/x-www-form-urlencoded&quot;);
            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);
            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);
            conn.setRequestProperty(&quot;user-agent&quot;,
                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);
            // 发送POST请求必须设置如下两行
            conn.setReadTimeout(30000);
            conn.setDoOutput(true);
            conn.setDoInput(true);
            // 获取URLConnection对象对应的输出流
            out = new PrintWriter(conn.getOutputStream());
            // 发送请求参数
            out.print(param);
            // flush输出流的缓冲
            out.flush();
            // 定义BufferedReader输入流来读取URL的响应

            InputStream inputStream = conn.getInputStream();
            in = new BufferedReader(new InputStreamReader(inputStream,&quot;UTF-8&quot;));
            String line;
            while ((line = in.readLine()) != null) {
                result += line;
            }
        }
        catch (IOException e) {
            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);
        }
        //使用finally块来关闭输出流、输入流
        finally{
            try{
                if(out!=null){
                    out.close();
                }
                if(in!=null){
                    in.close();
                }
            }
            catch(IOException ex){
                ex.printStackTrace();
            }
        }
        return result;
    }

    /*
     * params 填写的URL的参数 encode 字节编码
     */
    public static String sendPostMessage(String url1,Map&lt;String,Object&gt; params){
        String response = null;
        Reader in = null;
        try {
            //访问准备
            URL url = new URL(url1);
            //开始访问
            StringBuilder postData = new StringBuilder();
            for (Map.Entry&lt;String,Object&gt; param : params.entrySet()) {
                if (postData.length() != 0) postData.append(&#39;&amp;&#39;);
                postData.append(URLEncoder.encode(param.getKey(), &quot;UTF-8&quot;));
                postData.append(&#39;=&#39;);
                postData.append(URLEncoder.encode(String.valueOf(param.getValue()), &quot;UTF-8&quot;));
            }
            byte[] postDataBytes = postData.toString().getBytes(&quot;UTF-8&quot;);
            URLConnection conn = url.openConnection();
            //URLConnection conn = url.openConnection();
            //conn.setRequestMethod(&quot;POST&quot;);
            //conn.setInstanceFollowRedirects(false);
            //conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/x-www-form-urlencoded&quot;);
            conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/json&quot;);
            conn.setRequestProperty(&quot;Content-Length&quot;, String.valueOf(postDataBytes.length));
            conn.setDoOutput(true);
            conn.getOutputStream().write(postDataBytes);

            in = new BufferedReader(new InputStreamReader(conn.getInputStream(), &quot;UTF-8&quot;));

            StringBuilder sb = new StringBuilder();
            for (int c; (c = in.read()) &gt;= 0;)
                sb.append((char)c);
            response = sb.toString();
           //System.out.println(response);
        } catch (IOException e) {
            LOG.error(null,e);
        }finally {
            if(in != null){
                try {
                    in.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
        return response;
    }

    /**
     * 向指定 URL 发送POST方法的请求
     * @param url  发送请求的 URL
     * @param param  请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。
     * @return  所代表远程资源的响应结果
     */
    public static void sendPostWithoutReturn(String url, String param) {
        PrintWriter out = null;
        BufferedReader in = null;
        String result = &quot;&quot;;
        try {
            URL realUrl = new URL(url);
            // 打开和URL之间的连接
            HttpURLConnection conn = (HttpURLConnection )realUrl.openConnection();
            // 设置通用的请求属性
            conn.setRequestProperty(&quot;Content-Type&quot;,&quot;application/json&quot;);
            conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;);
            conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;);
            conn.setRequestProperty(&quot;user-agent&quot;,
                    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;);

            //根据需求设置读超时的时间
            conn.setReadTimeout(1000);
            // 发送POST请求必须设置如下两行
            conn.setDoOutput(true);
            conn.setDoInput(true);
            // 获取URLConnection对象对应的输出流
            out = new PrintWriter(conn.getOutputStream());
            // 发送请求参数
            out.print(param);
            // flush输出流的缓冲
            out.flush();
            // 定义BufferedReader输入流来读取URL的响应
            if (conn.getResponseCode() == 200) {
                System.out.println(&quot;连接成功,传送数据...&quot;);
            } else {
                System.out.println(&quot;连接失败,错误代码:&quot;+conn.getResponseCode());
            }
        }
        catch (IOException e) {
            LOG.info(&quot;发送POST请求出现异常！&quot; + (url+param),e);
        }
        //使用finally块来关闭输出流、输入流
        finally{
            try{
                if(out!=null){
                    out.close();
                }
                in.close();

            }
            catch(Exception ex){
                ex.printStackTrace();
            }
        }
    }
}</code></pre><h4 id="8、netb-db-DBCommon—mysql的连接、关闭基础类"><a href="#8、netb-db-DBCommon—mysql的连接、关闭基础类" class="headerlink" title="8、netb/db/DBCommon—mysql的连接、关闭基础类"></a>8、netb/db/DBCommon—mysql的连接、关闭基础类</h4><pre><code>package com.hsiehchou.common.netb.db;

import com.hsiehchou.common.config.ConfigUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.*;
import java.util.Properties;

public class DBCommon {

    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);
    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;
    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);

    private static Connection conn ;
    private DBCommon(){}

    public static void main(String[] args) {
        System.out.println(properties);
        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);
        System.out.println(xz_bigdata);
    }

    //TODO  配置文件
    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;
    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);
    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);
    private static final String IP = properties.getProperty(&quot;db_ip&quot;);
    private static final String PORT = properties.getProperty(&quot;db_port&quot;);
    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;

    static {
        try {
            Class.forName(JDBC_DRIVER);
        } catch (ClassNotFoundException e) {
            LOG.error(null, e);
        }
    }

    /**
     * 获取数据库连接
     * @param dbName
     * @return
     */
    public static Connection getConn(String dbName) {
        Connection conn = null;
        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;
        try {
            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);
        } catch (SQLException e) {
            e.printStackTrace();
            LOG.error(null, e);
        }
        return conn;
    }

    /**
     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;
     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;
     * @param user eg:&quot;ucase&quot;
     * @param password eg:&quot;ucase123&quot;
     * @return
     * @throws ClassNotFoundException
     * @throws SQLException
     */
    public static Connection getConn(String url, String driver, String user,
                                     String password) throws ClassNotFoundException, SQLException{
        Class.forName(driver);
        conn = DriverManager.getConnection(url, user, password);
        return  conn;
    }

    public static void close(Connection conn){
        try {
            if( conn != null ){
                conn.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Statement statement){
        try {
            if( statement != null ){
                statement.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Connection conn,PreparedStatement statement){
        try {
            if( conn != null ){
                conn.close();
            }
            if( statement != null ){
                statement.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{

        if( resultSet != null ){
            resultSet.close();
        }
        if( statement != null ){
            statement.close();
        }
        if( conn != null ){
            conn.close();
        }
    }
}</code></pre><h4 id="9、project-datatype-DataTypeProperties-java"><a href="#9、project-datatype-DataTypeProperties-java" class="headerlink" title="9、project/datatype/DataTypeProperties.java"></a>9、project/datatype/DataTypeProperties.java</h4><pre><code>package com.hsiehchou.common.project.datatype;

import com.hsiehchou.common.config.ConfigUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;

public class DataTypeProperties {
    private static final Logger logger = LoggerFactory.getLogger(DataTypeProperties.class);

    private static final String DATA_PATH = &quot;common/datatype.properties&quot;;

    public static Map&lt;String,ArrayList&lt;String&gt;&gt; dataTypeMap = null;

    static {
        Properties properties = ConfigUtil.getInstance().getProperties(DATA_PATH);
        dataTypeMap = new HashMap&lt;&gt;();
        Set&lt;Object&gt; keys = properties.keySet();
        keys.forEach(key-&gt;{
            String[] split = properties.getProperty(key.toString()).split(&quot;,&quot;);
            dataTypeMap.put(key.toString(),new ArrayList&lt;&gt;(Arrays.asList(split)));
        });
    }

    public static void main(String[] args) {
        Map&lt;String, ArrayList&lt;String&gt;&gt; dataTypeMap = DataTypeProperties.dataTypeMap;
        System.out.println(dataTypeMap.toString());
    }
}</code></pre><h4 id="10、regex-Validation-java—验证工具类"><a href="#10、regex-Validation-java—验证工具类" class="headerlink" title="10、regex/Validation.java—验证工具类"></a>10、regex/Validation.java—验证工具类</h4><pre><code>package com.hsiehchou.common.regex;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * 验证工具类
 */
public class Validation {
    // ------------------常量定义
    /**
     * Email正则表达式=
     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;
     * ;
     */
    // public static final String EMAIL =
    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;
    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;

    /**
     * 电话号码正则表达式=
     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|
     * (^0?1[35]\d{9}$)
     */
    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;

    /**
     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$
     */
    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;

    /**
     * Integer正则表达式 ^-?(([1-9]\d*$)|0)
     */
    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;

    /**
     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$
     */
    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;

    /**
     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$
     */
    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;

    /**
     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$
     */
    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;

    /**
     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　
     */
    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;

    /**
     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$
     */
    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;

    /**
     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁
     */
    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;

    /**
     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编
     */
    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;

    /**
     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$
     */
    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;

    /**
     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$
     */
    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;

    /**
     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$
     */
    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;

    /**
     * 过滤特殊字符串正则 regEx=
     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;
     */
    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;

    /***
     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式
     */
    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;
            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;
            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;
            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;
            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;
            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;
            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;
            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;
            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;

    /***
     * 日期正则 支持： YYYY-MM-DD
     */
    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;


    /**
     * URL正则表达式 匹配 http www ftp
     */
    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;
            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;
            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;
            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;

    /**
     * 身份证正则表达式
     */
    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;
            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;
            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;

    /**
     * 机构代码
     */
    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;

    /**
     * 匹配数字组成的字符串 ^[0-9]+$
     */
    public static final String STR_NUM = &quot;^[0-9]+$&quot;;

    // //------------------验证方法
    /**
     * 判断字段是否为空 符合返回ture
     * @param str
     * @return boolean
     */
    public static synchronized boolean StrisNull(String str) {
        return null == str || str.trim().length() &lt;= 0 ? true : false;
    }

    /**
     * 判断字段是非空 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean StrNotNull(String str) {
        return !StrisNull(str);
    }

    /**
     * 字符串null转空
     * @param str
     * @return boolean
     */
    public static String nulltoStr(String str) {
        return StrisNull(str) ? &quot;&quot; : str;
    }

    /**
     * 字符串null赋值默认值
     * @param str  目标字符串
     * @param defaut  默认值
     * @return  String
     */
    public static String nulltoStr(String str, String defaut) {
        return StrisNull(str) ? defaut : str;
    }

    /**
     * 判断字段是否为Email 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isEmail(String str) {
        return Regular(str, EMAIL);
    }

    /**
     * 判断是否为电话号码 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isPhone(String str) {
        return Regular(str, PHONE);
    }

    /**
     * 判断是否为手机号码 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isMobile(String str) {
        return RegularSJHM(str, MOBILE);
    }

    /**
     * 判断是否为Url 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isUrl(String str) {
        return Regular(str, URL);
    }

    /**
     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isNumber(String str) {
        return Regular(str, DOUBLE);
    }

    /**
     * 判断字段是否为INTEGER 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isInteger(String str) {
        return Regular(str, INTEGER);
    }

    /**
     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isINTEGER_NEGATIVE(String str) {
        return Regular(str, INTEGER_NEGATIVE);
    }

    /**
     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isINTEGER_POSITIVE(String str) {
        return Regular(str, INTEGER_POSITIVE);
    }

    /**
     * 判断字段是否为DOUBLE 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDouble(String str) {
        return Regular(str, DOUBLE);
    }

    /**
     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDOUBLE_NEGATIVE(String str) {
        return Regular(str, DOUBLE_NEGATIVE);
    }

    /**
     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDOUBLE_POSITIVE(String str) {
        return Regular(str, DOUBLE_POSITIVE);
    }

    /**
     * 判断字段是否为日期 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDate(String str) {
        return Regular(str, DATE_ALL);
    }

    /**
     * 验证2010-12-10
     * @param str
     * @return
     */
    public static boolean isDate1(String str) {
        return Regular(str, DATE_FORMAT1);
    }

    /**
     * 判断字段是否为年龄 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isAge(String str) {
        return Regular(str, AGE);
    }

    /**
     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false
     * @param str
     * @param leng
     * @return boolean
     */
    public static boolean isLengOut(String str, int leng) {
        return StrisNull(str) ? false : str.trim().length() &gt; leng;
    }

    /**
     * 判断字段是否为身份证 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isIdCard(String str) {
        if (StrisNull(str))
            return false;
        if (str.trim().length() == 15 || str.trim().length() == 18) {
            return Regular(str, IDCARD);
        } else {
            return false;
        }
    }

    /**
     * 判断字段是否为邮编 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isCode(String str) {
        return Regular(str, CODE);
    }

    /**
     * 判断字符串是不是全部是英文字母
     * @param str
     * @return boolean
     */
    public static boolean isEnglish(String str) {
        return Regular(str, STR_ENG);
    }

    /**
     * 判断字符串是不是全部是英文字母+数字
     * @param str
     * @return boolean
     */
    public static boolean isENG_NUM(String str) {
        return Regular(str, STR_ENG_NUM);
    }

    /**
     * 判断字符串是不是全部是英文字母+数字+下划线
     * @param str
     * @return boolean
     */
    public static boolean isENG_NUM_(String str) {
        return Regular(str, STR_ENG_NUM_);
    }

    /**
     * 过滤特殊字符串 返回过滤后的字符串
     * @param str
     * @return boolean
     */
    public static String filterStr(String str) {
        Pattern p = Pattern.compile(STR_SPECIAL);
        Matcher m = p.matcher(str);
        return m.replaceAll(&quot;&quot;).trim();
    }

    /**
     * 校验机构代码格式
     * @return
     */
    public static boolean isJigouCode(String str) {
        return Regular(str, JIGOU_CODE);
    }

    /**
     * 判断字符串是不是数字组成
     * @param str
     * @return boolean
     */
    public static boolean isSTR_NUM(String str) {
        return Regular(str, STR_NUM);
    }

    /**
     * 匹配是否符合正则表达式pattern 匹配返回true
     * @param str 匹配的字符串
     * @param pattern 匹配模式
     * @return boolean
     */
    private static boolean Regular(String str, String pattern) {
        if (null == str || str.trim().length() &lt;= 0)
            return false;
        Pattern p = Pattern.compile(pattern);
        Matcher m = p.matcher(str);
        return m.matches();
    }

    /**
     * 匹配是否符合正则表达式pattern 匹配返回true
     * @param str 匹配的字符串
     * @param pattern 匹配模式
     * @return boolean
     */
    private static boolean RegularSJHM(String str, String pattern) {
        if (null == str || str.trim().length() &lt;= 0){
            return false;
        }
        if(str.contains(&quot;+86&quot;)){
            str=str.replace(&quot;+86&quot;,&quot;&quot;);
        }
        Pattern p = Pattern.compile(pattern);
        Matcher m = p.matcher(str);
        return m.matches();
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean 2016-7-19 下午5:13:25 by 
     */
    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;

    public static boolean isyyyyMMddHHmmss(String time) {
        if (time == null) {
            return false;
        }
        boolean bool = time.matches(yyyyMMddHHmmss);
        return bool;
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean 2016-7-19 下午5:13:25 by 
     */
    public static final String isMac = &quot;^[A-F0-9]{2}(-[A-F0-9]{2}){5}$&quot;;

    public static boolean isMac(String mac) {
        if (mac == null) {
            return false;
        }
        boolean bool = mac.matches(isMac);
        return bool;
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean 2016-7-19 下午5:13:25 by 
     */
    public static final String longtime = &quot;[0-9]{10}&quot;;

    public static boolean isTimestamp(String timestamp) {
        if (timestamp == null) {
            return false;
        }
        boolean bool = timestamp.matches(longtime);
        return bool;
    }

    /**
     * 判断字段是否为datatype 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String DATATYPE = &quot;^\\d{7}$&quot;;
    public static boolean isDATATYPE(String str) {
        return Regular(str, DATATYPE);
    }


    /**
     * 判断字段是否为QQ 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String QQ = &quot;^\\d{5,15}$&quot;;
    public static boolean isQQ(String str) {
        return Regular(str, QQ);
    }


    /**
     * 判断字段是否为IMSI 符合返回ture
     * @param str
     * @return boolean
     */
    //public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;
    public static final String IMSI = &quot;^[1-9][0-9][0-9]0[0,1,2,3,4,5,6,7,9]\\d{10}|[1-9][0-9][0-9](11|20)\\d{10}$&quot;;
    public static boolean isIMSI(String str) {
        return Regular(str, IMSI);
    }

    /**
     * 判断字段是否为IMEI 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;
    public static boolean isIMEI(String str) {return Regular(str, IMEI);}

    /**
     * 判断字段是否为CAPTURETIME 符合返回ture
     * @param str
     * @return boolean
     */


    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;
    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}

    /**
     * description:检测认证类型
     * @param auth
     * @return boolean
     */
    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;
    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}

    /**
     * description:检测FIRM_CODE
     * @param auth
     * @return boolean
     */
    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;
    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}

    /**
     * description:检测经度
     * @param auth
     * @return boolean
     */
    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,8})?$&quot;;

    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;
    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}

    /**
     * description:检测纬度
     * @param auth
     * @return boolean
     */
    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,8})?$&quot;;
    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}

    public static void main(String[] args) {
        boolean bool = isLATITUDE(&quot;26.0615854&quot;);
        System.out.println(bool);
    }
}</code></pre><h4 id="11、thread-ThreadPoolManager-java—线程池管理器单例"><a href="#11、thread-ThreadPoolManager-java—线程池管理器单例" class="headerlink" title="11、thread/ThreadPoolManager.java—线程池管理器单例"></a>11、thread/ThreadPoolManager.java—线程池管理器单例</h4><pre><code>package com.hsiehchou.common.thread;

import java.io.Serializable;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 *     线程池管理器单例
 *  默认创建   ewCachedThreadPool ：创建一个可缓存的线程池
 *  可通过指定线程的数量来创建：newFixedThreadPool  ： 创建固定大小的线程池
 */
public class ThreadPoolManager implements Serializable {

    private static final long serialVersionUID = 1465361469484903956L;
    public static final ThreadPoolManager threadPoolManager =  new ThreadPoolManager();

    private static ThreadPoolManager tpm;

    private transient ExecutorService newCachedThreadPool;
    private transient ExecutorService newFixedThreadPool;

    private int poolCapacity;

    private ThreadPoolManager(){
        if( newCachedThreadPool == null )
            newCachedThreadPool = Executors.newCachedThreadPool();
    }

    @Deprecated
    public static ThreadPoolManager getInstance(){
        if( tpm == null ){
            synchronized(ThreadPoolManager.class){
            if( tpm == null )
                tpm =  new ThreadPoolManager();
            }
        }
        return tpm;
    }

    /**
      * 返回 newCachedThreadPool
     */
    public ExecutorService getExecutorService(){
        if( newCachedThreadPool == null ){
            synchronized(ThreadPoolManager.class){
                if( newCachedThreadPool == null )
                    newCachedThreadPool = Executors.newCachedThreadPool();
            }
        }
        return newCachedThreadPool;
    }

    /** 
      * 返回 newFixedThreadPool
     */
    public ExecutorService getExecutorService(int poolCapacity){
        return getExecutorService(poolCapacity, false);
    }

    /**
      * 返回 newFixedThreadPool
     */
    public synchronized ExecutorService getExecutorService(int poolCapacity, boolean closeOld){
        if(newFixedThreadPool == null || (this.poolCapacity != poolCapacity)){
            if(newFixedThreadPool != null &amp;&amp; closeOld){
                newFixedThreadPool.shutdown();
            }
            newFixedThreadPool = Executors.newFixedThreadPool(poolCapacity);
            this.poolCapacity = poolCapacity;
        }
        return newFixedThreadPool;
    }
}</code></pre><h4 id="12、time-TimeTranstationUtils-java—时间转换工具类"><a href="#12、time-TimeTranstationUtils-java—时间转换工具类" class="headerlink" title="12、time/TimeTranstationUtils.java—时间转换工具类"></a>12、time/TimeTranstationUtils.java—时间转换工具类</h4><pre><code>package com.hsiehchou.common.time;

import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;

/**
 * Description: 时间转换工具类
 */
public class TimeTranstationUtils {

    private static final Logger logger = LoggerFactory.getLogger(TimeTranstationUtils.class);

/*    private static SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
    private static SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);
    private static SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);
    private static SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
    private static SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);*/

    private static Date nowTime;

    public static String Date2yyyyMMddHHmmss() {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        nowTime = new Date(System.currentTimeMillis());
        String time = sdFormatter.format(nowTime);
        return time;
    }

    public static String Date2yyyyMMddHHmmss(long timestamp) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        nowTime = new Date(timestamp);
        String time = sdFormatter.format(nowTime);
        return time;
    }

    public static String Date2yyyyMMdd(long timestamp) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMdd&quot;);
        nowTime = new Date(timestamp);
        String time = sdFormatter.format(nowTime);
        return time;
    }

    public static String Date2yyyyMMddHH(String str) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        SimpleDateFormat sdFormatternew = new SimpleDateFormat(&quot;yyyyMMddHH&quot;);
        try {
            nowTime = sdFormatter.parse(str);
        } catch (ParseException e) {
            e.printStackTrace();
        }
        String time = sdFormatternew.format(nowTime);
        return time;
    }

    public static String Date2yyyy_MM_dd() {
        SimpleDateFormat sdFormatter1 = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);
        nowTime = new Date(System.currentTimeMillis());
        String time = sdFormatter1.format(nowTime);
        return time;
    }

    public static String Date2yyyy_MM_dd_HH_mm_ss() {
        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
        nowTime = new Date(System.currentTimeMillis());
        String time = sdFormatter2.format(nowTime);
        return time;
    }

    public static String Date2yyyyMMdd() {
        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);
        nowTime = new Date(System.currentTimeMillis());
        String time = sdFormatter3.format(nowTime);
        return time;
    }

    public static String Date2yyyyMMdd(String str) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        SimpleDateFormat sdFormatter3 = new SimpleDateFormat(&quot;yyyyMMdd&quot;);
        try {
            nowTime = sdFormatter.parse(str);
        } catch (ParseException e) {
            e.printStackTrace();
        }
        String time = sdFormatter3.format(nowTime);
        return time;
    }

    public static Long Date2yyyyMMddHHmmssToLong() {
        return System.currentTimeMillis() / 1000;
    }

    public static String long2date(String capturetime){
        SimpleDateFormat sdf= new SimpleDateFormat(&quot;yyyyMMdd&quot;);
        //前面的lSysTime是秒数，先乘1000得到毫秒数，再转为java.util.Date类型
        Date dt = new Date(Long.valueOf(capturetime) * 1000);
        String sDateTime = sdf.format(dt);  //得到精确到秒的表示：08/31/2006 21:08:00
        return sDateTime;
    }

    public static Long yyyyMMddHHmmssToLong(String time) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        if (StringUtils.isBlank(time)) {
            return 0L;
        } else {
            boolean isNum = time.matches(&quot;[0-9]+&quot;);
            if (isNum) {
                long long1 = 0;
                try {
                    long1 = sdFormatter.parse(time).getTime();
                } catch (ParseException e) {
                    logger.error(time + &quot;时间转换为long错误&quot; + isNum);
                    return 0L;
                }
                return long1 / 1000;
            }
        }
        return 0L;
    }

    public static Date yyyyMMddHHmmssToDate(String time) {
        SimpleDateFormat sdFormatter = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;);
        if (StringUtils.isBlank(time)) {
            return new Date();
        } else {
            boolean isNum = time.matches(&quot;[0-9]+&quot;);
            if (isNum) {
                Date date = null;
                try {
                    date = sdFormatter.parse(time);
                } catch (ParseException e) {
                    logger.error(time + &quot;时间转换为date错误&quot; + isNum, e);
                    System.out.println(time);
                    System.out.println(isNum);
                    e.printStackTrace();
                }
                return date;
            }
        }
        return new Date();
    }

    public static Date yyyyMMddHHmmssToDate() {
        Date date = null;
        SimpleDateFormat sdFormatter2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
        try {
            date = sdFormatter2.parse(Date2yyyy_MM_dd_HH_mm_ss());
        } catch (ParseException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        return date;
    }

    public static java.sql.Date strToDate(String strDate) {
        String str = strDate;
        SimpleDateFormat format = new SimpleDateFormat(&quot;yyyy-mm-dd&quot;);
        Date d = null;
        try {
            d = format.parse(str);
        } catch (Exception e) {
            e.printStackTrace();
        }
        java.sql.Date date = new java.sql.Date(d.getTime());
        return date;
    }

    public static Long str2Long(String str){
        if(!StringUtils.isBlank(str)){
            return Long.valueOf(str);
        }else{
            return 0L;
        }
    }

    public static Double str2Double(String str){
        if(!StringUtils.isBlank(str)){
            return Double.valueOf(str);
        }else{
            return 0.0;
        }
    }

    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {
        String logouttime = map.get(key);
        if (!StringUtils.isBlank(logouttime)) {
            objectMap.put(key, Long.valueOf(logouttime));
        } else {
            objectMap.put(key, 0L);
        }
        return objectMap;
    }

    public static void main(String[] args) throws InterruptedException {
        System.out.println(long2date(&quot;1463487992&quot;));
    }
}</code></pre><h3 id="四、Resources开发"><a href="#四、Resources开发" class="headerlink" title="四、Resources开发"></a>四、Resources开发</h3><h4 id="xz-bigdata-resources结构"><a href="#xz-bigdata-resources结构" class="headerlink" title="xz_bigdata_resources结构"></a>xz_bigdata_resources结构</h4><p><img src="/medias/xz_bigdata_resources%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_resources整体结构"></p>
<p>注意：这里的resources要选中右键，选择Make Directory as，选择下级的Resources Root，变成Resources配置源文件，项目可以任意调用。</p>
<h4 id="1、resources下面"><a href="#1、resources下面" class="headerlink" title="1、resources下面"></a>1、resources下面</h4><p><strong>log4j2.properties</strong></p>
<pre><code>log4j.rootLogger = error,stdout,D,E

log4j.appender.stdout = org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target = System.out
log4j.appender.stdout.layout = org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%n

log4j.appender.D = org.apache.log4j.DailyRollingFileAppender
log4j.appender.D.File = F://logs/log.log
log4j.appender.D.Append = true
log4j.appender.D.Threshold = DEBUG 
log4j.appender.D.layout = org.apache.log4j.PatternLayout
log4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n

log4j.appender.E = org.apache.log4j.DailyRollingFileAppender
log4j.appender.E.File =F://logs/error.log 
log4j.appender.E.Append = true
log4j.appender.E.Threshold = ERROR 
log4j.appender.E.layout = org.apache.log4j.PatternLayout
log4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="2、common"><a href="#2、common" class="headerlink" title="2、common"></a>2、common</h4><p><strong>datatype.properties</strong></p>
<pre><code># base = datatype,idcard,name,age,collecttime,imei

# wechat = datatype,wechat,phone,collecttime,imei

wechat = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time

mail = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,send_mail,send_time,accept_mail,accept_time,mail_content,mail_type

qq = imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time</code></pre><p><strong>mysql.properties</strong></p>
<pre><code>db_ip = 192.168.116.201
db_port = 3306
user = root
password = root</code></pre><h4 id="3、es"><a href="#3、es" class="headerlink" title="3、es"></a>3、es</h4><p><strong>es_cluster.properties</strong></p>
<pre><code>es.cluster.name=xz_es
es.cluster.nodes = hadoop1,hadoop2,hadoop3
es.cluster.nodes1 = hadoop1
es.cluster.nodes2 = hadoop2
es.cluster.nodes3 = hadoop3

es.cluster.tcp.port = 9300
es.cluster.http.port = 9200</code></pre><p><strong>mapping/base.json</strong></p>
<pre><code>{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;datatype&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;idcard&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;age&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;collecttime&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;}
  }
}</code></pre><p><strong>mapping/fieldmapping.properties</strong></p>
<pre><code>tables = wechat,mail,qq

wechat.imei = string
wechat.imsi = string
wechat.longitude = double
wechat.latitude = double
wechat.phone_mac = string
wechat.device_mac = string
wechat.device_number = string
wechat.collect_time = long
wechat.username = string
wechat.phone = string
wechat.object_username = string
wechat.send_message = string
wechat.accept_message = string
wechat.message_time = long
wechat.id = string
wechat.table = string
wechat.filename = string
wechat.absolute_filename  = string


mail.imei = string
mail.imsi = string
mail.longitude = double
mail.latitude = double
mail.phone_mac = string
mail.device_mac = string
mail.device_number = string
mail.collect_time = long
mail.send_mail = string
mail.send_time = long
mail.accept_mail = string
mail.accept_time = long
mail.mail_content = string
mail.mail_type = string
mail.id = string
mail.table = string
mail.filename = string
mail.absolute_filename  = string

qq.imei = string
qq.imsi = string
qq.longitude = double
qq.latitude = double
qq.phone_mac = string
qq.device_mac = string
qq.device_number = string
qq.collect_time = long
qq.username = string
qq.phone = string
qq.object_username = string
qq.send_message = string
qq.accept_message = string
qq.message_time = long
qq.id = string
qq.table = string
qq.filename = string
qq.absolute_filename  = string</code></pre><p><strong>mapping/mail.json</strong></p>
<pre><code>{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},
     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}</code></pre><p><strong>mapping/qq.json</strong></p>
<pre><code>{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}</code></pre><p><strong>mapping/test.json</strong></p>
<pre><code>{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;source&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;target&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;library_id&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;source_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;target_sign&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;create_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;create_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;is_audit&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;is_del&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;last_modify_user_id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;last_modify_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;init_version&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;version&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;score&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;level&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;example&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;conflict&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;srcLangId&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;srcLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;tarLangId&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;tarLangCN&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;docId&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;source_simhash&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;sentence_id&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;section_id&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;type&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;industry&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;industry_name&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;querycount&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;reviser&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;comment&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}</code></pre><p><strong>mapping/wechat.json</strong></p>
<pre><code>{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}</code></pre><h4 id="4、flume"><a href="#4、flume" class="headerlink" title="4、flume"></a>4、flume</h4><p><strong>datatype.properties</strong></p>
<p><strong>flume-config.properties</strong></p>
<pre><code>#kafka topic
kafkatopic=test100</code></pre><p><strong>validation.properties</strong></p>
<pre><code># 文件名验证开关
FILENAME_VALIDATION=1

# DATATYPE转换开关
DATATYPE_TRANSACTION=1

# 经纬度验证开关
LONGLAIT_VALIDATION=1

# 是否入错误数据到ES
ERROR_ES=1</code></pre><h4 id="5、hadoop"><a href="#5、hadoop" class="headerlink" title="5、hadoop"></a>5、hadoop</h4><p><strong>core-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.trash.interval&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;io.compression.codecs&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;
    &lt;value&gt;simple&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;
    &lt;value&gt;authentication&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;
    &lt;value&gt;DEFAULT&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;net.topology.script.file.name&lt;/name&gt;
    &lt;value&gt;/etc/hadoop/conf.cloudera.yarn/topology.py&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;io.file.buffer.size&lt;/name&gt;
    &lt;value&gt;65536&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;
    &lt;value&gt;ssl-server.xml&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;
    &lt;value&gt;ssl-client.xml&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///dfs/nn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;
    &lt;value&gt;hadoop1:8022&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.https.address&lt;/name&gt;
    &lt;value&gt;hadoop1:50470&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.https.port&lt;/name&gt;
    &lt;value&gt;50470&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
    &lt;value&gt;hadoop1:50070&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;3&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.blocksize&lt;/name&gt;
    &lt;value&gt;134217728&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;
    &lt;value&gt;022&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><h4 id="6、hbase"><a href="#6、hbase" class="headerlink" title="6、hbase"></a>6、hbase</h4><p><strong>core-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://hadoop1:8020&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.trash.interval&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;io.compression.codecs&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.authentication&lt;/name&gt;
    &lt;value&gt;simple&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.authorization&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.rpc.protection&lt;/name&gt;
    &lt;value&gt;authentication&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt;
    &lt;value&gt;DEFAULT&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.oozie.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.oozie.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.mapred.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.flume.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.flume.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.HTTP.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.HTTP.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hive.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.yarn.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.yarn.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.group.mapping&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.security.ShellBasedUnixGroupsMapping&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.security.instrumentation.requires.admin&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.require.client.cert&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.keystores.factory.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.server.conf&lt;/name&gt;
    &lt;value&gt;ssl-server.xml&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hadoop.ssl.client.conf&lt;/name&gt;
    &lt;value&gt;ssl-client.xml&lt;/value&gt;
    &lt;final&gt;true&lt;/final&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><p><strong>hbase-server-config.properties</strong></p>
<pre><code>#hbase  开发环境
need.init.hbase=true
# hbase.zookeeper.quorum=hadoop1.ultiwill.com,hadoop2.ultiwill.com,hadoop3.ultiwill.com
hbase.zookeeper.quorum=hadoop1,hadoop2,hadoop3
hbase.zookeeper.property.clientPort=2181
hbase.rpc.timeout=120000
hbase.client.scanner.timeout.period=120000</code></pre><p><strong>hbase-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://hadoop1:8020/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.replication&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.write.buffer&lt;/name&gt;
    &lt;value&gt;2097152&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.pause&lt;/name&gt;
    &lt;value&gt;100&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.retries.number&lt;/name&gt;
    &lt;value&gt;35&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;
    &lt;value&gt;100&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.keyvalue.maxsize&lt;/name&gt;
    &lt;value&gt;10485760&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.ipc.client.allowsInterrupt&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.primaryCallTimeout.get&lt;/name&gt;
    &lt;value&gt;10&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.primaryCallTimeout.multiget&lt;/name&gt;
    &lt;value&gt;10&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.fs.tmp.dir&lt;/name&gt;
    &lt;value&gt;/user/${user.name}/hbase-staging&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.client.scanner.timeout.period&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.regionserver.thrift.http&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.thrift.support.proxyuser&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rpc.timeout&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.master.timeoutMillis&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.region.timeout&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.snapshot.master.timeout.millis&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.security.authentication&lt;/name&gt;
    &lt;value&gt;simple&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rpc.protection&lt;/name&gt;
    &lt;value&gt;authentication&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;
    &lt;value&gt;60000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;
    &lt;value&gt;/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;zookeeper.znode.rootserver&lt;/name&gt;
    &lt;value&gt;root-region-server&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rest.ssl.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><p><strong>hdfs-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.permissions&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:///dfs/nn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;
    &lt;value&gt;hadoop1:8022&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.https.address&lt;/name&gt;
    &lt;value&gt;hadoop1:50470&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.https.port&lt;/name&gt;
    &lt;value&gt;50470&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;
    &lt;value&gt;hadoop1:50070&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;3&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.blocksize&lt;/name&gt;
    &lt;value&gt;134217728&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.use.datanode.hostname&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;
    &lt;value&gt;022&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.use.legacy.blockreader&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
    &lt;value&gt;/var/run/hdfs-sockets/dn&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.client.domain.socket.data.traffic&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><h4 id="7、kafka"><a href="#7、kafka" class="headerlink" title="7、kafka"></a>7、kafka</h4><p><strong>kafka-data-push-info</strong></p>
<pre><code>--config                            kafka自动推送数据配置目录
--timeOut                           推送超时时间    默认 15 min  单位为分钟

kafka自动推送数据配置：
data.sources                        数据源列表。  （例如：data.sources =bhdb1,dpxx）  

{source}.source.type                某个数据源的类型。 （数据源分为数据库和文件两大类， 若为数据库 则使用 数据的名称 例如 oracle,mysql,sqlserver等， 否则使用 file）
                                                                                                            例如：bhdb1.source.type=oracle 或者  dpxx.source.type=file
数据源为数据库：
{source}.db.name                    数据库的名称
{source}.db.host                    数据库的ip或者主机名
{source}.db.port                    数据库的访问端口， 若不填写则使用该种数据库的默认端口
{source}.db.user                    用户名
{source}.db.pwd                     密码                                                                 
{source}.push.topic                 推送到topic的全局配置，即该数据库下配置的表没有配置topic的时候，其数据会推送到该topic。   
{source}.push.tables                需要推送数的表列表 
{source}.{table}.push.sql           只推送使用该sql查询到的数据    。       不填则表示推送全部。
{source}.{table}.push.adjusterfactory 对推送的数据进行调整  ， 必须为com.bh.d406.bigdata.kafka.producer.DataAdjuster的子类   ，  需要进行调整数据的时候填写
{source}.{table}.push.topic         该表的数据推送到topic名称  ， 若不填则使用全局的topic配置

数据源为文件：
{source}.file.dir                   文件目录    （注意：只支持本地目录 ）    
{source}.file.encoding              文件编码      （默认UTF-8）
{source}.file.extensions            需要过滤的文件格式列表
{source}.file.data.loaderfactory    文件加载器工厂类   
{source}.file.data.fields           记录的字段列表      与顺序有关
{source}.file.data.spliter          数据的分割符         默认 \t
{source}.file.skip.firstline        是否跳过第一行数据                       false  or true
{source}.file.data.adjusterfactory  数据矫正工厂类
{source}.push.thread.num            读取文件的线程数
{source}.push.batch.size            分批推送数据 ， 每批数据大小
{source}.push.topic                 数据推送的目标topic名称
{source}.store.table                存储的表名</code></pre><p><strong>kafka-server-config.properties</strong></p>
<pre><code>#################Kafka 全局配置 #######################
# 格式为host1:port1,host2:port2，
# 这是一个broker列表，用于获得元数据(topics，partitions和replicas)，建立起来的socket连接用于发送实际数据，
# 这个列表可以是broker的一个子集，或者一个VIP，指向broker的一个子集
# metadata.broker.list=hadoop1:9092,slaver01:9092,slaver02:9092
metadata.broker.list=hadoop1:9092

# zookeeper列表
zk.connect=hadoop1:2181,hadoop2:2181,hadoop3:2181

# 字消息的序列化类，默认是的encoder处理一个byte[]，返回一个byte[]
# 默认值为 kafka.serializer.DefaultEncoder
serializer.class=kafka.serializer.StringEncoder

# 用来控制一个produce请求怎样才能算完成，准确的说，是有多少broker必须已经提交数据到log文件，并向leader发送ack，可以设置如下的值：
# 0，意味着producer永远不会等待一个来自broker的ack，这就是0.7版本的行为。这个选项提供了最低的延迟，但是持久化的保证是最弱的，当server挂掉的时候会丢失一些数据。
# 1，意味着在leader replica已经接收到数据后，producer会得到一个ack。这个选项提供了更好的持久性，因为在server确认请求成功处理后，client才会返回。如果刚写到leader上，还没来得及复制leader就挂了，那么消息才可能会丢失。
# -1，意味着在所有的ISR都接收到数据后，producer才得到一个ack。这个选项提供了最好的持久性，只要还有一个replica存活，那么数据就不会丢失。
# 默认值  为 0
request.required.acks=1

# 请求超时时间     默认为 10000
request.timeout.ms=60000

#决定消息是否应在一个后台线程异步发送。
#合法的值为sync，表示异步发送；sync表示同步发送。
#设置为async则允许批量发送请求，这回带来更高的吞吐量，但是client的机器挂了的话会丢失还没有发送的数据。
#默认值为 sync
producer.type=sync</code></pre><h4 id="8、redis"><a href="#8、redis" class="headerlink" title="8、redis"></a>8、redis</h4><p><strong>redis.properties</strong></p>
<pre><code>redis.hostname = 192.168.116.202
redis.port  = 6379</code></pre><h4 id="9、spark"><a href="#9、spark" class="headerlink" title="9、spark"></a>9、spark</h4><p><strong>hive_fields_mapping.properties</strong></p>
<pre><code>datatype= base,wechat

#base = datatype,idcard,name,age,collecttime,imei
#wechat = datatype,wechat,phone,collecttime,imei
#============================================================base
base.datatype = string
base.idcard = string
base.name = string
base.age = long
base.collecttime = string
base.imei = string
#============================================================wechat
wechat.datatype = string
wechat.wechat = string
wechat.phone = string
wechat.collecttime = string
wechat.imei = string</code></pre><p><strong>relation.properties</strong></p>
<pre><code>#需要关联的字段
relationfield = phone_mac,phone,username,send_mail,imei,imsi

complex_relationfield = card,phone_mac,phone,username,send_mail,imei,imsi</code></pre><p><strong>spark-batch-config.properties</strong></p>
<pre><code># spark 常规 配置   不包括 流式处理的 配置

#################### 全局  #############################
# 在用户没有指定时，用于分布式随机操作(groupByKey,reduceByKey等等)的默认的任务数（ shuffle过程中 task的个数 ）
# 默认为 8
spark.default.parallelism=16

# Spark用于缓存的内存大小所占用的Java堆的比率。这个不应该大于JVM中老年代所分配的内存大小
# 默认情况下老年代大小是堆大小的2/3，但是你可以通过配置你的老年代的大小，然后再去增加这个比率
# 默认为 0.66
# spark 1.6 后 过期
# spark.storage.memoryFraction=0.66

# 在spark1.6.0版本默认大小为： (“Java Heap” – 300MB) * 0.75
# 例如：如果堆内存大小有4G，将有2847MB的Spark Memory,Spark Memory=(4*1024MB-300)*0.75=2847MB
# 这部分内存会被分成两部分：Storage Memory和Execution Memory
# 而且这两部分的边界由spark.memory.storageFraction参数设定，默认是0.5即50%
# 新的内存管理模型中的优点是，这个边界不是固定的，在内存压力下这个边界是可以移动的
# 如一个区域内存不够用时可以从另一区域借用内存
spark.memory.fraction=0.75
spark.memory.storageFraction=0.5

# 是否要压缩序列化的RDD分区（比如，StorageLevel.MEMORY_ONLY_SER）
# 在消耗一点额外的CPU时间的代价下，可以极大的提高减少空间的使用
# 默认为 false
spark.rdd.compress=true

# The codec used to compress internal data such as RDD partitions,
# broadcast variables and shuffle outputs. By default,
# Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec,
# e.g.
# 1. org.apache.spark.io.LZ4CompressionCodec, 
# 2. org.apache.spark.io.LZFCompressionCodec, 
# 3. org.apache.spark.io.SnappyCompressionCodec.   default
spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec

# Block size (in bytes) used in Snappy compression,
# in the case when Snappy compression codec is used.
# Lowering this block size will also lower shuffle memory usage when Snappy is used.
# default : 32K
spark.io.compression.snappy.blockSize=32768


# 同时获取每一个分解任务的时候，映射输出文件的最大的尺寸（以兆为单位）。
# 由于对每个输出都需要我们去创建一个缓冲区去接受它，这个属性值代表了对每个分解任务所使用的内存的一个上限值，
# 因此除非你机器内存很大，最好还是配置一下这个值。
# 默认48
spark.reducer.maxSizeInFlight=48

# 这个配置参数仅适用于HashShuffleMananger的实现，同样是为了解决生成过多文件的问题，
# 采用的方式是在不同批次运行的Map任务之间重用Shuffle输出文件，也就是说合并的是不同批次的Map任务的输出数据，
# 但是每个Map任务所需要的文件还是取决于Reduce分区的数量，因此，它并不减少同时打开的输出文件的数量，
# 因此对内存使用量的减少并没有帮助。只是HashShuffleManager里的一个折中的解决方案。
# 默认为false
#spark.shuffle.consolidateFiles=false

#java.io.Externalizable. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.
#default java.io.Serializable
#spark.serializer=org.apache.spark.serializer.KryoSerializer

# Speculation是在任务调度的时候，如果没有适合当前本地性要求的任务可供运行，
# 将跑得慢的任务在空闲计算资源上再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的
# 默认为false
# 慎用   可能导致数据重复的现象
#spark.speculation=true

# task失败重试次数
# 默认为4
spark.task.maxFailures=8

# Spark 是有任务的黑名单机制的，但是这个配置在官方文档里面并没有写，可以设置下面的参数，
# 比如设置成一分钟之内不要再把任务发到这个 Executor 上了，单位是毫秒。
# spark.scheduler.executorTaskBlacklistTime=60000

# 超过这个时间，可以执行 NODE_LOCAL 的任务
# 默认为 3000
spark.locality.wait.process=1

# 超过这个时间，可以执行 RACK_LOCAL 的任务
# 默认为 3000
spark.locality.wait.node=3 

# 超过这个时间，可以执行 ANY 的任务
# 默认为 3000
spark.locality.wait.rack=1000

#################### yarn  ###########################

# 提交的jar文件  的副本数
# 默认为 3
spark.yarn.submit.file.replication=1

# container中的线程数
# 默认为 25
spark.yarn.containerLauncherMaxThreads=25

# 解决yarn-cluster模式下 对处理  permGen space oom异常很有用
# spark.yarn.am.extraJavaOptions=
# spark.driver.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M

# 对象指针压缩 和 gc日志收集打印
# spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M -XX:MaxDirectMemorySize=1536M -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
# -XX:-UseGCOverheadLimit
# GC默认情况下有一个限制，默认是GC时间不能超过2%的CPU时间，但是如果大量对象创建（在Spark里很容易出现，代码模式就是一个RDD转下一个RDD），
# 就会导致大量的GC时间，从而出现OutOfMemoryError: GC overhead limit exceeded，可以通过设置-XX:-UseGCOverheadLimit关掉它。
# -XX:+UseCompressedOops  可以压缩指针（8字节变成4字节）
spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024m -XX:+CMSClassUnloadingEnabled -Xmn512m -XX:MaxTenuringThreshold=15 -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCompressedOops -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError

# 当shuffle缓存的数据超过此值  强制刷磁盘  单位为 byte
# spark.shuffle.spill.initialMemoryThreshold=671088640

################### AKKA 相关 ##########################

# 在控制面板通信（序列化任务和任务结果）的时候消息尺寸的最大值，单位是MB。
# 如果你需要给驱动器发回大尺寸的结果（比如使用在一个大的数据集上面使用collect()方法），那么你就该增加这个值了。
# 默认为 10
spark.akka.frameSize=1024

# 用于通信的actor线程数量。如果驱动器有很多CPU核心，那么在大集群上可以增大这个值。
# 默认为 4
spark.akka.threads=8

# Spark节点之间通信的超时时间，以秒为单位
# 默认为20s
spark.akka.timeout=120

# exector的堆外内存（不会占用 分配给executor的jvm内存）
# spark.yarn.executor.memoryOverhead=2560</code></pre><p><strong>spark-start-config.properties</strong></p>
<pre><code># Spark 任务 使用java -cp 方式启动的参数配置
#
spark.executor.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/native
spark.yarn.jar=local:/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/spark-assembly.jar
spark.authenticate=false
spark.driver.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/native
spark.yarn.historyServer.address=http://BH-LAN-Virtual-hadoop-9:18088
spark.yarn.am.extraLibraryPath=/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/hadoop/lib/native
spark.eventLog.enabled=true
spark.dynamicAllocation.schedulerBacklogTimeout=1
SPARK_SUBMIT=true
spark.yarn.config.gatewayPath=/opt/cloudera/parcels
spark.ui.killEnabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled=true
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.executorIdleTimeout=60
spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/../../..
spark.shuffle.service.port=7337
spark.eventLog.dir=hdfs://nameservice1/user/spark/applicationHistory
spark.dynamicAllocation.enabled=true

#/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/lib/*
#/etc/spark/conf.cloudera.spark_on_yarn/
#/etc/hadoop/conf.cloudera.yarn/

spark.submit.deployMode=client
spark.app.name=default
spark.master=yarn-client
spark.driver.memory=1g
spark.executor.instances=1
spark.executor.memory=4g
spark.executor.cores=2
spark.jars=</code></pre><p><strong>spark-streaming-config.properties</strong></p>
<pre><code># spark  流式处理的 配置

# job的并行度
# 默认为 1
spark.streaming.concurrentJobs=1

# Spark记忆任何元数据(stages生成，任务生成等等)的时间(秒)。周期性清除保证在这个时间之前的元数据会被遗忘。
#当长时间几小时，几天的运行Spark的时候设置这个是很有用的。注意：任何内存中的RDD只要过了这个时间就会被清除掉。
# 默认 disable
spark.cleaner.ttl=3600

# 将不再使用的缓存数据清除
# 默认为false
spark.streaming.unpersist=true

# 从网络中批量接受对象时的持续时间 , 单位  ms。
# 默认为200ms
spark.streaming.blockInterval=200

# 控制Receiver速度  单位 s
# 因为当streaming程序的数据源的数据量突然变大巨大，可能会导致streaming被撑住导致吞吐不过来，所以可以考虑对于最大吞吐做一下限制。
# 默认为 100000
spark.streaming.receiver.maxRate=10000

# kafka每个分区最大的读取速度   单位 s
# 控制kafka读取的量
spark.streaming.kafka.maxRatePerPartition=50

# 读取kafka的分区最新offset的最大尝试次数
# 默认为1
spark.streaming.kafka.maxRetries=5

# 1、为什么引入Backpressure
# 默认情况下，Spark Streaming通过Receiver以生产者生产数据的速率接收数据，计算过程中会出现batch processing time &gt; batch interval的情况，
# 其中batch processing time 为实际计算一个批次花费时间， batch interval为Streaming应用设置的批处理间隔。
# 这意味着Spark Streaming的数据接收速率高于Spark从队列中移除数据的速率，也就是数据处理能力低，在设置间隔内不能完全处理当前接收速率接收的数据。
# 如果这种情况持续过长的时间，会造成数据在内存中堆积，导致Receiver所在Executor内存溢出等问题（如果设置StorageLevel包含disk, 则内存存放不下的数据会溢写至disk, 加大延迟）。
# Spark 1.5以前版本，用户如果要限制Receiver的数据接收速率，可以通过设置静态配制参数“spark.streaming.receiver.maxRate”的值来实现，
# 此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题。比如：producer数据生产高于maxRate，当前集群处理能力也高于maxRate，这就会造成资源利用率下降等问题。
# 为了更好的协调数据接收速率与资源处理能力，Spark Streaming 从v1.5开始引入反压机制（back-pressure）,通过动态控制数据接收速率来适配集群数据处理能力。
# 2、Backpressure
# Spark Streaming Backpressure:  根据JobScheduler反馈作业的执行信息来动态调整Receiver数据接收率。
# 通过属性“spark.streaming.backpressure.enabled”来控制是否启用backpressure机制，默认值false，即不启用
spark.streaming.backpressure.enabled=true
spark.streaming.backpressure.initialRate=200</code></pre><p><strong>datatype/fieldtype.properties</strong></p>
<p><strong>hive/hive-server-config.properties</strong></p>
<pre><code># hbase  开发环境</code></pre><p><strong>hive/hive-site.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;!--Autogenerated by Cloudera Manager--&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.uris&lt;/name&gt;
    &lt;value&gt;thrift://hadoop1:9083&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt;
    &lt;value&gt;300&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.auto.convert.join.noconditionaltask.size&lt;/name&gt;
    &lt;value&gt;20971520&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.bucketmapjoin.sortedmerge&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.smbjoin.cache.rows&lt;/name&gt;
    &lt;value&gt;10000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
    &lt;value&gt;/hadoop_log/log/hive/operation_logs&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;mapred.reduce.tasks&lt;/name&gt;
    &lt;value&gt;-1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt;
    &lt;value&gt;67108864&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.copyfile.maxsize&lt;/name&gt;
    &lt;value&gt;33554432&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.reducers.max&lt;/name&gt;
    &lt;value&gt;1099&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.groupby.checkinterval&lt;/name&gt;
    &lt;value&gt;4096&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.groupby.flush.percent&lt;/name&gt;
    &lt;value&gt;0.1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.compute.query.using.stats&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.execution.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.vectorized.execution.reduce.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.mapfiles&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.cbo.enable&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt;
    &lt;value&gt;minimal&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.fetch.task.conversion.threshold&lt;/name&gt;
    &lt;value&gt;268435456&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.limit.pushdown.memory.usage&lt;/name&gt;
    &lt;value&gt;0.1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.sparkfiles&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt;
    &lt;value&gt;16777216&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.merge.size.per.task&lt;/name&gt;
    &lt;value&gt;268435456&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.reducededuplication&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.reducededuplication.min.reducer&lt;/name&gt;
    &lt;value&gt;4&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.map.aggr&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.map.aggr.hash.percentmemory&lt;/name&gt;
    &lt;value&gt;0.5&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.optimize.sort.dynamic.partition&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.execution.engine&lt;/name&gt;
    &lt;value&gt;mr&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.executor.memory&lt;/name&gt;
    &lt;value&gt;1369020825&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.driver.memory&lt;/name&gt;
    &lt;value&gt;966367641&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.executor.cores&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.yarn.driver.memoryOverhead&lt;/name&gt;
    &lt;value&gt;102&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.yarn.executor.memoryOverhead&lt;/name&gt;
    &lt;value&gt;230&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.initialExecutors&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.minExecutors&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.dynamicAllocation.maxExecutors&lt;/name&gt;
    &lt;value&gt;2147483647&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.execute.setugi&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.support.concurrency&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;hadoop1,hadoop3,hadoop2&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.zookeeper.namespace&lt;/name&gt;
    &lt;value&gt;hive_zookeeper_namespace_hive&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.cluster.delegation.token.store.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.hive.thrift.MemoryTokenStore&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.server2.use.SSL&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;spark.shuffle.service.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;</code></pre><h3 id="五．Flume开发"><a href="#五．Flume开发" class="headerlink" title="五．Flume开发"></a>五．Flume开发</h3><p><strong>xz_bigdata_flume</strong></p>
<p><strong>FTP–&gt;FlumeSource–&gt;拦截器–&gt;FlumeChannel–&gt;FlumeSink–&gt;Kafka</strong></p>
<p><strong>自定义的内容有：FlumeSource、拦截器、FlumeSink</strong></p>
<h4 id="1、maven冲突解决和pom-xml"><a href="#1、maven冲突解决和pom-xml" class="headerlink" title="1、maven冲突解决和pom.xml"></a>1、maven冲突解决和pom.xml</h4><p>1.1 安装Maven Helper插件，在Settings里面的Plugins里面搜索Maven Helper，点击Install，安装完毕。</p>
<p>1.2 ETL包括数据的抽取、转换、加载<br>①数据抽取：从源数据源系统抽取目的数据源系统需要的数据：<br>②数据转换：将从源数据源获取的数据按照业务需求，转换成目的数据源要求的形式，并对错误、不一致的数据进行清洗和加工；<br>③数据加载：将转换后的数据装载到目的数据源。</p>
<p><img src="/medias/Flume%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.PNG" alt="Flume数据处理流程"></p>
<p>1.3 pom.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_flume&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_flume&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;flume-ng.version&gt;1.6.0&lt;/flume-ng.version&gt;
        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;
        &lt;jdom.version&gt;1.0&lt;/jdom.version&gt;
        &lt;c3p0.version&gt;0.9.5&lt;/c3p0.version&gt;
        &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt;
        &lt;mybatis.version&gt;3.1.1&lt;/mybatis.version&gt;
        &lt;zookeeper.version&gt;3.4.6&lt;/zookeeper.version&gt;
        &lt;net.sf.json.version&gt;2.2.3&lt;/net.sf.json.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-configuration&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-io&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;snappy-java&lt;/artifactId&gt;
                    &lt;groupId&gt;org.xerial.snappy&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
                    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;
                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;log4j&lt;/artifactId&gt;
                    &lt;groupId&gt;log4j&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;!--flume核心依赖--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;
            &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;
            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;guava&lt;/artifactId&gt;
                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-codec&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-logging&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;jetty&lt;/artifactId&gt;
                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;
                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;
                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-io&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;
                    &lt;groupId&gt;commons-lang&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;
            &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt;
            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!--flume配置依赖--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;
            &lt;artifactId&gt;flume-ng-configuration&lt;/artifactId&gt;
            &lt;version&gt;${flume-ng.version}-${cdh.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;guava&lt;/artifactId&gt;
                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;jdom&lt;/groupId&gt;
            &lt;artifactId&gt;jdom&lt;/artifactId&gt;
            &lt;version&gt;${jdom.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-io&lt;/groupId&gt;
            &lt;artifactId&gt;commons-io&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-lang&lt;/groupId&gt;
            &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;commons-configuration&lt;/groupId&gt;
            &lt;artifactId&gt;commons-configuration&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;RELEASE&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;defaultGoal&gt;compile&lt;/defaultGoal&gt;
        &lt;sourceDirectory&gt;src/main/java/&lt;/sourceDirectory&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;
                            &lt;mainClass&gt;&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy&lt;/id&gt;
                        &lt;phase&gt;install&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;outputDirectory&gt;
                                ${project.build.directory}/jars
                            &lt;/outputDirectory&gt;
                            &lt;excludeArtifactIds&gt;javaee-api&lt;/excludeArtifactIds&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.7&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;</code></pre><h4 id="2、自定义source"><a href="#2、自定义source" class="headerlink" title="2、自定义source"></a>2、自定义source</h4><p><strong>2.1 继承AbstractSource 实现 Configurable, PollableSource接口</strong></p>
<pre><code>package com.hsiehchou.flume.source;

import com.hsiehchou.flume.constant.FlumeConfConstant;
import com.hsiehchou.flume.fields.MapFields;import com.hsiehchou.flume.utils.FileUtilsStronger;
import org.apache.commons.io.FileUtils;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.PollableSource;
import org.apache.flume.channel.ChannelProcessor;
import org.apache.flume.conf.Configurable;
import org.apache.flume.event.SimpleEvent;
import org.apache.flume.source.AbstractSource;
import org.apache.log4j.Logger;

import java.io.File;
import java.util.*;

/**
 * 固定写法，自定义Source 直接继承 AbstractSource 和 实现 Configurable, PollableSource 接口
 * 可参照官网 http://flume.apache.org/releases/content/1.9.0/FlumeDeveloperGuide.html#source
 */
public class FolderSource extends AbstractSource implements Configurable, PollableSource {

    private final Logger logger = Logger.getLogger(FolderSource.class);

    //tier1.sources.source1.sleeptime=5
    //tier1.sources.source1.filenum=3000
    //tier1.sources.source1.dirs =/usr/chl/data/filedir/
    //tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/

    //以下为配置在flume.conf文件中
    //读取的文件目录
    private String dirStr;
    //读取的文件目录，如果多个，以&quot;,&quot;分割，在flume.conf里面配置
    private String[] dirs;
    //处理成功的文件写入的目录
    private String successfile;
    //睡眠时间
    private long sleeptime = 5;
    //每批文件数量
    private int filenum = 500;

    //以下为配置在txtparse.properties文件中
    //读取的所有文件集合
    private Collection&lt;File&gt; allFiles;

    //一批处理的文件大小
    private List&lt;File&gt; listFiles;
    private ArrayList&lt;Event&gt; eventList = new ArrayList&lt;Event&gt;();

    /**
     * @param context 拿到flume配置里面的所有参数
     */
    @Override
    public void configure(Context context) {
        logger.info(&quot;开始初始化flume参数&quot;);
        initFlumeParams(context);
        logger.info(&quot;初始化flume参数成功&quot;);
    }

    @Override
    public Status process() {
        //定义处理逻辑
        try {
            Thread.currentThread().sleep(sleeptime * 1000);
        } catch (InterruptedException e) {
            logger.error(null, e);
        }

        Status status = null;
        try {
            // for (String dir : dirs) {
            logger.info(&quot;dirStr===========&quot; + dirStr);


            //TODO 1.监控目录下面的所有文件
            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件
            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);

            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理
            if (allFiles.size() &gt;= filenum) {
                //文件数量大于3000 只取3000条
                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);
            } else {
                //文件数量小于3000，取所有文件进行处理
                listFiles = ((List&lt;File&gt;) allFiles);
            }

            //TODO 2.遍历所有的文件进行解析
            if (listFiles.size() &gt; 0) {

                for (File file : listFiles) {
                    //文件名是需要传到channel中的
                    String fileName = file.getName();

                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容
                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);

                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容

                    //获取文件绝对路径
                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);

                    //获取文件内容
                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);

                    //TODO 解析出来之后，需要把解析出来的数据封装为Event
                    if (lines != null &amp;&amp; lines.size() &gt; 0) {

                        //遍历读取的内容
                        for (String line : lines) {

                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中
                            //构建event头
                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();

                            //文件名
                            map.put(MapFields.FILENAME, fileName);

                            //文件绝对路径
                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);

                            //构建event
                            SimpleEvent event = new SimpleEvent();

                            //把读取的一行数据转成字节
                            byte[] bytes = line.getBytes();
                            event.setBody(bytes);
                            event.setHeaders(map);
                            eventList.add(event);
                        }
                    }

                    try {
                        if (eventList.size() &gt; 0) {
                            //获取channelProcessor
                            ChannelProcessor channelProcessor = getChannelProcessor();

                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截
                            channelProcessor.processEventBatch(eventList);
                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());
                        }
                        eventList.clear();
                    } catch (Exception e) {
                        eventList.clear();
                        logger.error(&quot;发送数据到channel失败&quot;, e);
                    } finally {
                        eventList.clear();
                    }
                }
            }
            // 处理成功，返回成功状态
            status = Status.READY;
            return status;
        } catch (Exception e) {
            status = Status.BACKOFF;
            logger.error(&quot;异常&quot;, e);
            return status;
        }
    }

    /**
     * 初始化flume參數
     * @param context
     */
    public void initFlumeParams(Context context) {

        //读取flume，conf配置文件，初始化参数
        try {
            //文件处理目录

            //监控的文件目录
            dirStr = context.getString(FlumeConfConstant.DIRS);

            //监控多个目录
            dirs = dirStr.split(&quot;,&quot;);

            //成功处理的文件存放目录
            successfile = context.getString(FlumeConfConstant.SUCCESSFILE);

            //每批处理文件个数
            filenum = context.getInteger(FlumeConfConstant.FILENUM);

            //睡眠时间
            sleeptime = context.getLong(FlumeConfConstant.SLEEPTIME);

            logger.info(&quot;dirStr============&quot; + dirStr);
            logger.info(&quot;dirs==============&quot; + dirs);
            logger.info(&quot;successfile=======&quot; + successfile);
            logger.info(&quot;filenum===========&quot; + filenum);
            logger.info(&quot;sleeptime=========&quot; + sleeptime);

        } catch (Exception e) {
            logger.error(&quot;初始化flume参数失败&quot;, e);
        }
    }

    @Override
    public long getBackOffSleepIncrement() {
        return 0;
    }

    @Override
    public long getMaxBackOffSleepInterval() {
        return 0;
    }
}</code></pre><p><strong>2.2 实现process()方法</strong><br>此处代码已经在2.1里面，不用再写了</p>
<pre><code> public Status process() {
        //定义处理逻辑
        try {
            Thread.currentThread().sleep(sleeptime * 1000);
        } catch (InterruptedException e) {
            logger.error(null, e);
        }

        Status status = null;
        try {
            // for (String dir : dirs) {
            logger.info(&quot;dirStr===========&quot; + dirStr);


            //TODO 1.监控目录下面的所有文件
            //读取目录下的文件，获取目录下所有以 &quot;txt&quot;, &quot;bcp&quot; 结尾的文件
            allFiles = FileUtils.listFiles(new File(dirStr), new String[]{&quot;txt&quot;, &quot;bcp&quot;}, true);

            //如果目录下文件总数大于阈值，则只取 filenum 个文件进行处理
            if (allFiles.size() &gt;= filenum) {
                //文件数量大于3000 只取3000条
                listFiles = ((List&lt;File&gt;) allFiles).subList(0, filenum);
            } else {
                //文件数量小于3000，取所有文件进行处理
                listFiles = ((List&lt;File&gt;) allFiles);
            }

            //TODO 2.遍历所有的文件进行解析
            if (listFiles.size() &gt; 0) {

                for (File file : listFiles) {
                    //文件名是需要传到channel中的
                    String fileName = file.getName();

                    //解析文件  获取文件名及文件内容 文件绝对路径  文件内容
                    Map&lt;String, Object&gt; stringObjectMap = FileUtilsStronger.parseFile(file, successfile);

                    //返回的内容2个参数  一个是文件绝对路径  另一个是lines文件的所有内容

                    //获取文件绝对路径
                    String absoluteFilename = (String) stringObjectMap.get(MapFields.ABSOLUTE_FILENAME);

                    //获取文件内容
                    List&lt;String&gt; lines = (List&lt;String&gt;) stringObjectMap.get(MapFields.VALUE);

                    //TODO 解析出来之后，需要把解析出来的数据封装为Event
                    if (lines != null &amp;&amp; lines.size() &gt; 0) {

                        //遍历读取的内容
                        for (String line : lines) {

                            //封装event Header 将文件名及文件绝对路径通过header传送到channel中
                            //构建event头
                            Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();

                            //文件名
                            map.put(MapFields.FILENAME, fileName);

                            //文件绝对路径
                            map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);

                            //构建event
                            SimpleEvent event = new SimpleEvent();

                            //把读取的一行数据转成字节
                            byte[] bytes = line.getBytes();
                            event.setBody(bytes);
                            event.setHeaders(map);
                            eventList.add(event);
                        }
                    }

                    try {
                        if (eventList.size() &gt; 0) {
                            //获取channelProcessor
                            ChannelProcessor channelProcessor = getChannelProcessor();

                            //通过channelProcessor把eventList发送出去，可以通过拦截器进行拦截
                            channelProcessor.processEventBatch(eventList);
                            logger.info(&quot;批量推送到 拦截器 数据大小为&quot; + eventList.size());
                        }
                        eventList.clear();
                    } catch (Exception e) {
                        eventList.clear();
                        logger.error(&quot;发送数据到channel失败&quot;, e);
                    } finally {
                        eventList.clear();
                    }
                }
            }
            // 处理成功，返回成功状态
            status = Status.READY;
            return status;
        } catch (Exception e) {
            status = Status.BACKOFF;
            logger.error(&quot;异常&quot;, e);
            return status;
        }
    }</code></pre><p><strong>source/MySource.java—Flume官网上的案例</strong></p>
<pre><code>package com.hsiehchou.flume.source;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.EventDeliveryException;
import org.apache.flume.PollableSource;
import org.apache.flume.conf.Configurable;
import org.apache.flume.event.SimpleEvent;
import org.apache.flume.source.AbstractSource;

public class MySource extends AbstractSource implements Configurable, PollableSource {
    private String myProp;

    /**
     * 配置读取
     * @param context
     */
    @Override
    public void configure(Context context) {
        String myProp = context.getString(&quot;myProp&quot;, &quot;defaultValue&quot;);
        // Process the myProp value (e.g. validation, convert to another type, ...)
        // Store myProp for later retrieval by process() method
        this.myProp = myProp;
    }

    /**
     * 定义自己的业务逻辑
     * @return
     * @throws EventDeliveryException
     */
    @Override
    public Status process() throws EventDeliveryException {
        Status status = null;
        try {
            // This try clause includes whatever Channel/Event operations you want to do
            // Receive new data
            //需要把自己的数据封装为event进行传输
            Event e = new SimpleEvent();

            // Store the Event into this Source&#39;s associated Channel(s)
            getChannelProcessor().processEvent(e);
            status = Status.READY;
        } catch (Throwable t) {
            // Log exception, handle individual exceptions as needed
            status = Status.BACKOFF;
            // re-throw all Errors
            if (t instanceof Error) {
                throw (Error)t;
            }
        } finally {

        }
        return status;
    }

    @Override
    public long getBackOffSleepIncrement() {
        return 0;
    }

    @Override
    public long getMaxBackOffSleepInterval() {
        return 0;
    }

    @Override
    public void start() {
        // Initialize the connection to the external client
    }

    @Override
    public void stop () {
        // Disconnect from external client and do any additional cleanup
        // (e.g. releasing resources or nulling-out field values) ..
    }
}</code></pre><h4 id="3、自定义interceptor—数据清洗过滤器"><a href="#3、自定义interceptor—数据清洗过滤器" class="headerlink" title="3、自定义interceptor—数据清洗过滤器"></a>3、自定义interceptor—数据清洗过滤器</h4><p><strong>3.1实现Interceptor 接口</strong></p>
<pre><code>package com.hsiehchou.flume.interceptor;

import com.alibaba.fastjson.JSON;
import com.hsiehchou.flume.fields.MapFields;
import com.hsiehchou.flume.service.DataCheck;
import org.apache.commons.io.Charsets;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.event.SimpleEvent;
import org.apache.flume.interceptor.Interceptor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 数据清洗过滤器
 */
public class DataCleanInterceptor implements Interceptor {

    private static final Logger LOG = LoggerFactory.getLogger(DataCleanInterceptor.class);

    //datatpye.properties
    //private static Map&lt;String,ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;

    /**
     *  初始化
     */
    @Override
    public void initialize() {
    }

    /**
     * 单条处理
     * 拦截方法。数据解析，封装，数据清洗
     * @param event
     * @return
     */
    @Override
    public Event intercept(Event event) {
        SimpleEvent eventNew = new SimpleEvent();
        try {
            LOG.info(&quot;拦截器Event开始执行&quot;);
            Map&lt;String, String&gt; map = parseEvent(event);
            if(map == null){
                return null;
            }
            String lineJson = JSON.toJSONString(map);
            LOG.info(&quot;拦截器推送数据到channel:&quot; +lineJson);
            eventNew.setBody(lineJson.getBytes());
        } catch (Exception e) {
            LOG.error(null,e);
        }
        return eventNew;
    }

    /**
     * 批处理
     * @param events
     * @return
     */
    @Override
    public List&lt;Event&gt; intercept(List&lt;Event&gt; events) {
        List&lt;Event&gt; list = new ArrayList&lt;Event&gt;();
        for (Event event : events) {
            Event intercept = intercept(event);
            if (intercept != null) {
                list.add(intercept);
            }
        }
        return list;
    }

    @Override
    public void close() {
    }

    /**
     * 数据解析
     * @param event
     * @return
     */
    public static Map&lt;String,String&gt; parseEvent(Event event){
        if (event == null) {
            return null;
        }

        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
        String line = new String(event.getBody(), Charsets.UTF_8);

        //文件名 和 文件绝对路径
        String filename = event.getHeaders().get(MapFields.FILENAME);
        String absoluteFilename = event.getHeaders().get(MapFields.ABSOLUTE_FILENAME);

        //String转map，进行数据校验，检验错误入ES错误表
        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);
        return map;

        //wechat_source1_1111115.txt
        //String[] fileNames = filename.split(&quot;_&quot;);

        // String转map，并进行数据长度校验，校验错误入ES错误表
        //Map&lt;String, String&gt; map = JZDataCheck.txtParse(type, line, source, filename,absoluteFilename);
        //Map&lt;String,String&gt; map = new HashMap&lt;&gt;();

        //000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
        //String[] split = line.split(&quot;\t&quot;);

        //数据类别
        //String dataType = fileNames[0];

        //imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time
        //ArrayList&lt;String&gt; fields = dataMap.get(dataType);
        //for (int i = 0; i &lt; split.length; i++) {
        //    map.put(fields.get(i),split[i]);
        //}

        //添加ID
        //map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;));
        // map.put(MapFields.TABLE, dataType);
        // map.put(MapFields.FILENAME, filename);
        // map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);

//        Map&lt;String, String&gt; map = DataCheck.txtParseAndalidation(line,filename,absoluteFilename);
//        return map;
    }

    /**
     * 实例化创建
     */
    public static class Builder implements Interceptor.Builder {
        @Override
        public void configure(Context context) {
        }
        @Override
        public Interceptor build() {
            return new DataCleanInterceptor();
        }
    }
}</code></pre><h4 id="4、utils工具类"><a href="#4、utils工具类" class="headerlink" title="4、utils工具类"></a>4、utils工具类</h4><p><strong>utils/FileUtilsStronger.java</strong></p>
<pre><code>package com.hsiehchou.flume.utils;

import com.hsiehchou.common.time.TimeTranstationUtils;
import com.hsiehchou.flume.fields.MapFields;
import org.apache.commons.io.FileUtils;
import org.apache.log4j.Logger;

import java.io.File;
import java.util.*;

import static java.io.File.separator;

public class FileUtilsStronger {

    private static final Logger logger = Logger.getLogger(FileUtilsStronger.class);

    /**
     * @param file
     * @param path
     */
    public static Map&lt;String,Object&gt; parseFile(File file, String path) {

        Map&lt;String,Object&gt; map=new HashMap&lt;String,Object&gt;();
        List&lt;String&gt; lines;
        String fileNew = path+ TimeTranstationUtils.Date2yyyy_MM_dd()+getDir(file);

        try {
            if((new File(fileNew+file.getName())).exists()){
                try{
                    logger.info(&quot;文件名已经存在，开始删除同名已经存在文件&quot;+file.getAbsolutePath());
                    file.delete();
                    logger.info(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);
                }catch (Exception e){
                    logger.error(&quot;删除同名已经存在文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);
                }
            }else{
                lines = FileUtils.readLines(file);
                map.put(MapFields.ABSOLUTE_FILENAME,fileNew+file.getName());
                map.put(MapFields.VALUE,lines);
                FileUtils.moveToDirectory(file, new File(fileNew), true);
                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+fileNew+&quot;成功&quot;);
            }
        } catch (Exception e) {
            logger.error(&quot;移动文件&quot; + file.getAbsolutePath() + &quot;到&quot; + fileNew + &quot;失败&quot;, e);
        }
        return map;
    }

    /**
     * @param file
     * @param path
     */
    public static List&lt;String&gt; chanmodName(File file, String path) {

        List&lt;String&gt; lines=null;
        try {
            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){
                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());
                try{
                    file.delete();
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);
                }catch (Exception e){
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);
                }
            }else{
                lines = FileUtils.readLines(file);
                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);
                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);
            }
        } catch (Exception e) {
            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);
        }
        return lines;
    }

    /**
     * @param file
     * @param path
     */
    public static void moveFile2unmanage(File file, String path) {

        try {
            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){
                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +file.getAbsolutePath());
                try{
                    file.delete();
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);
                }catch (Exception e){
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);
                }
            }else{
                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);
                //logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);
            }
        } catch (Exception e) {
            logger.error(&quot;移动错误文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);
        }
    }

    /**
     * @param file
     * @param path
     */
    public static void shnegtingChanmodName(File file, String path) {
        try {
            if((new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName())).exists()){
                logger.warn(&quot;文件名已经存在，开始删除同名文件&quot; +path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;/&quot;+file.getName());
                try{
                    file.delete();
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;成功&quot;);
                }catch (Exception e){
                    logger.warn(&quot;删除同名文件&quot;+file.getAbsolutePath()+&quot;失败&quot;,e);
                }
            }else{
                FileUtils.moveToDirectory(file, new File(path+ TimeTranstationUtils.Date2yyyy_MM_dd()), true);
                logger.info(&quot;移动文件到&quot;+file.getAbsolutePath()+&quot;到&quot;+path+ TimeTranstationUtils.Date2yyyy_MM_dd()+&quot;成功&quot;);
            }
        } catch (Exception e) {
            logger.error(&quot;移动文件&quot; + file.getName() + &quot;到&quot; + path+ TimeTranstationUtils.Date2yyyy_MM_dd() + &quot;失败&quot;, e);
        }
    }

    /**
     * 获取文件父目录
     * @param file
     * @return
     */
    public static String getDir(File file){

        String dir=file.getParent();
        StringTokenizer dirs = new StringTokenizer(dir, separator);
        List&lt;String&gt; list=new ArrayList&lt;String&gt;();
        while(dirs.hasMoreTokens()){
            list.add((String)dirs.nextElement());
        }
        String str=&quot;&quot;;
        for(int i=2;i&lt;list.size();i++){
            str=str+separator+list.get(i);
        }
        return str+&quot;/&quot;;
    }
}</code></pre><p><strong>utils/Validation.java—验证工具类</strong></p>
<pre><code>package com.hsiehchou.flume.utils;

import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * 验证工具类
 */
@Deprecated
public class Validation {
     // ------------------常量定义
    /**
     * Email正则表达式=
     * &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;
     * ;
     */
    // public static final String EMAIL =
    // &quot;^([a-z0-9A-Z]+[-|\\.]?)+[a-z0-9A-Z]@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]{2,}$&quot;;;
    public static final String EMAIL = &quot;\\w+(\\.\\w+)*@\\w+(\\.\\w+)+&quot;;

    /**
     * 电话号码正则表达式=
     * (^(\d{2,4}[-_－—]?)?\d{3,8}([-_－—]?\d{3,8})?([-_－—]?\d{1,7})?$)|
     * (^0?1[35]\d{9}$)
     */
    public static final String PHONE = &quot;(^(\\d{2,4}[-_－—]?)?\\d{3,8}([-_－—]?\\d{3,8})?([-_－—]?\\d{1,7})?$)|(^0?1[35]\\d{9}$)&quot;;

    /**
     * 手机号码正则表达式=^(13[0-9]|15[0-9]|18[0-9])\d{8}$
     */
    public static final String MOBILE = &quot;^((13[0-9])|(14[5-7])|(15[^4])|(17[0-8])|(18[0-9]))\\d{8}$&quot;;

    /**
     * Integer正则表达式 ^-?(([1-9]\d*$)|0)
     */
    public static final String INTEGER = &quot;^-?(([1-9]\\d*$)|0)&quot;;

    /**
     * 正整数正则表达式 &gt;=0 ^[1-9]\d*|0$
     */
    public static final String INTEGER_NEGATIVE = &quot;^[1-9]\\d*|0$&quot;;

    /**
     * 负整数正则表达式 &lt;=0 ^-[1-9]\d*|0$
     */
    public static final String INTEGER_POSITIVE = &quot;^-[1-9]\\d*|0$&quot;;

    /**
     * Double正则表达式 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$
     */
    public static final String DOUBLE = &quot;^-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$&quot;;

    /**
     * 正Double正则表达式 &gt;=0 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$　
     */
    public static final String DOUBLE_NEGATIVE = &quot;^[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0$&quot;;

    /**
     * 负Double正则表达式 &lt;= 0 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$
     */
    public static final String DOUBLE_POSITIVE = &quot;^(-([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*))|0?\\.0+|0$&quot;;

    /**
     * 年龄正则表达式 ^(?:[1-9][0-9]?|1[01][0-9]|120)$ 匹配0-120岁
     */
    public static final String AGE = &quot;^(?:[1-9][0-9]?|1[01][0-9]|120)$&quot;;

    /**
     * 邮编正则表达式 [0-9]\d{5}(?!\d) 国内6位邮编
     */
    public static final String CODE = &quot;[0-9]\\d{5}(?!\\d)&quot;;

    /**
     * 匹配由数字、26个英文字母或者下划线组成的字符串 ^\w+$
     */
    public static final String STR_ENG_NUM_ = &quot;^\\w+$&quot;;

    /**
     * 匹配由数字和26个英文字母组成的字符串 ^[A-Za-z0-9]+$
     */
    public static final String STR_ENG_NUM = &quot;^[A-Za-z0-9]+&quot;;

    /**
     * 匹配由26个英文字母组成的字符串 ^[A-Za-z]+$
     */
    public static final String STR_ENG = &quot;^[A-Za-z]+$&quot;;

    /**
     * 过滤特殊字符串正则 regEx=
     * &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;
     */
    public static final String STR_SPECIAL = &quot;[`~!@#$%^&amp;*()+=|{}&#39;:;&#39;,\\[\\].&lt;&gt;/?~！@#￥%……&amp;*（）——+|{}【】‘；：”“’。，、？]&quot;;

    /***
     * 日期正则 支持： YYYY-MM-DD YYYY/MM/DD YYYY_MM_DD YYYYMMDD YYYY.MM.DD的形式
     */
    public static final String DATE_ALL = &quot;((^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(10|12|0?[13578])([-\\/\\._]?)(3[01]|[12][0-9]|0?[1-9])$)&quot;
            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(11|0?[469])([-\\/\\._]?)(30|[12][0-9]|0?[1-9])$)&quot;
            + &quot;|(^((1[8-9]\\d{2})|([2-9]\\d{3}))([-\\/\\._]?)(0?2)([-\\/\\._]?)(2[0-8]|1[0-9]|0?[1-9])$)|(^([2468][048]00)([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([3579][26]00)&quot;
            + &quot;([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)&quot;
            + &quot;|(^([1][89][0][48])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][0][48])([-\\/\\._]?)&quot;
            + &quot;(0?2)([-\\/\\._]?)(29)$)&quot;
            + &quot;|(^([1][89][2468][048])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|(^([2-9][0-9][2468][048])([-\\/\\._]?)(0?2)&quot;
            + &quot;([-\\/\\._]?)(29)$)|(^([1][89][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$)|&quot;
            + &quot;(^([2-9][0-9][13579][26])([-\\/\\._]?)(0?2)([-\\/\\._]?)(29)$))&quot;;

    /***
     * 日期正则 支持： YYYY-MM-DD
     */
    public static final String DATE_FORMAT1 = &quot;(([0-9]{3}[1-9]|[0-9]{2}[1-9][0-9]{1}|[0-9]{1}[1-9][0-9]{2}|[1-9][0-9]{3})-(((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01]))|((0[469]|11)-(0[1-9]|[12][0-9]|30))|(02-(0[1-9]|[1][0-9]|2[0-8]))))|((([0-9]{2})(0[48]|[2468][048]|[13579][26])|((0[48]|[2468][048]|[3579][26])00))-02-29)&quot;;

    /**
     * URL正则表达式 匹配 http www ftp
     */
    public static final String URL = &quot;^(http|www|ftp|)?(://)?(\\w+(-\\w+)*)(\\.(\\w+(-\\w+)*))*((:\\d+)?)(/(\\w+(-\\w+)*))*(\\.?(\\w)*)(\\?)?&quot;
            + &quot;(((\\w*%)*(\\w*\\?)*(\\w*:)*(\\w*\\+)*(\\w*\\.)*(\\w*&amp;)*(\\w*-)*(\\w*=)*(\\w*%)*(\\w*\\?)*&quot;
            + &quot;(\\w*:)*(\\w*\\+)*(\\w*\\.)*&quot;
            + &quot;(\\w*&amp;)*(\\w*-)*(\\w*=)*)*(\\w*)*)$&quot;;

    /**
     * 身份证正则表达式
     */
    public static final String IDCARD = &quot;((11|12|13|14|15|21|22|23|31|32|33|34|35|36|37|41|42|43|44|45|46|50|51|52|53|54|61|62|63|64|65)[0-9]{4})&quot;
            + &quot;(([1|2][0-9]{3}[0|1][0-9][0-3][0-9][0-9]{3}&quot;
            + &quot;[Xx0-9])|([0-9]{2}[0|1][0-9][0-3][0-9][0-9]{3}))&quot;;

    /**
     * 机构代码
     */
    public static final String JIGOU_CODE = &quot;^[A-Z0-9]{8}-[A-Z0-9]$&quot;;

    /**
     * 匹配数字组成的字符串 ^[0-9]+$
     */
    public static final String STR_NUM = &quot;^[0-9]+$&quot;;

    // //------------------验证方法
    /**
     * 判断字段是否为空 符合返回ture
     * @param str
     * @return boolean
     */
    public static synchronized boolean StrisNull(String str) {
        return null == str || str.trim().length() &lt;= 0 ? true : false;
    }

    /**
     * 判断字段是非空 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean StrNotNull(String str) {
        return !StrisNull(str);
    }

    /**
     * 字符串null转空
     * @param str
     * @return boolean
     */
    public static String nulltoStr(String str) {
        return StrisNull(str) ? &quot;&quot; : str;
    }

    /**
     * 字符串null赋值默认值
     * @param str 目标字符串
     * @param defaut 默认值
     * @return String
     */
    public static String nulltoStr(String str, String defaut) {
        return StrisNull(str) ? defaut : str;
    }

    /**
     * 判断字段是否为Email 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isEmail(String str) {
        return Regular(str, EMAIL);
    }

    /**
     * 判断是否为电话号码 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isPhone(String str) {
        return Regular(str, PHONE);
    }

    /**
     * 判断是否为手机号码 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isMobile(String str) {
        return RegularSJHM(str, MOBILE);
    }

    /**
     * 判断是否为Url 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isUrl(String str) {
        return Regular(str, URL);
    }

    /**
     * 判断字段是否为数字 正负整数 正负浮点数 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isNumber(String str) {
        return Regular(str, DOUBLE);
    }

    /**
     * 判断字段是否为INTEGER 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isInteger(String str) {
        return Regular(str, INTEGER);
    }

    /**
     * 判断字段是否为正整数正则表达式 &gt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isINTEGER_NEGATIVE(String str) {
        return Regular(str, INTEGER_NEGATIVE);
    }

    /**
     * 判断字段是否为负整数正则表达式 &lt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isINTEGER_POSITIVE(String str) {
        return Regular(str, INTEGER_POSITIVE);
    }

    /**
     * 判断字段是否为DOUBLE 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDouble(String str) {
        return Regular(str, DOUBLE);
    }

    /**
     * 判断字段是否为正浮点数正则表达式 &gt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDOUBLE_NEGATIVE(String str) {
        return Regular(str, DOUBLE_NEGATIVE);
    }

    /**
     * 判断字段是否为负浮点数正则表达式 &lt;=0 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDOUBLE_POSITIVE(String str) {
        return Regular(str, DOUBLE_POSITIVE);
    }

    /**
     * 判断字段是否为日期 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isDate(String str) {
        return Regular(str, DATE_ALL);
    }

    /**
     * 验证
     * @param str
     * @return
     */
    public static boolean isDate1(String str) {
        return Regular(str, DATE_FORMAT1);
    }

    /**
     * 判断字段是否为年龄 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isAge(String str) {
        return Regular(str, AGE);
    }

    /**
     * 判断字段是否超长 字串为空返回fasle, 超过长度{leng}返回ture 反之返回false
     * @param str
     * @param leng
     * @return boolean
     */
    public static boolean isLengOut(String str, int leng) {
        return StrisNull(str) ? false : str.trim().length() &gt; leng;
    }

    /**
     * 判断字段是否为身份证 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isIdCard(String str) {
        if (StrisNull(str))
            return false;
        if (str.trim().length() == 15 || str.trim().length() == 18) {
            return Regular(str, IDCARD);
        } else {
            return false;
        }
    }

    /**
     * 判断字段是否为邮编 符合返回ture
     * @param str
     * @return boolean
     */
    public static boolean isCode(String str) {
        return Regular(str, CODE);
    }

    /**
     * 判断字符串是不是全部是英文字母
     * @param str
     * @return boolean
     */
    public static boolean isEnglish(String str) {
        return Regular(str, STR_ENG);
    }

    /**
     * 判断字符串是不是全部是英文字母+数字
     * @param str
     * @return boolean
     */
    public static boolean isENG_NUM(String str) {
        return Regular(str, STR_ENG_NUM);
    }

    /**
     * 判断字符串是不是全部是英文字母+数字+下划线
     * @param str
     * @return boolean
     */
    public static boolean isENG_NUM_(String str) {
        return Regular(str, STR_ENG_NUM_);
    }

    /**
     * 过滤特殊字符串 返回过滤后的字符串
     * @param str
     * @return boolean
     */
    public static String filterStr(String str) {
        Pattern p = Pattern.compile(STR_SPECIAL);
        Matcher m = p.matcher(str);
        return m.replaceAll(&quot;&quot;).trim();
    }

    /**
     * 校验机构代码格式
     * @return
     */
    public static boolean isJigouCode(String str) {
        return Regular(str, JIGOU_CODE);
    }

    /**
     * 判断字符串是不是数字组成
     * @param str
     * @return boolean
     */
    public static boolean isSTR_NUM(String str) {
        return Regular(str, STR_NUM);
    }

    /**
     * 匹配是否符合正则表达式pattern 匹配返回true
     * @param str 匹配的字符串
     * @param pattern 匹配模式
     * @return boolean
     */
    private static boolean Regular(String str, String pattern) {
        if (null == str || str.trim().length() &lt;= 0)
            return false;
        Pattern p = Pattern.compile(pattern);
        Matcher m = p.matcher(str);
        return m.matches();
    }

    /**
     * 匹配是否符合正则表达式pattern 匹配返回true
     * @param str 匹配的字符串
     * @param pattern 匹配模式
     * @return boolean
     */
    private static boolean RegularSJHM(String str, String pattern) {
        if (null == str || str.trim().length() &lt;= 0){
            return false;
        }
        if(str.contains(&quot;+86&quot;)){
            str=str.replace(&quot;+86&quot;,&quot;&quot;);
        }
        Pattern p = Pattern.compile(pattern);
        Matcher m = p.matcher(str);
        return m.matches();
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean
     */
    public static final String yyyyMMddHHmmss = &quot;[0-9]{14}&quot;;

    public static boolean isyyyyMMddHHmmss(String time) {
        if (time == null) {
            return false;
        }
        boolean bool = time.matches(yyyyMMddHHmmss);
        return bool;
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean
     */
    public static final String isMac = &quot;^[A-Fa-f0-9]{2}(-[A-Fa-f0-9]{2}){5}$&quot;;

    public static boolean isMac(String mac) {
        if (mac == null) {
            return false;
        }
        boolean bool = mac.matches(isMac);
        return bool;
    }

    /**
     * description:匹配yyyyMMddHHmmss格式时间
     * @param time
     * @return boolean
     */
    public static final String longtime = &quot;[0-9]{10}&quot;;

    public static boolean isTimestamp(String timestamp) {
        if (timestamp == null) {
            return false;
        }
        boolean bool = timestamp.matches(longtime);
        return bool;
    }

    /**
     * 判断字段是否为datatype 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String DATATYPE = &quot;^\\d{7}$&quot;;
    public static boolean isDATATYPE(String str) {
        return Regular(str, DATATYPE);
    }

    /**
     * 判断字段是否为QQ 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String QQ = &quot;^\\d{5,15}$&quot;;
    public static boolean isQQ(String str) {
        return Regular(str, QQ);
    }


    /**
     * 判断字段是否为IMSI 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String IMSI = &quot;^4600[0,1,2,3,4,5,6,7,9]\\d{10}|(46011|46020)\\d{10}$&quot;;
    public static boolean isIMSI(String str) {
        return Regular(str, IMSI);
    }

    /**
     * 判断字段是否为IMEI 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String IMEI = &quot;^\\d{8}$|^[a-fA-F0-9]{14}$|^\\d{15}$&quot;;
    public static boolean isIMEI(String str) {return Regular(str, IMEI);}

    /**
     * 判断字段是否为CAPTURETIME 符合返回ture
     * @param str
     * @return boolean
     */
    public static final String CAPTURETIME = &quot;^\\d{10}|(20[0-9][0-9])\\d{10}$&quot;;
    public static boolean isCAPTURETIME(String str) {return Regular(str, CAPTURETIME);}

    /**
     * description:检测认证类型
     * @param auth
     * @return boolean
     */
    public static final String AUTH_TYPE = &quot;^\\d{7}$&quot;;
    public static boolean isAUTH_TYPE(String str) {return Regular(str, CAPTURETIME);}


    /**
     * description:检测FIRM_CODE
     * @param auth
     * @return boolean
     */
    public static final String FIRM_CODE = &quot;^\\d{9}$&quot;;
    public static boolean isFIRM_CODE(String str) {return Regular(str, FIRM_CODE);}

    /**
     * description:检测经度
     * @param auth
     * @return boolean
     */
    public static final String LONGITUDE = &quot;^-?(([1-9]\\d?)|(1[0-7]\\d)|180)(\\.\\d{1,6})?$&quot;;


    //public static final String LONGITUDE =&quot;^([-]?(\\d|([1-9]\\d)|(1[0-7]\\d)|(180))(\\.\\d*)\\,[-]?(\\d|([1-8]\\d)|(90))(\\.\\d*))$&quot;;
    public static boolean isLONGITUDE(String str) {return Regular(str, LONGITUDE);}

    /**
     * description:检测纬度
     *
     * @param auth
     * @return boolean 2016-7-19 下午4:50:06 by 
     */
    public static final String LATITUDE = &quot;^-?(([1-8]\\d?)|([1-8]\\d)|90)(\\.\\d{1,6})?$&quot;;
    public static boolean isLATITUDE(String str) {return Regular(str, LATITUDE);}

    public static void main(String[] args) {
        boolean bool = isLATITUDE(&quot;25.546685&quot;);
        System.out.println(bool);
    }
}</code></pre><h4 id="5、constant常量"><a href="#5、constant常量" class="headerlink" title="5、constant常量"></a>5、constant常量</h4><p><strong>constant/FlumeConfConstant.java</strong></p>
<pre><code>package com.hsiehchou.flume.constant;

public class FlumeConfConstant {

    //flumeSource配置
    public static final String UNMANAGE=&quot;unmanage&quot;;
    public static final String DIRS=&quot;dirs&quot;;
    public static final String SUCCESSFILE=&quot;successfile&quot;;
    public static final String ALL=&quot;all&quot;;
    public static final String SOURCE=&quot;source&quot;;
    public static final String FILENUM=&quot;filenum&quot;;
    public static final String SLEEPTIME=&quot;sleeptime&quot;;

    //ESSINK配置
    public static final String TIMECELL=&quot;timecell&quot;;
    public static final String MAXNUM=&quot;maxnum&quot;;
    public static final String SINK_SOURCE=&quot;source&quot;;
    public static final String THREADNUM=&quot;threadnum&quot;;
    public static final String REDISHOST=&quot;redishost&quot;;
}</code></pre><p><strong>constant/TxtConstant.java</strong></p>
<pre><code>package com.hsiehchou.flume.constant;

public class TxtConstant {

    public static final String TYPE_ES=&quot;TYPE_ES&quot;;

    public static final String STATIONCENTER=&quot;STATIONCENTER&quot;;
    public static final String APCENTER=&quot;APCENTER&quot;;
    public static final String IPLOGINLOG=&quot;IPLOGINLOG&quot;;
    public static final String IMSIIMEI=&quot;IMSIIMEI&quot;;
    public static final String MACHOUR=&quot;MACHOUR&quot;;


    public static final String TYPE_SITEMANAGE=&quot;TYPE_SITEMANAGE&quot;;
    public static final String JZWA=&quot;JZWA&quot;;


    public static final String FIRMCODE=&quot;FIRMCODE&quot;;

    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;

    public static final String FILENAME_FIELDS2=&quot;FILENAME_FIELDS2&quot;;

    public static final String FILENAME_FIELDS3=&quot;FILENAME_FIELDS3&quot;;

    public static final String FILENAME_FIELDS4=&quot;FILENAME_FIELDS4&quot;;

    public static final String FILENAME_FIELDS5=&quot;FILENAME_FIELDS5&quot;;

    public static final String FILENAME_VALIDATION=&quot;FILENAME_VALIDATION&quot;;

    public static final String AUTHTYPE_LIST=&quot;AUTHTYPE_LIST&quot;;

    public static final String SOURCE_FEIJING=&quot;SOURCE_FEIJING&quot;;
    public static final String SOURCE_650=&quot;SOURCE_650&quot;;
    public static final String OFFICE_11=&quot;OFFICE_11&quot;;
    public static final String OFFICE_12=&quot;OFFICE_12&quot;;
    public static final String WLZK=&quot;WLZK&quot;;
    public static final String FEIJING=&quot;FEIJING&quot;;
    public static final String HLWZC=&quot;HLWZC&quot;;
    public static final String WIFIWL=&quot;WIFIWL&quot;;

    // 错误索引
    public static final String ERROR_INDEX=&quot;es.errorindex&quot;;
    public static final String ERROR_TYPE=&quot;es.errortype&quot;;

    //WIFI索引
    public static final String WIFILOG_INDEX=&quot;es.index.wifilog&quot;;
    public static final String IPLOGINLOG_TYPE=&quot;es.type.iploginlog&quot;;
    public static final String EMAIL_TYPE=&quot;es.type.email&quot;;
    public static final String FTP_TYPE=&quot;es.type.ftp&quot;;
    public static final String GAME_TYPE=&quot;es.type.game&quot;;
    public static final String HEARTBEAT_TYPE=&quot;es.type.heartbeat&quot;;
    public static final String HTTP_TYPE=&quot;es.type.http&quot;;
    public static final String IMINFO_TYPE=&quot;es.type.iminfo&quot;;
    public static final String ORGANIZATION_TYPE=&quot;es.type.organization&quot;;
    public static final String SEARCH_TYPE=&quot;es.type.search&quot;;
    public static final String IMSIIMEI_TYPE=&quot;es.type.imsiimei&quot;;
}</code></pre><h4 id="6、field字段"><a href="#6、field字段" class="headerlink" title="6、field字段"></a>6、field字段</h4><p><strong>field/ErrorMapFields.java</strong></p>
<pre><code>package com.hsiehchou.flume.fields;

public class ErrorMapFields {

    public static final String RKSJ=&quot;RKSJ&quot;;

    public static final String RECORD=&quot;RECORD&quot;;

    public static final String LENGTH=&quot;LENGTH&quot;;
    public static final String LENGTH_ERROR=&quot;LENGTH_ERROR&quot;;
    public static final String LENGTH_ERROR_NUM=&quot;10001&quot;;

    public static final String FILENAME=&quot;FILENAME&quot;;
    public static final String FILENAME_ERROR=&quot;FILENAME_ERROR&quot;;
    public static final String FILENAME_ERROR_NUM=&quot;10010&quot;;
    public static final String ABSOLUTE_FILENAME=&quot;ABSOLUTE_FILENAME&quot;;


    public static final String SJHM=&quot;SJHM&quot;;
    public static final String SJHM_ERROR=&quot;SJHM_ERROR&quot;;
    public static final String SJHM_ERRORCODE=&quot;10007&quot;;


    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;
    public static final String DATA_TYPE_ERROR=&quot;DATA_TYPE_ERROR&quot;;
    public static final String DATA_TYPE_ERRORCODE=&quot;10011&quot;;

    public static final String QQ=&quot;QQ&quot;;
    public static final String QQ_ERROR=&quot;QQ_ERROR&quot;;
    public static final String QQ_ERRORCODE=&quot;10002&quot;;

    public static final String IMSI=&quot;IMSI&quot;;
    public static final String IMSI_ERROR=&quot;IMSI_ERROR&quot;;
    public static final String IMSI_ERRORCODE=&quot;10005&quot;;

    public static final String IMEI=&quot;IMEI&quot;;
    public static final String IMEI_ERROR=&quot;IMEI_ERROR&quot;;
    public static final String IMEI_ERRORCODE=&quot;10006&quot;;

    public static final String MAC=&quot;MAC&quot;;
    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;
    public static final String STATIONMAC=&quot;STATIONMAC&quot;;
    public static final String BSSID=&quot;BSSID&quot;;
    public static final String MAC_ERROR=&quot;MAC_ERROR&quot;;
    public static final String MAC_ERRORCODE=&quot;10003&quot;;

    public static final String DEVICENUM=&quot;DEVICENUM&quot;;
    public static final String DEVICENUM_ERROR=&quot;DEVICENUM_ERROR&quot;;
    public static final String DEVICENUM_ERRORCODE=&quot;10014&quot;;

    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;
    public static final String CAPTURETIME_ERROR=&quot;CAPTURETIME_ERROR&quot;;
    public static final String CAPTURETIME_ERRORCODE=&quot;10019&quot;;

    public static final String EMAIL=&quot;EMAIL&quot;;
    public static final String EMAIL_ERROR=&quot;EMAIL_ERROR&quot;;
    public static final String EMAIL_ERRORCODE=&quot;10004&quot;;

    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;
    public static final String AUTH_TYPE_ERROR=&quot;AUTH_TYPE_ERROR&quot;;
    public static final String AUTH_TYPE_ERRORCODE=&quot;10020&quot;;

    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;
    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;
    public static final String FIRM_CODE_ERROR=&quot;FIRM_CODE_ERROR&quot;;
    public static final String FIRM_CODE_ERRORCODE=&quot;10009&quot;;

    public static final String STARTTIME=&quot;STARTTIME&quot;;
    public static final String STARTTIME_ERROR=&quot;STARTTIME_ERROR&quot;;
    public static final String STARTTIME_ERRORCODE=&quot;10015&quot;;
    public static final String ENDTIME=&quot;ENDTIME&quot;;
    public static final String ENDTIME_ERROR=&quot;ENDTIME_ERROR&quot;;
    public static final String ENDTIME_ERRORCODE=&quot;10016&quot;;

    public static final String LOGINTIME=&quot;LOGINTIME&quot;;
    public static final String LOGINTIME_ERROR=&quot;LOGINTIME_ERROR&quot;;
    public static final String LOGINTIME_ERRORCODE=&quot;10017&quot;;
    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;
    public static final String LOGOUTTIME_ERROR=&quot;LOGOUTTIME_ERROR&quot;;
    public static final String LOGOUTTIME_ERRORCODE=&quot;10018&quot;;

    public static final String LONGITUDE=&quot;LONGITUDE&quot;;
    public static final String LONGITUDE_ERROR=&quot;LONGITUDE_ERROR&quot;;
    public static final String LONGITUDE_ERRORCODE=&quot;10012&quot;;
    public static final String LATITUDE=&quot;LATITUDE&quot;;
    public static final String LATITUDE_ERROR=&quot;LATITUDE_ERROR&quot;;
    public static final String LATITUDE_ERRORCODE=&quot;10013&quot;;

    //TODO 其他类型DATA_TYPE  记录
    public static final String DATA_TYPE_OTHER=&quot;DATA_TYPE_OTHER&quot;;
    public static final String DATA_TYPE_OTHER_ERROR=&quot;DATA_TYPE_OTHER_ERROR&quot;;
    public static final String DATA_TYPE_OTHER_ERRORCODE=&quot;10022&quot;;

    //TODO USERNAME 错误
    public static final String USERNAME=&quot;USERNAME&quot;;
    public static final String USERNAME_ERROR=&quot;USERNAME_ERROR&quot;;
    public static final String USERNAME_ERRORCODE=&quot;10023&quot;;
}</code></pre><p><strong>field/MapFields.java</strong></p>
<pre><code>package com.hsiehchou.flume.fields;

public class MapFields {

    public static final String ID=&quot;id&quot;;
    public static final String SOURCE=&quot;source&quot;;
    public static final String TYPE=&quot;TYPE&quot;;
    public static final String TABLE=&quot;table&quot;;
    public static final String FILENAME=&quot;filename&quot;;
    public static final String RKSJ=&quot;rksj&quot;;
    public static final String ABSOLUTE_FILENAME=&quot;absolute_filename&quot;;
    public static final String BSSID=&quot;BSSID&quot;;
    public static final String USERNAME=&quot;USERNAME&quot;;
    public static final String DAYID=&quot;DAYID&quot;;

    public static final String FIRMCODE_NUM=&quot;FIRMCODE_NUM&quot;;
    public static final String FIRM_CODE=&quot;FIRM_CODE&quot;;
    public static final String IMEI=&quot;IMEI&quot;;
    public static final String IMSI=&quot;IMSI&quot;;

    public static final String DATA_TYPE_NAME=&quot;DATA_TYPE_NAME&quot;;

    public static final String AUTH_TYPE=&quot;AUTH_TYPE&quot;;
    public static final String AUTH_ACCOUNT=&quot;AUTH_ACCOUNT&quot;;

    //TODO 时间类参数
    public static final String CAPTURETIME=&quot;CAPTURETIME&quot;;
    public static final String LOGINTIME=&quot;LOGINTIME&quot;;
    public static final String LOGOUTTIME=&quot;LOGOUTTIME&quot;;
    public static final String STARTTIME=&quot;STARTTIME&quot;;
    public static final String ENDTIME=&quot;ENDTIME&quot;;
    public static final String FIRSTTIME=&quot;FIRSTTIME&quot;;
    public static final String LASTTIME=&quot;LASTTIME&quot;;

    //TODO 去重参数
    public static final String COUNT=&quot;COUNT&quot;;
    public static final String DATA_TYPE=&quot;DATA_TYPE&quot;;
    public static final String VALUE=&quot;value&quot;;
    public static final String SITECODE=&quot;SITECODE&quot;;
    public static final String SITECODENEW=&quot;SITECODENEW&quot;;

    public static final String DEVICENUM=&quot;DEVICENUM&quot;;
    public static final String MAC=&quot;MAC&quot;;
    public static final String CLIENTMAC=&quot;CLIENTMAC&quot;;
    public static final String STATIONMAC=&quot;STATIONMAC&quot;;

    public static final String BRAND=&quot;BRAND&quot;;
    public static final String INDEX=&quot;INDEX&quot;;
    public static final String ACTION_TYPE=&quot;ACTION_TYPE&quot;;


    public static final String CITY_CODE=&quot;CITY_CODE&quot;;
    /* public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;
    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;
    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;
    public static final String FILENAME_FIELDS1=&quot;FILENAME_FIELDS1&quot;;*/

}</code></pre><h4 id="7、自定义sink"><a href="#7、自定义sink" class="headerlink" title="7、自定义sink"></a>7、自定义sink</h4><p><strong>sink/KafkaSink.java—将数据下沉到kafka</strong></p>
<pre><code>package com.hsiehchou.flume.sink;

import com.google.common.base.Throwables;
import com.hsiehchou.kafka.producer.StringProducer;
import org.apache.flume.*;
import org.apache.flume.conf.Configurable;
import org.apache.flume.sink.AbstractSink;
import org.apache.log4j.Logger;

import java.util.ArrayList;
import java.util.List;

public class KafkaSink extends AbstractSink implements Configurable {

    private final Logger logger = Logger.getLogger(KafkaSink.class);
    private String[] kafkatopics = null;
    //private List&lt;KeyedMessage&lt;String,String&gt;&gt; listKeyedMessage=null;
    private List&lt;String&gt; listKeyedMessage=null;
    private Long proTimestamp=System.currentTimeMillis();

    /**
     * 配置读取
     * @param context
     */
    @Override
    public void configure(Context context) {
        //tier1.sinks.sink1.kafkatopic=chl_test7
        //获取 推送kafkatopic参数
        kafkatopics = context.getString(&quot;kafkatopics&quot;).split(&quot;,&quot;);
        logger.info(&quot;获取kafka topic配置&quot; + context.getString(&quot;kafkatopics&quot;));
        listKeyedMessage=new ArrayList&lt;&gt;();
    }

    @Override
    public Status process() throws EventDeliveryException {

        logger.info(&quot;sink开始执行&quot;);
        Channel channel = getChannel();
        Transaction transaction = channel.getTransaction();
        transaction.begin();
        try {
            //从channel中拿到event
            Event event = channel.take();
            if (event == null) {
                transaction.rollback();
                return Status.BACKOFF;
            }
            // 解析记录 获取事件内容
            String recourd = new String(event.getBody());
            // 发送数据到kafka
            try {
                //调用kafka的消息推送，将数据推送到kafka
                StringProducer.producer(kafkatopics[0],recourd);
            /*    if(listKeyedMessage.size()&gt;1000){
                    logger.info(&quot;数据大与10000,推送数据到kafka&quot;);
                    sendListKeyedMessage();
                    logger.info(&quot;数据大与10000,推送数据到kafka成功&quot;);
                }else if(System.currentTimeMillis()-proTimestamp&gt;=60*1000){
                    logger.info(&quot;时间间隔大与60,推送数据到kafka&quot;);
                    sendListKeyedMessage();
                    logger.info(&quot;时间间隔大与60,推送数据到kafka成功&quot;+listKeyedMessage.size());
                }*/
            } catch (Exception e) {
                logger.error(&quot;推送数据到kafka失败&quot; , e);
                throw Throwables.propagate(e);
            }
            transaction.commit();
            return Status.READY;
        } catch (ChannelException e) {
            logger.error(e);
            transaction.rollback();
            return Status.BACKOFF;
        } finally {
            if(transaction != null){
                transaction.close();
            }
        }
    }

    @Override
    public synchronized void stop() {
        super.stop();
    }

    /*private void sendListKeyedMessage(){
        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());
        producer.send(listKeyedMessage);
        listKeyedMessage.clear();
        proTimestamp=System.currentTimeMillis();
        producer.close();
    }*/
}</code></pre><h4 id="8、service"><a href="#8、service" class="headerlink" title="8、service"></a>8、service</h4><p><strong>DataCheck.java—数据校验</strong></p>
<pre><code>package com.hsiehchou.flume.service;

import com.alibaba.fastjson.JSON;
import com.hsiehchou.common.net.HttpRequest;
import com.hsiehchou.common.project.datatype.DataTypeProperties;
import com.hsiehchou.common.time.TimeTranstationUtils;
import com.hsiehchou.flume.fields.ErrorMapFields;
import com.hsiehchou.flume.fields.MapFields;
import org.apache.log4j.Logger;

import java.util.*;

/**
 * 数据校验
 */
public class DataCheck {

    private final static Logger LOG = Logger.getLogger(DataCheck.class);

    /**
     * 获取数据类型对应的字段  对应的文件
     * 结构为 [ 数据类型1 = [字段1，字段2。。。。]，
     * 数据类型2 = [字段1，字段2。。。。]]
     */
    private static Map&lt;String, ArrayList&lt;String&gt;&gt; dataMap = DataTypeProperties.dataTypeMap;

    /**
     * 数据解析
     * @param line
     * @param fileName
     * @param absoluteFilename
     * @return
     */
    public static Map&lt;String, String&gt; txtParse(String line, String fileName, String absoluteFilename) {

        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
        String[] fileNames = fileName.split(&quot;_&quot;);
        String dataType = fileNames[0];

        if (dataMap.containsKey(dataType)) {
            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());
            String[] splits = line.split(&quot;\t&quot;);
            //长度校验
            if (fields.size() == splits.length) {
                //添加公共字段
                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));
                map.put(MapFields.TABLE, dataType.toLowerCase());
                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);
                map.put(MapFields.FILENAME, fileName);
                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);
                for (int i = 0; i &lt; splits.length; i++) {
                    map.put(fields.get(i), splits[i]);
                }
            } else {
                map = null;
                LOG.error(&quot;字段长度不匹配fields&quot;+fields.size()  + &quot;/t&quot; + splits.length);
            }
        } else {
            map = null;
            LOG.error(&quot;配置文件中不存在此数据类型&quot;);
        }
        return map;
    }

    /**
     * 数据长度校验添加必要字段并转map，将长度不符合的插入ES数据库
     * @param line
     * @param fileName
     * @param absoluteFilename
     * @return
     */
    public static Map&lt;String, String&gt; txtParseAndalidation(String line, String fileName, String absoluteFilename) {

        Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
        Map&lt;String, Object&gt; errorMap = new HashMap&lt;String, Object&gt;();

        //文件名按&quot;_&quot;切分  wechat_source1_1111142.txt
        //wechat 数据类型
        //source1 数据来源
        //1111142  不让文件名相同
        String[] fileNames = fileName.split(&quot;_&quot;);
        String dataType = fileNames[0];
        String source = fileNames[1];

        if (dataMap.containsKey(dataType)) {
            //获取数据类型字段
            // imei,imsi,longitude,latitude,phone_mac,device_mac,device_number,collect_time,username,phone,object_username,send_message,accept_message,message_time
            //根据数据类型，获取改类型的字段
            List&lt;String&gt; fields = dataMap.get(dataType.toLowerCase());
            //line
            String[] splits = line.split(&quot;\t&quot;);

            //长度校验
            if (fields.size() == splits.length) {
                for (int i = 0; i &lt; splits.length; i++) {
                    map.put(fields.get(i), splits[i]);
                }
                //添加公共字段
                // map.put(SOURCE, source);
                map.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));
                map.put(MapFields.TABLE, dataType.toLowerCase());
                map.put(MapFields.RKSJ, (System.currentTimeMillis() / 1000) + &quot;&quot;);
                map.put(MapFields.FILENAME, fileName);
                map.put(MapFields.ABSOLUTE_FILENAME, absoluteFilename);

                //数据封装完成  开始进行数据校验
                errorMap = DataValidation.dataValidation(map);
            } else {
                errorMap.put(ErrorMapFields.LENGTH, &quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);
                errorMap.put(ErrorMapFields.LENGTH_ERROR, ErrorMapFields.LENGTH_ERROR_NUM);
                LOG.info(&quot;字段数不匹配 实际&quot; + fields.size() + &quot;\t&quot; + &quot;结果&quot; + splits.length);
                map = null;
            }

            //判断数据是否存在错误
            if (null != errorMap &amp;&amp; errorMap.size() &gt; 0) {
                LOG.info(&quot;errorMap===&quot; + errorMap);
                if (&quot;1&quot;.equals(&quot;1&quot;)) {
                    //addErrorMapES(errorMap, map, fileName, absoluteFilename);
                    //验证没通过，将错误数据写到ES，并将map置空
                    addErrorMapESByHTTP(errorMap, map, fileName, absoluteFilename);
                }
                map = null;
            }
        } else {
            map = null;
            LOG.error(&quot;配置文件中不存在此数据类型&quot;);
        }
        return map;
    }

    /**
     *  将错误信息写入ES，方便查错
     * @param errorMap
     * @param map
     * @param fileName
     * @param absoluteFilename
     */
    public static void addErrorMapESByHTTP(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {

        String errorType = fileName.split(&quot;_&quot;)[0];
        errorMap.put(MapFields.TABLE, errorType);
        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));
        errorMap.put(ErrorMapFields.RECORD, map);
        errorMap.put(ErrorMapFields.FILENAME, fileName);
        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);
        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());
        String url=&quot;http://192.168.116.201:9200/error_recourd/error_recourd/&quot;+ errorMap.get(MapFields.ID).toString();
        String json = JSON.toJSONString(errorMap);
        HttpRequest.sendPost(url,json);
        //HttpRequest.sendPostMessage(url, errorMap);
    }

    /*
    public static void addErrorMapES(Map&lt;String, Object&gt; errorMap, Map&lt;String, String&gt; map, String fileName, String absoluteFilename) {

        String errorType = fileName.split(&quot;_&quot;)[0];
        errorMap.put(MapFields.TABLE, errorType);
        errorMap.put(MapFields.ID, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;));
        errorMap.put(ErrorMapFields.RECORD, map);
        errorMap.put(ErrorMapFields.FILENAME, fileName);
        errorMap.put(ErrorMapFields.ABSOLUTE_FILENAME, absoluteFilename);
        errorMap.put(ErrorMapFields.RKSJ, TimeTranstationUtils.Date2yyyy_MM_dd_HH_mm_ss());


        TransportClient client = null;
        try {
            LOG.info(&quot;开始获取客户端===============================&quot; + errorMap);
            client = ESClientUtils.getClient();
        } catch (Throwable t) {
            if (t instanceof Error) {
                throw (Error)t;
            }
            LOG.error(null,t);
        }
        //JestClient jestClient = JestService.getJestClient();
        //boolean bool = JestService.indexOne(jestClient,TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE,errorMap.get(MapFields.ID).toString(),errorMap);
        LOG.info(&quot;开始写入错误数据到ES===============================&quot; + errorMap);
        boolean bool = IndexUtil.putIndexData(TxtConstant.ERROR_INDEX, TxtConstant.ERROR_TYPE, errorMap.get(MapFields.ID).toString(), errorMap,client);
        if(bool){
            LOG.info(&quot;写入错误数据到ES===============================&quot; + errorMap);
        }else{
            LOG.info(&quot;写入错误数据到ES===============================失败&quot;);
        }

    }*/

    public static void main(String[] args) {

    }
}</code></pre><p><strong>DataValidation.java</strong></p>
<pre><code>package com.hsiehchou.flume.service;

import com.hsiehchou.flume.fields.ErrorMapFields;
import com.hsiehchou.flume.fields.MapFields;
import com.hsiehchou.flume.utils.Validation;
import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class DataValidation {
    private static final Logger LOG = LoggerFactory.getLogger(DataValidation.class);

   //  private static final TxtConfigurationFileReader reader = TxtConfigurationFileReader.getInstance();
   //  private static final DataTypeConfigurationFileReader datatypereader = DataTypeConfigurationFileReader.getInstance();
   //  private static final ValidationConfigurationFileReader readerValidation = ValidationConfigurationFileReader.getInstance();

    private static Map&lt;String,String&gt;  dataTypeMap;
    private static List&lt;String&gt; listAuthType;
    private static String isErrorES;
    private static final String USERNAME=ErrorMapFields.USERNAME;

    private static final String DATA_TYPE=ErrorMapFields.DATA_TYPE;
    private static final String DATA_TYPE_ERROR=ErrorMapFields.DATA_TYPE_ERROR;
    private static final String DATA_TYPE_ERRORCODE=ErrorMapFields.DATA_TYPE_ERRORCODE;

    private static final String SJHM=ErrorMapFields.SJHM;
    private static final String SJHM_ERROR=ErrorMapFields.SJHM_ERROR;
    private static final String SJHM_ERRORCODE=ErrorMapFields.SJHM_ERRORCODE;

    private static final String QQ=ErrorMapFields.QQ;
    private static final String QQ_ERROR=ErrorMapFields.QQ_ERROR;
    private static final String QQ_ERRORCODE=ErrorMapFields.QQ_ERRORCODE;

    private static final String IMSI=ErrorMapFields.IMSI;
    private static final String IMSI_ERROR=ErrorMapFields.IMSI_ERROR;
    private static final String IMSI_ERRORCODE=ErrorMapFields.IMSI_ERRORCODE;

    private static final String IMEI=ErrorMapFields.IMEI;
    private static final String IMEI_ERROR=ErrorMapFields.IMEI_ERROR;
    private static final String IMEI_ERRORCODE=ErrorMapFields.IMEI_ERRORCODE;

    private static final String MAC=ErrorMapFields.MAC;
    private static final String CLIENTMAC=ErrorMapFields.CLIENTMAC;
    private static final String STATIONMAC=ErrorMapFields.STATIONMAC;
    private static final String BSSID=ErrorMapFields.BSSID;
    private static final String MAC_ERROR=ErrorMapFields.MAC_ERROR;
    private static final String MAC_ERRORCODE=ErrorMapFields.MAC_ERRORCODE;

    private static final String DEVICENUM=ErrorMapFields.DEVICENUM;
    private static final String DEVICENUM_ERROR=ErrorMapFields.DEVICENUM_ERROR;
    private static final String DEVICENUM_ERRORCODE=ErrorMapFields.DEVICENUM_ERRORCODE;

    private static final String CAPTURETIME=ErrorMapFields.CAPTURETIME;
    private static final String CAPTURETIME_ERROR=ErrorMapFields.CAPTURETIME_ERROR;
    private static final String CAPTURETIME_ERRORCODE=ErrorMapFields.CAPTURETIME_ERRORCODE;

    private static final String EMAIL=ErrorMapFields.EMAIL;
    private static final String EMAIL_ERROR=ErrorMapFields.EMAIL_ERROR;
    private static final String EMAIL_ERRORCODE=ErrorMapFields.EMAIL_ERRORCODE;

    private static final String AUTH_TYPE=ErrorMapFields.AUTH_TYPE;
    private static final String AUTH_TYPE_ERROR=ErrorMapFields.AUTH_TYPE_ERROR;
    private static final String AUTH_TYPE_ERRORCODE=ErrorMapFields.AUTH_TYPE_ERRORCODE;

    private static final String FIRM_CODE=ErrorMapFields.FIRM_CODE;
    private static final String FIRM_CODE_ERROR=ErrorMapFields.FIRM_CODE_ERROR;
    private static final String FIRM_CODE_ERRORCODE=ErrorMapFields.FIRM_CODE_ERRORCODE;

    private static final String STARTTIME=ErrorMapFields.STARTTIME;
    private static final String STARTTIME_ERROR=ErrorMapFields.STARTTIME_ERROR;
    private static final String STARTTIME_ERRORCODE=ErrorMapFields.STARTTIME_ERRORCODE;
    private static final String ENDTIME=ErrorMapFields.ENDTIME;
    private static final String ENDTIME_ERROR=ErrorMapFields.ENDTIME_ERROR;
    private static final String ENDTIME_ERRORCODE=ErrorMapFields.ENDTIME_ERRORCODE;

    private static final String LOGINTIME=ErrorMapFields.LOGINTIME;
    private static final String LOGINTIME_ERROR=ErrorMapFields.LOGINTIME_ERROR;
    private static final String LOGINTIME_ERRORCODE=ErrorMapFields.LOGINTIME_ERRORCODE;
    private static final String LOGOUTTIME=ErrorMapFields.LOGOUTTIME;
    private static final String LOGOUTTIME_ERROR=ErrorMapFields.LOGOUTTIME_ERROR;
    private static final String LOGOUTTIME_ERRORCODE=ErrorMapFields.LOGOUTTIME_ERRORCODE;

    public static Map&lt;String, Object&gt; dataValidation( Map&lt;String, String&gt; map){
        if(map == null){
            return null;
        }

        Map&lt;String,Object&gt; errorMap = new HashMap&lt;String,Object&gt;();
        //验证手机号码
        sjhmValidation(map,errorMap);
        //验证MAC
        macValidation(map,errorMap);
        //验证经纬度
        longlaitValidation(map,errorMap);

        //定义自己的清洗规则

        //TODO 大小写统一
        //TODO 时间类型统一
        //TODO 数据字段统一
        //TODO 业务字段转换
        //TODO 数据矫正
        //TODO 验证MAC不能为空
        //TODO 验证IMSI不能为空
        //TODO 验证 QQ IMSI IMEI
        //TODO 验证DEVICENUM是否为空 为空返回错误
        /*devicenumValidation(map,errorMap);
        //TODO 验证CAPTURETIME是否为空 为空过滤   不为10，14位数字过滤
        capturetimeValidation(map,errorMap);
        //TODO 验证EMAIL
        emailValidation(map,errorMap);
        //TODO 验证STARTTIME ENDTIME LOGINTIME LOGOUTTIME
        timeValidation(map,errorMap);
        */
        return errorMap;
    }

    /**
     * 手机号码验证
     * @param map
     * @param errorMap
     */
    public static void sjhmValidation(Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map.containsKey(&quot;phone&quot;)){
            String sjhm=map.get(&quot;phone&quot;);
            //调用正则做手机号码验证，是否是正确的一个，检验
            boolean ismobile = Validation.isMobile(sjhm);
            if(!ismobile){
                errorMap.put(SJHM,sjhm);
                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);
            }
        }
    }

    //TODO QQ验证  10002  QQ编码 1030001    需要根据DATATYPE来判断数据类型的一起验证
    public static void virtualValidation(String dataType, Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){

        //TODO USERNAME验证  10023  长度》=2
        if(map.containsKey(ErrorMapFields.USERNAME)){
            String username=map.get(ErrorMapFields.USERNAME);
            if(StringUtils.isNotBlank(username)){
                if(username.length()&lt;2){
                    errorMap.put(ErrorMapFields.USERNAME,username);
                    errorMap.put(ErrorMapFields.USERNAME_ERROR,ErrorMapFields.USERNAME_ERRORCODE);
                }
            }
        }

        //TODO QQ验证  10002  QQ编码 1030001
        if(&quot;1030001&quot;.equals(dataType)&amp;&amp; map.containsKey(USERNAME)){
            String qqnum= map.get(USERNAME);
            boolean bool = Validation.isQQ(qqnum);
            if(!bool){
                errorMap.put(QQ,qqnum);
                errorMap.put(QQ_ERROR,QQ_ERRORCODE);
            }
        }

        //TODO IMSI验证  10005  IMSI编码 1429997
        if(&quot;1429997&quot;.equals(dataType)&amp;&amp; map.containsKey(IMSI)){
            String imsi= map.get(IMSI);
            boolean bool = Validation.isIMSI(imsi);
            if(!bool){
                errorMap.put(IMSI,imsi);
                errorMap.put(IMSI_ERROR,IMSI_ERRORCODE);
            }
        }

        //TODO IMEI验证  10006  IMEI编码 1429998
        if(&quot;1429998&quot;.equals(dataType)&amp;&amp; map.containsKey(IMEI)){
            String imei= map.get(IMEI);
            boolean bool = Validation.isIMEI(imei);
            if(!bool){
                errorMap.put(IMEI,imei);
                errorMap.put(IMEI_ERROR,IMEI_ERRORCODE);
            }
        }
    }

    //MAC验证  10003
    public static void macValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.containsKey(&quot;phone_mac&quot;)){
            String mac=map.get(&quot;phone_mac&quot;);
            if(StringUtils.isNotBlank(mac)){
                boolean bool = Validation.isMac(mac);
                if(!bool){
                    LOG.info(&quot;MAC验证失败&quot;);
                    errorMap.put(MAC,mac);
                    errorMap.put(MAC_ERROR,MAC_ERRORCODE);
                }
            }else{
                LOG.info(&quot;MAC验证失败&quot;);
                errorMap.put(MAC,mac);
                errorMap.put(MAC_ERROR,MAC_ERRORCODE);
            }
        }
    }

    /**
     * TODO DEVICENUM 验证 为空过滤
     * @param map
     * @param errorMap
     */
    public static void devicenumValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.containsKey(&quot;device_number&quot;)){
            String devicenum=map.get(&quot;device_number&quot;);
            if(StringUtils.isBlank(devicenum)){
                errorMap.put(DEVICENUM,&quot;设备编码不能为空&quot;);
                errorMap.put(DEVICENUM_ERROR,DEVICENUM_ERRORCODE);
            }
        }
    }

    /**
     * TODO CAPTURETIME验证 为空过滤  10019  验证时间长度为10或14位
     * @param map
     * @param errorMap
     */
    public static void capturetimeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.containsKey(CAPTURETIME)){
            String capturetime=map.get(CAPTURETIME);
            if(StringUtils.isBlank(capturetime)){
                errorMap.put(CAPTURETIME,&quot;CAPTURETIME不能为空&quot;);
                errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);
            }else{
                boolean bool = Validation.isCAPTURETIME(capturetime);
                if(!bool){
                    errorMap.put(CAPTURETIME,capturetime);
                    errorMap.put(CAPTURETIME_ERROR,CAPTURETIME_ERRORCODE);
                }
            }
        }
    }

    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证
    public static void emailValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.get(&quot;TABLE&quot;).equals(EMAIL)){
            String email=map.get(USERNAME);
            if(StringUtils.isNotBlank(email)){
                boolean bool = Validation.isEmail(email);
                if(!bool){
                    errorMap.put(EMAIL,email);
                    errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);
                }
            }else{
                errorMap.put(EMAIL,&quot;EMAIL不能为空&quot;);
                errorMap.put(EMAIL_ERROR,EMAIL_ERRORCODE);
            }
        }
    }

    //TODO EMAIL验证 为空过滤 为错误过滤  10004  通过TABLE取USERNAME验证
    public static void timeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.containsKey(STARTTIME)&amp;&amp;map.containsKey(ENDTIME)){
            String starttime=map.get(STARTTIME);
            String endtime=map.get(ENDTIME);
            if(StringUtils.isBlank(starttime)&amp;&amp;StringUtils.isBlank(endtime)){
                errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);
                errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);
                errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME不能同时为空&quot;);
                errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);
            }else{
                Boolean bool1 = istime(starttime, STARTTIME, STARTTIME_ERROR, STARTTIME_ERRORCODE, errorMap);
                Boolean bool2 = istime(endtime, ENDTIME, ENDTIME_ERROR, ENDTIME_ERRORCODE, errorMap);

                if(bool1&amp;&amp;bool2&amp;&amp;(starttime.length()!=endtime.length())){
                    errorMap.put(STARTTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);
                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);
                    errorMap.put(ENDTIME,&quot;STARTTIME和ENDTIME长度不等 STARTTIME：&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);
                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);
                }
                else if(bool1&amp;&amp;bool2&amp;&amp;(endtime.compareTo(starttime)&lt;0)){
                    errorMap.put(STARTTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);
                    errorMap.put(STARTTIME_ERROR,STARTTIME_ERRORCODE);
                    errorMap.put(ENDTIME,&quot;ENDTIME必须大于STARTTIME  STARTTIME:&quot;+starttime + &quot;\t&quot;+&quot;ENDTIME:&quot; + endtime);
                    errorMap.put(ENDTIME_ERROR,ENDTIME_ERRORCODE);
                }
            }

        }else if(map.containsKey(LOGINTIME)&amp;&amp;map.containsKey(LOGOUTTIME)){

            String logintime=map.get(LOGINTIME);
            String logouttime=map.get(LOGOUTTIME);

            if(StringUtils.isBlank(logintime)&amp;&amp;StringUtils.isBlank(logouttime)){
                errorMap.put(LOGINTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);
                errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);
                errorMap.put(LOGOUTTIME,&quot;LOGINTIME和LOGOUTTIME不能同时为空&quot;);
                errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);
            }else{
                Boolean bool1 = istime(logintime, LOGINTIME, LOGINTIME_ERROR, LOGINTIME_ERRORCODE, errorMap);
                Boolean bool2 = istime(logouttime, LOGOUTTIME, LOGOUTTIME_ERROR, LOGOUTTIME_ERRORCODE, errorMap);

                if(bool1&amp;&amp;bool2&amp;&amp;(logintime.length()!=logouttime.length())){
                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);
                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);
                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME LOGINTIME长度不等 LOGINTIME：&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);
                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);
                }
                else if(bool1&amp;&amp;bool2&amp;&amp;(logouttime.compareTo(logintime)&lt;0)){
                    errorMap.put(LOGINTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);
                    errorMap.put(LOGINTIME_ERROR,LOGINTIME_ERRORCODE);
                    errorMap.put(LOGOUTTIME,&quot;LOGOUTTIME必须大于LOGINTIME  LOGINTIME:&quot;+logintime + &quot;\t&quot;+&quot;LOGOUTTIME:&quot; + logouttime);
                    errorMap.put(LOGOUTTIME_ERROR,LOGOUTTIME_ERRORCODE);
                }
            }
        }
    }

    //TODO AUTH_TYPE验证  为空过滤 为错误过滤  10020
    public static void authtypeValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        String fileName=map.get(MapFields.FILENAME);

        if(fileName.split(&quot;_&quot;).length&lt;=2){
            map = null;
            return;
        }

        if(StringUtils.isNotBlank(fileName)){
            if(&quot;bh&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;wy&quot;.equals(fileName.split(&quot;_&quot;)[2])||&quot;yc&quot;.equals(fileName.split(&quot;_&quot;)[2])){
                return ;
            }else if(map.containsKey(AUTH_TYPE)){
                String authtype=map.get(AUTH_TYPE);
                if(StringUtils.isNotBlank(authtype)){
                    if(listAuthType.contains(authtype)){
                        if(&quot;1020004&quot;.equals(authtype)){
                            String sjhm=map.get(MapFields.AUTH_ACCOUNT);

                            boolean ismobile = Validation.isMobile(sjhm);
                            if(!ismobile){
                                errorMap.put(SJHM,sjhm);
                                errorMap.put(SJHM_ERROR,SJHM_ERRORCODE);
                            }
                        }
                        if(&quot;1020002&quot;.equals(authtype)){

                            String mac=map.get(MapFields.AUTH_ACCOUNT);
                            boolean ismac = Validation.isMac(mac);
                            if(!ismac){
                                errorMap.put(MAC,mac);
                                errorMap.put(MAC_ERROR,MAC_ERRORCODE);
                            }
                        }
                    }else{
                        errorMap.put(AUTH_TYPE,&quot;AUTHTYPE_LIST 影射里没有&quot;+ &quot;\t&quot;+ &quot;[&quot;+ authtype+&quot;]&quot;);
                        errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);
                    }
                }else{
                    errorMap.put(AUTH_TYPE,&quot;AUTH_TYPE 不能为空&quot;);
                    errorMap.put(AUTH_TYPE_ERROR,AUTH_TYPE_ERRORCODE);
                }
            }
        }
    }

    private static final String LONGITUDE = &quot;longitude&quot;;
    private static final String LATITUDE = &quot;latitude&quot;;
    private static final String LONGITUDE_ERROR=ErrorMapFields.LONGITUDE_ERROR;
    private static final String LONGITUDE_ERRORCODE=ErrorMapFields.LONGITUDE_ERRORCODE;
    private static final String LATITUDE_ERROR=ErrorMapFields.LATITUDE_ERROR;
    private static final String LATITUDE_ERRORCODE=ErrorMapFields.LATITUDE_ERRORCODE;

    /**
     * 经纬度验证  错误过滤  10012  10013
     * @param map
     * @param errorMap
     */
    public static void longlaitValidation( Map&lt;String, String&gt; map,Map&lt;String,Object&gt; errorMap){
        if(map == null){
            return ;
        }
        if(map.containsKey(LONGITUDE)&amp;&amp;map.containsKey(LATITUDE)){
            String longitude=map.get(LONGITUDE);
            String latitude=map.get(LATITUDE);

            boolean bool1 = Validation.isLONGITUDE(longitude);
            boolean bool2 = Validation.isLATITUDE(latitude);

            if(!bool1){
                errorMap.put(LONGITUDE,longitude);
                errorMap.put(LONGITUDE_ERROR,LONGITUDE_ERRORCODE);
            }
            if(!bool2){
                errorMap.put(LATITUDE,latitude);
                errorMap.put(LATITUDE_ERROR,LATITUDE_ERRORCODE);
            }
        }
    }

    public static Boolean istime(String time,String str1,String str2,String str3,Map&lt;String,Object&gt; errorMap){

        if(StringUtils.isNotBlank(time)){
            boolean bool = Validation.isCAPTURETIME(time);
            if(!bool){
                errorMap.put(str1,time);
                errorMap.put(str2,str3);
                return false;
            }
            return true;
        }
        return false;
    }
}</code></pre><h4 id="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"><a href="#9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应" class="headerlink" title="9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应"></a>9、配置CDH上的Agent文件—-跟FolderSource等里面读取配置文件相对应</h4><p><img src="/medias/Flume%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6.PNG" alt="Flume配置文件"></p>
<p><strong>Flume配置：</strong></p>
<pre><code>tier1.sources= source1
tier1.channels=channel1
tier1.sinks=sink1

#定义source1
tier1.sources.source1.type = com.hsiehchou.flume.source.FolderSource

#读取文件之后睡眠时间
tier1.sources.source1.sleeptime=5
tier1.sources.source1.filenum=3000
tier1.sources.source1.dirs =/usr/chl/data/filedir/
tier1.sources.source1.successfile=/usr/chl/data/filedir_successful/
tier1.sources.source1.deserializer.outputCharset=UTF-8
tier1.sources.source1.channels = channel1

# 定义拦截器1
tier1.sources.source1.interceptors=i1
tier1.sources.source1.interceptors.i1.type=com.hsiehchou.flume.interceptor.DataCleanInterceptor$Builder

#定义channel
tier1.channels.channel1.type = memory
tier1.channels.channel1.keep-alive= 300
tier1.channels.channel1.capacity = 1000000
tier1.channels.channel1.transactionCapacity = 5000
tier1.channels.channel1.byteCapacityBufferPercentage = 200
tier1.channels.channel1.byteCapacity = 80000

#定义sink1
tier1.sinks.sink1.type = com.hsiehchou.flume.sink.KafkaSink
tier1.sinks.sink1.kafkatopics = chl_test7
tier1.sinks.sink1.channel = channel1</code></pre><p><img src="/medias/ftp%E7%9B%91%E6%8E%A7%E6%96%87%E4%BB%B6.PNG" alt="ftp监控文件"></p>
<p><strong>flumesource 不断监控 ftp 文件目录，通过自定义拦截器拦截，然后推送到flumechannel，然后通过flumesink下沉到kafka</strong></p>
<h4 id="10、flume打包到服务器执行"><a href="#10、flume打包到服务器执行" class="headerlink" title="10、flume打包到服务器执行"></a>10、flume打包到服务器执行</h4><p><img src="/medias/flume%E6%8F%92%E4%BB%B6%E7%9B%AE%E5%BD%95.PNG" alt="flume插件目录"></p>
<p><strong>不能放在默认的/usr/lib/flume-ng/plugins.d下面</strong></p>
<p>mkdir -p /var/lib/flume-ng/plugins.d/chl/lib<br>mkdir -p /usr/chl/data/filedir/<br>mkdir -p /usr/chl/data/filedir_successful/</p>
<p><strong>要设置777，flume启动的时候是以flume权限启动的，所以要更改权限</strong><br><strong>chmod 777 /usr/chl/data/filedir/</strong></p>
<p>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p>
<p>kafka-topics –zookeeper hadoop1:2181 –list</p>
<p>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p>
<p>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning<br><img src="/medias/%E6%B6%88%E8%B4%B9kafka.PNG" alt="消费kafka"></p>
<h3 id="六、Kafka开发"><a href="#六、Kafka开发" class="headerlink" title="六、Kafka开发"></a>六、Kafka开发</h3><p><strong>xz_bigdata_kafka</strong></p>
<h4 id="1、pom-xml"><a href="#1、pom-xml" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_kafka&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_kafka&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;poi.version&gt;3.14&lt;/poi.version&gt;
        &lt;kafka.version&gt;0.9.0-kafka-2.0.2&lt;/kafka.version&gt;
        &lt;mysql.connector.version&gt;5.1.46&lt;/mysql.connector.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;
            &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;
            &lt;version&gt;${zookeeper.version}-${cdh.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka_2.10&lt;/artifactId&gt;
            &lt;version&gt;${kafka.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.poi&lt;/groupId&gt;
            &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt;
            &lt;version&gt;${poi.version}&lt;/version&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;artifactId&gt;scala-reflect&lt;/artifactId&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

&lt;/project&gt;</code></pre><h4 id="2、config-KafkaConfig-java—kafka配置文件-解析器"><a href="#2、config-KafkaConfig-java—kafka配置文件-解析器" class="headerlink" title="2、config/KafkaConfig.java—kafka配置文件 解析器"></a>2、config/KafkaConfig.java—kafka配置文件 解析器</h4><pre><code>package com.hsiehchou.kafka.config;

import com.hsiehchou.common.config.ConfigUtil;
import kafka.producer.ProducerConfig;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Properties;

/**
 * kafka配置文件 解析器
 */
public class KafkaConfig {

    private static final Logger LOG = LoggerFactory.getLogger(KafkaConfig.class);

    private static final String DEFUALT_CONFIG_PATH = &quot;kafka/kafka-server-config.properties&quot;;

    private volatile static KafkaConfig kafkaConfig = null;

    private ProducerConfig config;
    private Properties properties;

    private KafkaConfig() throws IOException{
        try {
            properties = ConfigUtil.getInstance().getProperties(DEFUALT_CONFIG_PATH);
        } catch (Exception e) {
            IOException ioException = new IOException();
            ioException.addSuppressed(e);
            throw ioException;
        }
        config = new ProducerConfig(properties);
    }

    public static KafkaConfig getInstance(){
        if(kafkaConfig == null){
            synchronized (KafkaConfig.class) {
                if(kafkaConfig == null){
                    try {
                        kafkaConfig = new KafkaConfig();
                    } catch (IOException e) {
                        LOG.error(&quot;实例化kafkaConfig失败&quot;, e);
                    }
                }
            }
        }
        return kafkaConfig;
    }

    public ProducerConfig getProducerConfig(){
        return config;
    }

    /**
      * 获取当前时间的字符串       格式为：yyyy-MM-dd HH:mm:ss
      * @return String
     */
    public static String nowStr(){
        return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format( new Date() );
    }
}</code></pre><h4 id="3、producer-StringProducer-java—生产者"><a href="#3、producer-StringProducer-java—生产者" class="headerlink" title="3、producer/StringProducer.java—生产者"></a>3、producer/StringProducer.java—生产者</h4><pre><code>package com.hsiehchou.kafka.producer;

import com.hsiehchou.common.thread.ThreadPoolManager;
import com.hsiehchou.kafka.config.KafkaConfig;
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;

public class StringProducer {
    private static final Logger LOG = LoggerFactory.getLogger(StringProducer.class);

    public static void main(String[] args) {
        StringProducer.producer(&quot;chl_test2&quot;,&quot;{\&quot;rksj\&quot;:\&quot;1558177156\&quot;,\&quot;latitude\&quot;:\&quot;24.000000\&quot;,\&quot;imsi\&quot;:\&quot;000000000000000\&quot;,\&quot;accept_message\&quot;:\&quot;\&quot;,\&quot;phone_mac\&quot;:\&quot;aa-aa-aa-aa-aa-aa\&quot;,\&quot;device_mac\&quot;:\&quot;bb-bb-bb-bb-bb-bb\&quot;,\&quot;message_time\&quot;:\&quot;1789098762\&quot;,\&quot;filename\&quot;:\&quot;wechat_source1_1111119.txt\&quot;,\&quot;absolute_filename\&quot;:\&quot;/usr/chl/data/filedir_successful/2019-05-18/data/filedir/wechat_source1_1111119.txt\&quot;,\&quot;phone\&quot;:\&quot;18609765432\&quot;,\&quot;device_number\&quot;:\&quot;32109231\&quot;,\&quot;imei\&quot;:\&quot;000000000000000\&quot;,\&quot;id\&quot;:\&quot;1792d6529e2143fa85717e706403c83c\&quot;,\&quot;collect_time\&quot;:\&quot;1557305988\&quot;,\&quot;send_message\&quot;:\&quot;\&quot;,\&quot;table\&quot;:\&quot;wechat\&quot;,\&quot;object_username\&quot;:\&quot;judy\&quot;,\&quot;longitude\&quot;:\&quot;23.000000\&quot;,\&quot;username\&quot;:\&quot;andiy\&quot;}&quot;);
    }

    private static int threadSize = 6;

    /**
     * 生产单条消息 单条推送
     * @param topic
     * @param recourd
     */
    public static void producer(String topic,String recourd){
        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());
        KeyedMessage&lt;String, String&gt; keyedMessage = new KeyedMessage&lt;&gt;(topic, recourd);
        producer.send(keyedMessage);
        LOG.info(&quot;发送数据&quot;+recourd+&quot;到kafka成功&quot;);
        producer.close();
    }

    /**
     * 批量推送
     * @param topic
     * @param listRecourd
     */
    public static void producerList(String topic,List&lt;String&gt; listRecourd){
        Producer&lt;String, String&gt; producer = new Producer&lt;&gt;(KafkaConfig.getInstance().getProducerConfig());
        List&lt;KeyedMessage&lt;String, String&gt;&gt; listKeyedMessage= new ArrayList&lt;&gt;();
        listRecourd.forEach(recourd-&gt;{
            listKeyedMessage.add(new KeyedMessage&lt;&gt;(topic, recourd));
        });
        producer.send(listKeyedMessage);
        producer.close();
    }

    /**
     * 多线程推送
     * @param topic  kafka  topic
     * @param listMessage 消息
     * @throws Exception
     */
    public void producer(String topic,List&lt;String&gt; listMessage) throws Exception{
        //  int size = listMessage.size();

        List&lt;List&lt;String&gt;&gt; lists = splitList(listMessage, 5);
        int threadNum = lists.size();

        long t1 = System.currentTimeMillis();
        CountDownLatch cdl = new CountDownLatch(threadNum);

        //使用线程池
        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();
        LOG.info(&quot;开启 &quot; + threadNum + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);

        for (int i = 0; i &lt; threadNum; i++) {
            try {
                executorService.execute(new ProducerTask(topic,lists.get(i)));
            } catch (Exception e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        cdl.await();
        long t = System.currentTimeMillis() - t1;
        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );
        executorService.shutdown();
    }

    /**
     * 拆分消息集合,计算使用多少个线程执行运算
     * @param mtList
     */
    public static List&lt;List&lt;String&gt;&gt; splitList(List&lt;String&gt; mtList, int splitSize){
        if(mtList == null || mtList.size()==0){
            return null;
        }
        int length = mtList.size();

        // 计算可以分成多少组
        int num = ( length + splitSize - 1 )/splitSize ;
        List&lt;List&lt;String&gt;&gt; spiltList = new ArrayList&lt;&gt;(num);

        for (int i = 0; i &lt; num; i++) {
            // 开始位置
            int fromIndex = i * splitSize;
            // 结束位置
            int toIndex = (i+1) * splitSize &lt; length ? ( i+1 ) * splitSize : length ;
            spiltList.add(mtList.subList(fromIndex,toIndex)) ;
        }
        return  spiltList;
    }

    class ProducerTask implements Runnable{
        private String topic;
        private List&lt;String&gt; listRecourd;
        public ProducerTask( String topic, List&lt;String&gt; listRecourd){
            this.topic = topic;
            this.listRecourd = listRecourd;
        }
        public void run() {
            producerList(topic,listRecourd);
        }
    }

   /* public static void producer(String topic,List&lt;KeyedMessage&lt;String,String&gt;&gt; listMessage) throws Exception{
        int size = listMessage.size();
        int threads = ( ( size - 1  ) / threadSize ) + 1;

        long t1 = System.currentTimeMillis();
        CountDownLatch cdl = new CountDownLatch(threads);
        //使用线程池
        ExecutorService executorService = ThreadPoolManager.getInstance().getExecutorService();
        LOG.info(&quot;开启 &quot; + threads + &quot; 个线程来向  topic &quot; + topic + &quot; 生产数据 . &quot;);
      *//*  for( int i = 0 ; i &lt; threads ; i++ ){
            executorService.execute( new StringProducer.ChildProducer( start , end ,  topic , id,  cdl ));
        }*//*
        cdl.await();
        long t = System.currentTimeMillis() - t1;
        LOG.info(  &quot; 一共耗时  ：&quot; + t + &quot;  毫秒 ... &quot; );
        executorService.shutdown();
    }

    static class ChildProducer implements Runnable{

        public ChildProducer( int start , int end ,  String topic , String id,  CountDownLatch cdl ){


        }

        public void run() {

        }
    }
    */

}</code></pre><h3 id="七、Spark—kafka2es开发"><a href="#七、Spark—kafka2es开发" class="headerlink" title="七、Spark—kafka2es开发"></a>七、Spark—kafka2es开发</h3><h4 id="Cloudera查找对应的maven依赖地址"><a href="#Cloudera查找对应的maven依赖地址" class="headerlink" title="Cloudera查找对应的maven依赖地址"></a>Cloudera查找对应的maven依赖地址</h4><p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk" target="_blank" rel="noopener">https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html#concept_flv_nwn_yk</a></p>
<p><strong>SparkStreaming+Kafka的两种模式receiver模式和Direct模式</strong></p>
<h4 id="Sparkstreming-kafka-receiver模式理解"><a href="#Sparkstreming-kafka-receiver模式理解" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84receiver%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的receiver模式"></p>
<p> <strong>receiver模式理解</strong><br>在SparkStreaming程序运行起来后，Executor中会有receiver tasks接收kafka推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。receiver task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。备份完成后去zookeeper中更新消费偏移量，然后向Driver中的receiver tracker汇报数据的位置。最后Driver根据数据本地化将task分发到不同节点上执行。</p>
<p><strong>receiver模式中存在的问题</strong><br>当Driver进程挂掉后，Driver下的Executor都会被杀掉，当更新完zookeeper消费偏移量的时候，Driver如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。</p>
<p><strong>如何解决这个问题？</strong><br>开启WAL(write ahead log)预写日志机制,在接受过来数据备份到其他节点的时候，同时备份到HDFS上一份（我们需要将接收来的数据的持久化级别降级到MEMORY_AND_DISK），这样就能保证数据的安全性。不过，因为写HDFS比较消耗性能，要在备份完数据之后才能进行更新zookeeper以及汇报位置等，这样会增加job的执行时间，这样对于任务的执行提高了延迟度。</p>
<p><strong>注意</strong><br>1）开启WAL之后，接受数据级别要降级，有效率问题<br>2）开启WAL要checkpoint<br>3）开启WAL(write ahead log),往HDFS中备份一份数据</p>
<h4 id="Sparkstreming-kafka-receiver模式理解-1"><a href="#Sparkstreming-kafka-receiver模式理解-1" class="headerlink" title="Sparkstreming + kafka receiver模式理解"></a>Sparkstreming + kafka receiver模式理解</h4><p><img src="/medias/kafka%E7%9A%84direct%E6%A8%A1%E5%BC%8F.PNG" alt="kafka的direct模式"></p>
<ol>
<li>简化数据处理流程</li>
<li>自己定义offset存储，保证数据0丢失，但是会存在重复消费问题。（解决消费等幂问题）</li>
<li>不用接收数据，自己去kafka中拉取</li>
</ol>
<h4 id="1、spark下的pom-xml"><a href="#1、spark下的pom-xml" class="headerlink" title="1、spark下的pom.xml"></a>1、spark下的pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_spark&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_spark&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;spark.version&gt;1.6.0&lt;/spark.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;
                    &lt;groupId&gt;javax.servlet&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;gson&lt;/artifactId&gt;
                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;httpcore&lt;/artifactId&gt;
                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;httpclient&lt;/artifactId&gt;
                    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
                    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;gson&lt;/artifactId&gt;
                    &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}-${cdh.version}&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch-spark-13_2.10&lt;/artifactId&gt;
            &lt;version&gt;6.2.3&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.15.2&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;
                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;
                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;
                &lt;/configuration&gt;

                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;
                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;
                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;
                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;</code></pre><h4 id="2、spark中的文件结构"><a href="#2、spark中的文件结构" class="headerlink" title="2、spark中的文件结构"></a>2、spark中的文件结构</h4><p><img src="/medias/spark%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="spark中的文件结构"></p>
<p><img src="/medias/%E8%AE%A9IDEA%E8%83%BD%E6%96%B0%E5%BB%BAscala.class.PNG" alt="让IDEA能新建scala.class"></p>
<p>点击”+”号，选择Scala SDK，点击Browse，选择本地下载的scala-sdk-2.10.4</p>
<h4 id="3、xz-bigdata-spark-spark-common"><a href="#3、xz-bigdata-spark-spark-common" class="headerlink" title="3、xz_bigdata_spark/spark/common"></a>3、xz_bigdata_spark/spark/common</h4><p><strong>SparkContextFactory.scala</strong></p>
<pre><code>package com.hsiehchou.spark.common

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{Accumulator, SparkContext}

object SparkContextFactory {

  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {
    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)
    new SparkContext(sparkConf)
  }

  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {
    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)
    sparkConf.set(&quot;&quot;,&quot;&quot;)
    new SparkContext(sparkConf)
  }

  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {
    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)
    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)
    accumulator
  }

  /**
    * 创建本地流streamingContext
    * @param appName             appName
    * @param batchInterval      多少秒读取一次
    * @param threads            开启多少个线程
    * @return
    */
  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,
                                    batchInterval:Long = 30L ,
                                    threads : Int = 4) : StreamingContext = {
    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)
    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)
    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)
    new StreamingContext(sparkConf, Seconds(batchInterval))
  }

  /**
    * 创建集群模式streamingContext
    * 这里不设置线程数，在submit中指定
    * @param appName
    * @param batchInterval
    * @return
    */
  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {
    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)
    new StreamingContext(sparkConf, Seconds(batchInterval))
  }

  def startSparkStreaming(ssc:StreamingContext){
    ssc.start()
      ssc.awaitTermination()
      ssc.stop()
  }
}</code></pre><p><strong>convert/DataConvert.scala</strong></p>
<pre><code>package com.hsiehchou.spark.common.convert

import java.util

import com.hsiehchou.common.config.ConfigUtil
import org.apache.spark.Logging

import scala.collection.JavaConversions._

/**
  * 数据类型转换
  */
object DataConvert extends Serializable with Logging {

  val fieldMappingPath = &quot;es/mapping/fieldmapping.properties&quot;

  private val typeFieldMap: util.HashMap[String, util.HashMap[String, String]] = getEsFieldtypeMap()

  /**
    * 将Map&lt;String,String&gt;转化为Map&lt;String,Object&gt;
    */
  def strMap2esObjectMap(map:util.Map[String,String]):util.Map[String,Object] ={

    //获取配置文件中的数据类型
    val dataType = map.get(&quot;table&quot;)

    //获取配置文件中的数据类型的 字段类型
    val fieldMap = typeFieldMap.get(dataType)

    //获取数据类型的所有字段，配置文件里的字段
    val keySet = fieldMap.keySet()

    //var objectMap:util.HashMap[String,Object] = new util.HashMap[String,Object]()
    var objectMap = new java.util.HashMap[String, Object]()

    //数据里的字段
    val set = map.keySet().iterator()

    try {
      //遍历真实数据的所有字段
      while (set.hasNext()) {
        val key = set.next()
        var dataType:String = &quot;string&quot;

        //如果在配置文件中的key包含真实数据的key
        if (keySet.contains(key)) {
          //则获取真实数据字段的数据类型
          dataType = fieldMap.get(key)
        }
        dataType match {
          case &quot;long&quot; =&gt; objectMap = BaseDataConvert.mapString2Long(map, key, objectMap)
          case &quot;string&quot; =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)
          case &quot;double&quot; =&gt; objectMap = BaseDataConvert.mapString2Double(map, key, objectMap)
          case _ =&gt; objectMap = BaseDataConvert.mapString2String(map, key, objectMap)
        }
      }
    }catch {
      case e: Exception =&gt; logInfo(&quot;转换异常&quot;, e)
    }
    println(&quot;转换后&quot; + objectMap)
    objectMap
  }

  /**
    * 读取 &quot;es/mapping/fieldmapping.properties 配置文件
    * 主要作用是将 真实数据 根据配置来作数据类型转换 转换为和ES mapping结构保持一致
    * @return
    */
  def getEsFieldtypeMap(): util.HashMap[String, util.HashMap[String, String]] = {

    // [&quot;wechat&quot;:[&quot;phone_mac&quot;:&quot;string&quot;,&quot;latitude&quot;:&quot;long&quot;]]

    //定义返回Map
    val mapMap = new util.HashMap[String, util.HashMap[String, String]]
    val properties = ConfigUtil.getInstance().getProperties(fieldMappingPath)
    val tables = properties.get(&quot;tables&quot;).toString.split(&quot;,&quot;)
    val tableFields = properties.keySet()

    tables.foreach(table =&gt; {
      val map = new util.HashMap[String, String]()
      tableFields.foreach(tableField =&gt; {
        if (tableField.toString.startsWith(table)) {
          val key = tableField.toString.split(&quot;\\.&quot;)(1)
          val value = properties.get(tableField).toString
          map.put(key, value)
        }
      })
      mapMap.put(table, map)
    })
    mapMap
  }
}</code></pre><p><img src="/medias/scala%E4%B8%AD%E7%9A%84scala%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84.PNG" alt="scala中的scala文件结构"></p>
<h4 id="4、org-apache-spark-streaming-kafka-KafkaManager-scala"><a href="#4、org-apache-spark-streaming-kafka-KafkaManager-scala" class="headerlink" title="4、org/apache/spark/streaming/kafka/KafkaManager.scala"></a>4、org/apache/spark/streaming/kafka/KafkaManager.scala</h4><p>构建Kafka时用到，KafkaCluster在org.apache.spark.streaming.kafka下面，而且只能在spark里面使用，这时候我们就可以新建相同的目录结构，就可以引用了，如下图所示：</p>
<p><img src="/medias/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%96%B0%E5%BB%BAorg.apache.spark.streaming.kafka.PNG" alt="为什么要新建org.apache.spark.streaming.kafka"></p>
<pre><code>package org.apache.spark.streaming.kafka

import com.alibaba.fastjson.TypeReference
import kafka.common.TopicAndPartition
import kafka.message.MessageAndMetadata
import kafka.serializer.{Decoder, StringDecoder}
import org.apache.spark.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.{DStream, InputDStream}

import scala.reflect.ClassTag

/**
  * 包名说明 ：KafkaCluster是私有类，只能在spark包中使用，
  *           所以包名保持和 KafkaCluster 一致才能调用
  * @param kafkaParams
  * @param autoUpdateoffset
  */
class KafkaManager(val kafkaParams:Map[String, String],
                   val autoUpdateoffset:Boolean =true) extends Serializable with Logging {

  //构造一个KafkaCluster
  @transient
  private var cluster = new KafkaCluster(kafkaParams)

  //定义一个单例
  def kc(): KafkaCluster = {
    if (cluster == null) {
      cluster = new KafkaCluster(kafkaParams)
    }
    cluster
  }

  /**
    * 泛型流读取器
    * @param ssc
    * @param topics kafka topics,多个topic按&quot;,&quot;分割
    * @tparam K  泛型 K
    * @tparam V  泛型 V
    * @tparam KD scala泛型 KD &lt;: Decoder[K] 说明KD 的类型必须是Decoder[K]的子类型  上下界
    * @tparam VD scala泛型 VD &lt;: Decoder[V] 说明VD 的类型必须是Decoder[V]的子类型  上下界
    * @return
    */
  def createDirectStream[K: ClassTag, V: ClassTag,
  KD &lt;: Decoder[K] : ClassTag,
  VD &lt;: Decoder[V] : ClassTag](ssc: StreamingContext, topics: Set[String]): InputDStream[(K, V)] = {

    //获取消费者组ID
    //val groupId = &quot;test&quot;
    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)

    // 在zookeeper上读取offsets前先根据实际情况更新offsets
    setOrUpdateOffsets(topics, groupId)

    //把所有的offsets处理完成，就可以从zookeeper上读取offset开始消费message
    val messages = {
      //获取kafka分区信息  为了打印信息
      val partitionsE = kc.getPartitions(topics)
      require(partitionsE.isRight, s&quot;获取 kafka topic ${topics}`s partition 失败。&quot;)
      val partitions = partitionsE.right.get
      println(&quot;打印分区信息&quot;)
      partitions.foreach(println(_))

      //获取分区的offset
      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
      require(consumerOffsetsE.isRight, s&quot;获取 kafka topic ${topics}`s consumer offsets 失败。&quot;)
      val consumerOffsets = consumerOffsetsE.right.get
      println(&quot;打印消费者分区偏移信息&quot;)
      consumerOffsets.foreach(println(_))
      //读取数据
      KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)](
        ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message))
    }

    if (autoUpdateoffset) {
      //更新offset
      messages.foreachRDD(rdd =&gt; {
        logInfo(&quot;RDD 消费成功，开始更新zookeeper上的偏移&quot;)
        updateZKOffsets(rdd)
      })
    }
    messages
  }


  /**
    * 创建数据流前，根据实际消费情况更新消费offsets
    * @param topics
    * @param groupId
    */
  private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = {
    topics.foreach(topic =&gt; {
      //先获取Kafka offset信息  Kafka partions的节点信息
      //获取kafka本身的偏移量, Either类型可以认为就是封装了2种信息
      val partitionsE = kc.getPartitions(Set(topic))
      logInfo(partitionsE + &quot;&quot;)
      //require(partitionsE.isRight, &quot;获取partition失败&quot;)
      require(partitionsE.isRight, s&quot;获取 kafka topic ${topic}`s partition 失败。&quot;)
      println(&quot;partitionsE=&quot; + partitionsE)
      val partitions = partitionsE.right.get
      println(&quot;打印分区信息&quot;)
      partitions.foreach(println(_))

      //获取kafka partions最早的offsets
      val earliestLeader = kc.getEarliestLeaderOffsets(partitions)
      require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)
      val earliestLeaderOffsets = earliestLeader.right.get
      println(&quot;kafka最早的消息偏移量&quot;)
      earliestLeaderOffsets.foreach(println(_))

      //获取kafka最末尾的offsets
      val latestLeader = kc.getLatestLeaderOffsets(partitions)
      //require(latestLeader.isRight, &quot;获取latestLeader失败&quot;)
      val latestLeaderOffsets = latestLeader.right.get
      println(&quot;kafka最末尾的消息偏移量&quot;)
      latestLeaderOffsets.foreach(println(_))

      //获取消费者的offsets
      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)

      //判断消费者是否消费过,消费者offset存在
      if (consumerOffsetsE.isRight) {
        /**
          * 如果zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。
          * 针对这种情况，只要判断一下zk上的consumerOffsets和earliestLeaderOffsets的大小，
          * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时,
          * 这时把consumerOffsets更新为earliestLeaderOffsets
          */
        //如果消费过，直接取过来的kafka消费，，earliestLeader 存在
        if (earliestLeader.isRight) {
          //获取到最早的offset  也就是最小的offset
          require(earliestLeader.isRight, &quot;获取earliestLeader失败&quot;)
          val earliestLeaderOffsets = earliestLeader.right.get
          //获取消费者组的offset
          val consumerOffsets = consumerOffsetsE.right.get
          // 将 consumerOffsets 和 earliestLeaderOffsets 的offsets 做比较
          // 可能只是存在部分分区consumerOffsets过时，所以只更新过时分区的consumerOffsets为earliestLeaderOffsets
          var offsets: Map[TopicAndPartition, Long] = Map()

          consumerOffsets.foreach({ case (tp, n) =&gt;
            val earliestLeaderOffset = earliestLeaderOffsets(tp).offset
            //如果消費者的偏移小于 kafka中最早的offset,那么，將最早的offset更新到zk
            if (n &lt; earliestLeaderOffset) {
              logWarning(&quot;consumer group:&quot; + groupId + &quot;,topic:&quot; + tp.topic + &quot;,partition:&quot; + tp.partition +
                &quot; offsets已经过时，更新为&quot; + earliestLeaderOffset)
              offsets += (tp -&gt; earliestLeaderOffset)
            }
          })
          //设置offsets
          setOffsets(groupId, offsets)
        }
      } else {
        //如果没有消费过，那么就去取kafka获取earliestLeader写到zk中
        // 消费者还没有消费过  也就是zookeeper中还没有消费者的信息
        if (earliestLeader.isLeft)
          logError(s&quot;${topic} hasConsumed but earliestLeaderOffsets is null。&quot;)

        //看是从头消费还是从末开始消费  smallest表示从头开始消费
        val reset = kafkaParams.get(&quot;auto.offset.reset&quot;).map(_.toLowerCase).getOrElse(&quot;smallest&quot;)

        //往zk中去写，构建消费者 偏移
        var leaderOffsets: Map[TopicAndPartition, Long] = Map.empty

        //从头消费
        if (reset.equals(&quot;smallest&quot;)) {
          //分为 存在 和 不存在 最早的消费记录 两种情况
          //如果kafka 最小偏移存在，则将消费者偏移设置为和kafka偏移一样
          if (earliestLeader.isRight) {
            leaderOffsets = earliestLeader.right.get.map {
              case (tp, offset) =&gt; (tp, offset.offset)
            }
          } else {
            //如果不存在，则从新构建偏移全部为0 offsets
            leaderOffsets = partitions.map(tp =&gt; (tp, 0L)).toMap
          }
        } else {
          //直接获取最新的offset
          leaderOffsets = kc.getLatestLeaderOffsets(partitions).right.get.map {
            case (tp, offset) =&gt; (tp, offset.offset)
          }
        }
        //设置offsets 写到zk中
        setOffsets(groupId, leaderOffsets)
      }
    })
  }

  /**
    * 设置消费者组的offsets
    * @param groupId
    * @param offsets
    */
  private def setOffsets(groupId: String, offsets: Map[TopicAndPartition, Long]): Unit = {
    if (offsets.nonEmpty) {
      //更新offset
      val o = kc.setConsumerOffsets(groupId, offsets)
      logInfo(s&quot;更新zookeeper中消费组为：${groupId} 的 topic offset信息为： ${offsets}&quot;)
      if (o.isLeft) {
        logError(s&quot;Error updating the offset to Kafka cluster: ${o.left.get}&quot;)
      }
    }
  }

  /**
    * 通过spark的RDD 更新zookeeper上的消费offsets
    * @param rdd
    */
  def updateZKOffsets[K: ClassTag, V: ClassTag](rdd: RDD[(K, V)]) : Unit = {
    //获取消费者组
    val groupId = kafkaParams.get(&quot;group.id&quot;).getOrElse(&quot;default&quot;)
    //spark使用kafka低阶API进行消费的时候,每个partion的offset是保存在 spark的RDD中，所以这里可以直接在
    //RDD的 HasOffsetRanges 中获取倒offsets信息。因为这个信息spark不会把则个信息存储到zookeeper中，所以
    //我们需要自己实现将这部分offsets信息存储到zookeeper中
    val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
    //打印出spark中保存的offsets信息
    offsetsList.foreach(x=&gt;{
      println(&quot;获取spark 中的偏移信息&quot;+x)
    })

    for (offsets &lt;- offsetsList) {
      //根据topic和partition 构建topicAndPartition
      val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)
      logInfo(&quot;将SPARK中的 偏移信息 存到zookeeper中&quot;)
      //将消费者组的offsets更新到zookeeper中
      setOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))
    }
  }

  //(null,{&quot;rksj&quot;:&quot;1558178497&quot;,&quot;latitude&quot;:&quot;24.000000&quot;,&quot;imsi&quot;:&quot;000000000000000&quot;})
  //读取kafka流，并将json数据转为map
  def createJsonToJMapObjectDirectStreamWithOffset(ssc:StreamingContext, topicsSet:Set[String]): DStream[java.util.Map[String,Object]] = {
    //一个转换器
    val converter = {json:String =&gt;
      println(json)
      var res : java.util.Map[String,Object] = null
      try {
        //JSON转map的操作
        res = com.alibaba.fastjson.JSON.parseObject(json,
          new TypeReference[java.util.Map[String, Object]]() {})
      } catch {
        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)
      }
      res
    }
    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
  }

  /**
    * 根据converter创建流数据
    * @param ssc
    * @param topicsSet
    * @param converter
    * @tparam T
    * @return
    */
  def createDirectStreamWithOffset[T:ClassTag](ssc:StreamingContext,
                                               topicsSet:Set[String], converter:String =&gt; T): DStream[T] = {
    createDirectStream[String, String, StringDecoder, StringDecoder](ssc, topicsSet)
      .map(pair =&gt;converter(pair._2))
  }

  def createJsonToJMapDirectStreamWithOffset(ssc:StreamingContext,
                                             topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {
    val converter = {json:String =&gt;
      var res : java.util.Map[String,String] = null
      try {
        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})
      } catch {
        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)
      }
      res
    }
    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
  }

  /*
    /**
      * @param ssc
      * @param topicsSet
      * @return
      */
    def createJsonToJavaBeanDirectStreamWithOffset(ssc:StreamingContext ,
                                                   topicsSet:Set[String]): DStream[Object] = {
      val converter = {json:String =&gt;
        var res : Object = null
        try {
          res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[Object]() {})
        } catch {
          case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)
        }
        res
      }
      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
    }
  */

  /*
    def createStringDirectStreamWithOffset(ssc:StreamingContext ,
                                           topicsSet:Set[String]): DStream[String] = {
      val converter = {json:String =&gt;
        json
      }
      createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
    }
  */

  /**
    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息
    * @param ssc   spark ssc
    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据
    * @return  DStream[java.util.Map[String,String
    */
  def createJsonToJMapStringDirectStreamWithOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {
    val converter = {json:String =&gt;
      var res : java.util.Map[String,String] = null
      try {
        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})
      } catch {
        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)
      }
      res
    }
    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
  }

  /**
    * 读取JSON的流 并将JSON流 转为MAP流  并且这个流支持RDD向zookeeper中记录消费信息
    * @param ssc   spark ssc
    * @param topicsSet topic 集合 支持从多个kafka topic同时读取数据
    * @return  DStream[java.util.Map[String,String
    */
  def createJsonToJMapStringDirectStreamWithoutOffset(ssc:StreamingContext , topicsSet:Set[String]): DStream[java.util.Map[String,String]] = {
    val converter = {json:String =&gt;
      var res : java.util.Map[String,String] = null
      try {
        res = com.alibaba.fastjson.JSON.parseObject(json, new TypeReference[java.util.Map[String, String]]() {})
      } catch {
        case e: Exception =&gt; logError(s&quot;解析topic ${topicsSet}, 的记录 ${json} 失败。&quot;, e)
      }
      res
    }
    createDirectStreamWithOffset(ssc, topicsSet, converter).filter(_ != null)
  }

}

object KafkaManager extends Logging{

  def apply(broker:String, groupId:String = &quot;default&quot;,
            numFetcher:Int = 1, offset:String = &quot;smallest&quot;,
            autoUpdateoffset:Boolean = true): KafkaManager ={
    new KafkaManager(
      createKafkaParam(broker, groupId, numFetcher, offset),
      autoUpdateoffset)
  }

  def createKafkaParam(broker:String, groupId:String = &quot;default&quot;,
                       numFetcher:Int = 1, offset:String = &quot;smallest&quot;): Map[String, String] ={
    //创建 stream 时使用的 topic 名字集合
    Map[String, String](
      &quot;metadata.broker.list&quot; -&gt; broker,
      &quot;auto.offset.reset&quot; -&gt; offset,
      &quot;group.id&quot; -&gt; groupId,
      &quot;num.consumer.fetchers&quot; -&gt; numFetcher.toString)
  }
}</code></pre><h4 id="5、resources-log4j-properties"><a href="#5、resources-log4j-properties" class="headerlink" title="5、resources/log4j.properties"></a>5、resources/log4j.properties</h4><pre><code>### 设置###
log4j.rootLogger = error,stdout,D,E

### 输出信息到控制抬 ###
log4j.appender.stdout = org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target = System.out
log4j.appender.stdout.layout = org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern = [%-5p] %d{yyyy-MM-dd HH:mm:ss,SSS} method:%l%n%m%n

### 输出DEBUG 级别以上的日志到=E://logs/error.log ###
log4j.appender.D = org.apache.log4j.DailyRollingFileAppender
log4j.appender.D.File = E://logs/log.log
log4j.appender.D.Append = true
log4j.appender.D.Threshold = stdout 
log4j.appender.D.layout = org.apache.log4j.PatternLayout
log4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n

###输出ERROR 级别以上的日志到=E://logs/error.log ###
log4j.appender.E = org.apache.log4j.DailyRollingFileAppender
log4j.appender.E.File =E://logs/error.log 
log4j.appender.E.Append = true
log4j.appender.E.Threshold = ERROR 
log4j.appender.E.layout = org.apache.log4j.PatternLayout
log4j.appender.E.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n</code></pre><h4 id="6、xz-bigdata-spark-spark-streaming-kafka"><a href="#6、xz-bigdata-spark-spark-streaming-kafka" class="headerlink" title="6、xz_bigdata_spark/spark/streaming/kafka"></a>6、xz_bigdata_spark/spark/streaming/kafka</h4><p><strong>Spark_Es_ConfigUtil.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka

import org.apache.spark.Logging

object Spark_Es_ConfigUtil extends Serializable with Logging{

 // val ES_NODES = &quot;es.cluster.nodes&quot;
 // val ES_PORT = &quot;es.cluster.http.port&quot;
 // val ES_CLUSTERNAME = &quot;es.cluster.name&quot;

  val ES_NODES = &quot;es.nodes&quot;
  val ES_PORT = &quot;es.port&quot;
  val ES_CLUSTERNAME = &quot;es.clustername&quot;

  def getEsParam(id_field : String): Map[String,String] ={
    Map[String ,String](&quot;es.mapping.id&quot; -&gt; id_field,
      ES_NODES -&gt; &quot;hadoop1,hadoop2,hadoop3&quot;,
      //ES_NODES -&gt; &quot;hadoop1&quot;,
      ES_PORT -&gt; &quot;9200&quot;,
      ES_CLUSTERNAME -&gt; &quot;xz_es&quot;,
      &quot;es.batch.size.entries&quot;-&gt;&quot;6000&quot;,
      /*   &quot;es.nodes.wan.only&quot;-&gt;&quot;true&quot;,*/
      &quot;es.nodes.discovery&quot;-&gt;&quot;true&quot;,
      &quot;es.batch.size.bytes&quot;-&gt;&quot;300000000&quot;,
      &quot;es.batch.write.refresh&quot;-&gt;&quot;false&quot;
    )
  }
}</code></pre><p><strong>Spark_Kafka_ConfigUtil.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka

import org.apache.spark.Logging

object Spark_Kafka_ConfigUtil extends Serializable with Logging{

  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={
    val kafkaParam=Map[String,String](
      &quot;metadata.broker.list&quot; -&gt; brokerList,
      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,
      &quot;group.id&quot; -&gt; groupId,
      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,
      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)
    kafkaParam
  }
}</code></pre><h4 id="7、kafka2es"><a href="#7、kafka2es" class="headerlink" title="7、kafka2es"></a>7、kafka2es</h4><p><strong>Kafka2esJob.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2es

import com.hsiehchou.es.admin.AdminUtil
import com.hsiehchou.es.client.ESClientUtils
import com.hsiehchou.spark.common.convert.DataConvert
import com.hsiehchou.spark.streaming.kafka.Spark_Es_ConfigUtil
import org.apache.spark.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.DStream
import org.elasticsearch.client.transport.TransportClient
import org.elasticsearch.spark.rdd.EsSpark

object Kafka2esJob extends Serializable with Logging {

  /**
    * 按日期分组写入ES
    * @param dataType
    * @param typeDS
    */
  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={

    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd
    //主要就是时间混杂  通过时间分组就行了 groupby       filter
    //index前缀  通过对日期进行过滤 避免shuffle操作
    val index_prefix = dataType
    val client: TransportClient = ESClientUtils.getClient
    typeDS.foreachRDD(rdd=&gt;{

      //如果时少量数据可以这样处理
      //rdd.groupBy()
      //吧所有的日期拿到
      val days = getDays(dataType,rdd)

      //我们使用日期对数据进行过滤  par时scala并发集合
      days.par.foreach(day=&gt;{

        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;
        val index = index_prefix + &quot;_&quot; + day

        //判断索引是否存在
        val bool = AdminUtil.indexExists(client,index)
        if(!bool){
          //如果不存在，创建
          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;
          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)
        }
        //构建RDD，数据类型 某一天的数据RDD
        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD
        val tableRDD = rdd.filter(map=&gt;{
          day.equals(map.get(&quot;index_date&quot;))
        }).map(x=&gt;{
          //将map[String,String] 转为map[String,obJECT]
          DataConvert.strMap2esObjectMap(x)
        })
        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))
      })
    })
    //日期为后
  }

  /**
    * 获取日期的集合
    * @param dataType
    * @param rdd
    * @return
    */
  def getDays(dataType:String,rdd:RDD[java.util.Map[String,String]]): Array[String] ={
    //对日期去重，然后集中到driver
    return  rdd.map(x=&gt;{x.get(&quot;index_date&quot;)}).distinct().collect()
  }

  /**
    * 将RDD转换之后写入ES
    * @param dataType
    * @param typeRDD
    */
  def insertData2Es(dataType:String,typeRDD:RDD[java.util.Map[String,String]]): Unit = {
    val index = dataType
    val esRDD =  typeRDD.map(x=&gt;{
      DataConvert.strMap2esObjectMap(x)
    })
    EsSpark.saveToEs(esRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))
    println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)
  }

  /**
    * 将RDD转换后写入ES
    * @param dataType
    * @param typeDS
    */
  def insertData2Es(dataType:String, typeDS:DStream[java.util.Map[String, String]]): Unit = {
    val index = dataType
    typeDS.foreachRDD(rdd=&gt;{
      val esRDD = rdd.map(x=&gt;{
        DataConvert.strMap2esObjectMap(x)
      })
      EsSpark.saveToEs(rdd, dataType+&quot;/&quot;+dataType, Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))
      println(&quot;写入ES&quot; + esRDD.count() + &quot;条数据成功&quot;)
    })
  }
}</code></pre><p><strong>Kafka2esStreaming.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2es

import java.util
import java.util.Properties

import com.hsiehchou.common.config.ConfigUtil
import com.hsiehchou.common.project.datatype.DataTypeProperties
import com.hsiehchou.common.time.TimeTranstationUtils
import com.hsiehchou.spark.common.SparkContextFactory
import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtil
import org.apache.commons.lang3.StringUtils
import org.apache.spark.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka.KafkaManager

import scala.collection.JavaConversions._

object Kafka2esStreaming extends Serializable with Logging {
  //获取数据类型
  private val dataTypes: util.Set[String] = DataTypeProperties.dataTypeMap.keySet()

  val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)

  def main(args: Array[String]): Unit = {

    //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)
    val topics = args(1).split(&quot;,&quot;)

    //   val ssc = SparkConfFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)
    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))

    //构建kafkaManager
    val kafkaManager = new KafkaManager(
      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;XZ3&quot;)
    )

    //使用kafkaManager创建DStreaming流
    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)
      //添加一个日期分组字段
      //如果数据其他的转换，可以先在这里进行统一转换
      .map(map=&gt;{
      map.put(&quot;index_date&quot;,TimeTranstationUtils.Date2yyyyMMddHHmmss(java.lang.Long.valueOf(map.get(&quot;collect_time&quot;)+&quot;000&quot;)))
      map
    }).persist(StorageLevel.MEMORY_AND_DISK)

    //使用par并发集合可以是任务并发执行。在资源充足的情况下
    dataTypes.foreach(datatype=&gt;{
      //过滤出单个类别的数据种类
      val tableDS = kafkaDS.filter(x=&gt;{datatype.equals(x.get(&quot;table&quot;))})
      Kafka2esJob.insertData2Es(datatype,tableDS)
    })

    ssc.start()
    ssc.awaitTermination()
  }

  /**
    * 启动参数检查
    * @param args
    */
  def sparkParamCheck(args: Array[String]): Unit ={
    if (args.length == 4) {
      if (StringUtils.isBlank(args(1))) {
        logInfo(&quot;kafka集群地址不能为空&quot;)
        logInfo(&quot;kafka集群地址格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)
        logInfo(&quot;格式为     主机1名：9092,主机2名：9092,主机3名：9092...&quot;)
        System.exit(-1)
      }
      if (StringUtils.isBlank(args(2))) {
        logInfo(&quot;kafka topic1不能为空&quot;)
        System.exit(-1)
      }
      if (StringUtils.isBlank(args(3))) {
        logInfo(&quot;kafka topic2不能为空&quot;)
        System.exit(-1)
      }
    }else{
      logError(&quot;启动参数个数错误&quot;)
    }
  }

  def startJob(ds:DStream[String]): Unit ={
  }
}</code></pre><p><strong>java/com/hsiehchou/spark/common/convert/BaseDataConvert.java</strong></p>
<pre><code>package com.hsiehchou.spark.common.convert;

import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.HashMap;
import java.util.Map;

public class BaseDataConvert {

    private static final Logger LOG = LoggerFactory.getLogger(BaseDataConvert.class);

    public static HashMap&lt;String,Object&gt; mapString2Long(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {
        String logouttime = map.get(key);
        if (StringUtils.isNotBlank(logouttime)) {
            objectMap.put(key, Long.valueOf(logouttime));
        } else {
            objectMap.put(key, 0L);
        }
        return objectMap;
    }

    public static HashMap&lt;String,Object&gt; mapString2Double(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {
        String logouttime = map.get(key);
        if (StringUtils.isNotBlank(logouttime)) {
            objectMap.put(key, Double.valueOf(logouttime));
        } else {
            objectMap.put(key, 0.000000);
        }
        return objectMap;
    }

    public static HashMap&lt;String,Object&gt; mapString2String(Map&lt;String,String&gt; map, String key, HashMap&lt;String,Object&gt; objectMap) {
        String logouttime = map.get(key);
        if (StringUtils.isNotBlank(logouttime)) {
            objectMap.put(key, logouttime);
        } else {
            objectMap.put(key, &quot;&quot;);
        }
        return objectMap;
    }
}</code></pre><h4 id="8、ES动态索引创建"><a href="#8、ES动态索引创建" class="headerlink" title="8、ES动态索引创建"></a>8、ES动态索引创建</h4><pre><code>/**
    * 按日期分组写入ES
    * @param dataType
    * @param typeDS
    */
  def insertData2EsBydate(dataType:String,typeDS:DStream[java.util.Map[String,String]]): Unit ={

    //通过 dataType + 日期来动态创建 分索引。 日期格式为 yyyyMMdd
    //主要就是时间混杂  通过时间分组就行了 groupby       filter
    //index前缀  通过对日期进行过滤 避免shuffle操作
    val index_prefix = dataType
    val client: TransportClient = ESClientUtils.getClient
    typeDS.foreachRDD(rdd=&gt;{

      //如果时少量数据可以这样处理
      //rdd.groupBy()
      //吧所有的日期拿到
      val days = getDays(dataType,rdd)

      //我们使用日期对数据进行过滤  par时scala并发集合
      days.par.foreach(day=&gt;{

        //通过前缀+日期组成一个动态的索引   比例  qq + &quot;_&quot; + &quot;20190508&quot;
        val index = index_prefix + &quot;_&quot; + day

        //判断索引是否存在
        val bool = AdminUtil.indexExists(client,index)
        if(!bool){
          //如果不存在，创建
          val mappingPath = s&quot;es/mapping/${index_prefix}.json&quot;
          AdminUtil.buildIndexAndTypes(index, index, mappingPath, 5, 1)
        }
        //构建RDD，数据类型 某一天的数据RDD
        //返回一个map[String,obJECT] 的RDD   //就是一个单一类型  单一天数的RDD
        val tableRDD = rdd.filter(map=&gt;{
          day.equals(map.get(&quot;index_date&quot;))
        }).map(x=&gt;{
          //将map[String,String] 转为map[String,obJECT]
          DataConvert.strMap2esObjectMap(x)
        })
        EsSpark.saveToEs(tableRDD,index+ &quot;/&quot;+index,Spark_Es_ConfigUtil.getEsParam(&quot;id&quot;))
      })
    })
    //日期为后
  }</code></pre><p><strong>xz_bigdata_es下一节展示代码</strong><br><img src="/medias/%E5%85%A5ES%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E7%B4%A2%E5%BC%95.PNG" alt="入ES使用动态索引"></p>
<h4 id="9、CDH的java配置和Elasticsearch的配置"><a href="#9、CDH的java配置和Elasticsearch的配置" class="headerlink" title="9、CDH的java配置和Elasticsearch的配置"></a>9、CDH的java配置和Elasticsearch的配置</h4><p><strong>cdh的jdk设置</strong><br>/usr/local/jdk1.8</p>
<p><strong>kafka配置</strong></p>
<p>Default Number of Partitions：num.partitions 8</p>
<p>Offset Commit Topic Number of Partitions：180天</p>
<p>Log Compaction Delete Record Retention Time：log.cleaner.delete.retention.ms 30天</p>
<p>Data Log Roll Hours：log.retention.hours 30天  log.roll.hours 30天</p>
<p>Java Heap Size of Broker：broker_max_heap_size  1吉字节</p>
<p><strong>YARN</strong><br>容器内存 5g 5g 1g 10g</p>
<p><strong>这里的CDH安装另一篇文章介绍</strong></p>
<p><strong>前提安装好elasticsearch</strong></p>
<p>mkdir /opt/software/elasticsearch/data/</p>
<p>mkdir /opt/software/elasticsearch/logs/</p>
<p>chmod 777 /opt/software/elasticsearch/data/</p>
<p>useradd elasticsearch<br>passwd elasticsearch</p>
<p>chown -R elasticsearch elasticsearch/</p>
<p><strong>vim /etc/security/limits.conf</strong><br>添加如下内容:<br><code>*</code> <strong>soft nofile 65536</strong><br><code>*</code> <strong>hard nofile 131072</strong><br><code>*</code> <strong>soft nproc 2048</strong><br><code>*</code> <strong>hard nproc 4096</strong></p>
<p>进入limits.d目录下修改配置文件<br><strong>vim /etc/security/limits.d/90-nproc.conf</strong></p>
<p>修改如下内容：<br><strong>soft nproc 4096（修改为此参数，6版本的默认就是4096）</strong></p>
<p>修改配置sysctl.conf<br><strong>vim /etc/sysctl.conf</strong></p>
<p>添加下面配置：<br><strong>vm.max_map_count=655360</strong></p>
<p>并执行命令：<br><strong>sysctl -p</strong></p>
<p><strong>hadoop1的conf配置</strong><br><strong>elasticsearch.yml</strong></p>
<pre><code># ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
#cluster.name: my-application

cluster.name: xz_es

#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
#node.name: node-1
#

node.name: node-1

node.master: true

node.data: true

# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
#path.data: /path/to/data

path.data: /opt/software/elasticsearch/data

#
# Path to log files:
#
#path.logs: /path/to/logs

path.logs: /opt/software/elasticsearch/logs

#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true

bootstrap.memory_lock: false

bootstrap.system_call_filter: false

#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
#network.host: 192.168.0.1

network.host: 192.168.116.201

#
# Set a custom port for HTTP:
#
#http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]
#
#discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]

discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]

#
# Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
#action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p>
<p><strong>hadoop2的conf配置</strong><br><strong>elasticsearch.yml</strong></p>
<pre><code># ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
#cluster.name: my-application

cluster.name: xz_es

#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
#node.name: node-1

node.name: node-2

node.master: false

node.data: true

#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
#path.data: /path/to/data

path.data: /opt/software/elasticsearch/data

#
# Path to log files:
#
#path.logs: /path/to/logs

path.logs: /opt/software/elasticsearch/logs

#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true

bootstrap.memory_lock: false

bootstrap.system_call_filter: false

#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
#network.host: 192.168.0.1

network.host: 192.168.116.202

#
# Set a custom port for HTTP:
#
#http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]
#
#discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]

discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]

#
# Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
#action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p>
<p><strong>hadoop3的conf配置</strong><br><strong>elasticsearch.yml</strong></p>
<pre><code># ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
#cluster.name: my-application

cluster.name: xz_es

#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
#node.name: node-1

node.name: node-3

node.master: false

node.data: true

#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
#path.data: /path/to/data

path.data: /opt/software/elasticsearch/data

#
# Path to log files:
#
#path.logs: /path/to/logs

path.logs: /opt/software/elasticsearch/logs

#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true

bootstrap.memory_lock: false

bootstrap.system_call_filter: false

#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
#network.host: 192.168.0.1

network.host: 192.168.116.203

#
# Set a custom port for HTTP:
#
#http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]
#
#discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;]

discovery.zen.ping.unicast.hosts: [&quot;hadoop1&quot;, &quot;hadoop2&quot;, &quot;hadoop3&quot;]

#
# Prevent the &quot;split brain&quot; by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
#discovery.zen.minimum_master_nodes: 
#
# For more information, consult the zen discovery module documentation.
#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
#gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Require explicit names when deleting indices:
#
#action.destructive_requires_name: true</code></pre><p><strong>jvm.options</strong><br>修改下<br>-Xms64m<br>-Xmx64m</p>
<p><strong>Kibana的conf配置</strong></p>
<p><strong>kibana.yml</strong></p>
<pre><code># Kibana is served by a back end server. This setting specifies the port to use.

server.port: 5601

# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.
# The default is &#39;localhost&#39;, which usually means remote machines will not be able to connect.
# To allow connections from remote users, set this parameter to a non-loopback address.
#server.host: &quot;localhost&quot;

server.host: &quot;192.168.116.202&quot;

# Enables you to specify a path to mount Kibana at if you are running behind a proxy. This only affects
# the URLs generated by Kibana, your proxy is expected to remove the basePath value before forwarding requests
# to Kibana. This setting cannot end in a slash.
#server.basePath: &quot;&quot;

# The maximum payload size in bytes for incoming server requests.
#server.maxPayloadBytes: 1048576

# The Kibana server&#39;s name.  This is used for display purposes.
#server.name: &quot;your-hostname&quot;

# The URL of the Elasticsearch instance to use for all your queries.
#elasticsearch.url: &quot;http://localhost:9200&quot;

elasticsearch.url: &quot;http://192.168.116.201:9200&quot;</code></pre><p><strong>运行Elasticsearch</strong><br>cd /opt/software/elasticsearch<br>su elasticsearch<br>bin/elasticsearch &amp;</p>
<p><strong>运行Kibana</strong><br>cd /opt/software/kibana/<br>bin/kibana &amp;</p>
<h4 id="10、kafka2es打包到集群执行"><a href="#10、kafka2es打包到集群执行" class="headerlink" title="10、kafka2es打包到集群执行"></a>10、kafka2es打包到集群执行</h4><p><strong>打包</strong><br>使用maven工具点击install</p>
<p><strong>放入集群</strong><br>将打包完成的jar文件和xz_bigdata_spark-1.0-SNAPSHOT.jar 一起放入/usr/chl/spark7/目录下面</p>
<p><strong>执行</strong><br>spark-submit <code>--</code>master yarn-cluster <code>--</code>num-executors 1 <code>--</code>driver-memory 500m <code>--</code>executor-memory 1g <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</p>
<p>spark-submit<br><code>--</code>master yarn-cluster    //集群启动<br><code>--</code>num-executors 1        //分配多少个进程<br><code>--</code>driver-memory 500m  //driver内存<br><code>--</code>executor-memory 1g //进程内存<br><code>--</code>executor-cores 1       //开多少个核，线程<br><code>--</code>jars $(echo /usr/chl/spark8/jars/*.jar | tr ‘ ‘ ‘,’) //加载jar<br><code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>
<h4 id="11、运行截图"><a href="#11、运行截图" class="headerlink" title="11、运行截图"></a>11、运行截图</h4><p><img src="/medias/kafka2esstreaming%E6%88%AA%E5%9B%BE.PNG" alt="kafka2esstreaming截图"></p>
<p><img src="/medias/Elasticsearch%E5%90%84%E4%B8%AA%E8%8A%82%E7%82%B9%E7%8A%B6%E5%86%B5.PNG" alt="Elasticsearch各个节点状况"></p>
<h4 id="12、冲突查找快捷键"><a href="#12、冲突查找快捷键" class="headerlink" title="12、冲突查找快捷键"></a>12、冲突查找快捷键</h4><p><strong>Ctrl+Alt+Shift+N</strong></p>
<h3 id="八、xz-bigdata-es开发"><a href="#八、xz-bigdata-es开发" class="headerlink" title="八、xz_bigdata_es开发"></a>八、xz_bigdata_es开发</h3><h4 id="1、pom-xml-1"><a href="#1、pom-xml-1" class="headerlink" title="1、pom.xml"></a>1、pom.xml</h4><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_es&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_es&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
            &lt;artifactId&gt;transport&lt;/artifactId&gt;
            &lt;version&gt;6.2.3&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;io.searchbox&lt;/groupId&gt;
            &lt;artifactId&gt;jest&lt;/artifactId&gt;
            &lt;version&gt;6.3.1&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;</code></pre><h4 id="2、admin"><a href="#2、admin" class="headerlink" title="2、admin"></a>2、admin</h4><p><strong>AdminUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.admin;

import com.hsiehchou.common.file.FileCommon;
import com.hsiehchou.es.client.ESClientUtils;
import org.elasticsearch.action.admin.indices.create.CreateIndexResponse;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.Settings;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class AdminUtil {
    private static Logger LOG = LoggerFactory.getLogger(AdminUtil.class);

    public static void main(String[] args) throws Exception{
        //创建索引核mapping
        AdminUtil.buildIndexAndTypes(&quot;tanslator_test1111&quot;,&quot;tanslator_test1111&quot;, &quot;es/mapping/test.json&quot;,3,1);
        //index = 类型+日期
        //查找类  Ctrl+Shift+Alt+N
    }

    /**
     * @param index
     * @param type
     * @param path
     * @param shard
     * @param replication
     * @return
     * @throws Exception
     */
    public static boolean buildIndexAndTypes(String index,String type,String path,int shard,int replication) throws Exception{
        boolean flag ;
        TransportClient client = ESClientUtils.getClient();
        String mappingJson = FileCommon.getAbstractPath(path);

        boolean indices = AdminUtil.createIndices(client, index, shard, replication);
        if(indices){
            LOG.info(&quot;创建索引&quot;+ index + &quot;成功&quot;);
            flag = MappingUtil.addMapping(client, index, type, mappingJson);
        }
        else{
            LOG.error(&quot;创建索引&quot;+ index + &quot;失败&quot;);
            flag = false;
        }
        return flag;
    }

    /**
     * @desc 判断需要创建的index是否存在
     * */
    public static boolean indexExists(TransportClient client,String index){
        boolean ifExists = false;
        try {
            System.out.println(&quot;client===&quot; + client);
            IndicesExistsResponse existsResponse = client.admin().indices().prepareExists(index).execute().actionGet();
            ifExists = existsResponse.isExists();
        } catch (Exception e) {
            e.printStackTrace();
            LOG.error(&quot;判断index是否存在失败...&quot;);
            return ifExists;
        }
        return ifExists;
    }

    /**
     * 创建索引
     * @param client
     * @param index
     * @param shard
     * @param replication
     * @return
     */
    public static boolean createIndices(TransportClient client, String index, int shard , int replication){

        if(!indexExists(client,index)) {
            LOG.info(&quot;该index不存在，创建...&quot;);
            CreateIndexResponse createIndexResponse =null;
            try {
                createIndexResponse = client.admin().indices().prepareCreate(index)
                        .setSettings(Settings.builder()
                                .put(&quot;index.number_of_shards&quot;, shard)
                                .put(&quot;index.number_of_replicas&quot;, replication)
                                .put(&quot;index.codec&quot;, &quot;best_compression&quot;)
                                .put(&quot;refresh_interval&quot;, &quot;30s&quot;))
                        .execute().actionGet();
                return createIndexResponse.isAcknowledged();
            } catch (Exception e) {
                LOG.error(null, e);
                return false;
            }
        }
        LOG.warn(&quot;该index &quot; + index + &quot; 已经存在...&quot;);
        return false;
    }
}</code></pre><p><strong>MappingUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.admin;

import com.alibaba.fastjson.JSON;
import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;
import org.elasticsearch.action.admin.indices.mapping.put.PutMappingResponse;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;

public class MappingUtil {

    private static Logger LOG = LoggerFactory.getLogger(MappingUtil.class);
    //关闭自动添加字段，关闭后索引数据中如果有多余字段不会修改mapping,默认true
    private boolean dynamic = true;

    public static XContentBuilder buildMapping(String tableName) throws IOException {
        XContentBuilder builder = null;
        try {
            builder = XContentFactory.jsonBuilder().startObject()
                    .startObject(tableName)
                    .startObject(&quot;_source&quot;).field(&quot;enabled&quot;, true).endObject()
                    .startObject(&quot;properties&quot;)
                    .startObject(&quot;id&quot;).field(&quot;type&quot;, &quot;long&quot;).endObject()
                    .startObject(&quot;sn&quot;).field(&quot;type&quot;, &quot;text&quot;).endObject()
                    .endObject()  
                .endObject()  
                .endObject();
        } catch (IOException e) {
            e.printStackTrace();
        }
        return builder;
    }

    public static boolean addMapping(TransportClient client, String index, String type, String jsonString){
        PutMappingResponse putMappingResponse = null;
        try {
            PutMappingRequest mappingRequest = new PutMappingRequest(index)
                    .type(type).source(JSON.parseObject(jsonString));
            putMappingResponse = client.admin().indices().putMapping(mappingRequest).actionGet();
        } catch (Exception e) {
            LOG.error(null,e);
            e.printStackTrace();
            LOG.error(&quot;添加&quot; + type + &quot;的mapping失败....&quot;,e);
            return false;
        }
        boolean success = putMappingResponse.isAcknowledged();
        if (success){
            LOG.info(&quot;创建&quot; + type + &quot;的mapping成功....&quot;);
            return success;
        }
        return success;
    }

    public static void main(String[] args) throws Exception {
        /*String singleConf = ConsulConfigUtil.getSingleConf(&quot;es6.1.0/mapping/http&quot;);
        int i = singleConf.length() / 2;
        System.out.println(i);*/
    }
}</code></pre><h4 id="3、client"><a href="#3、client" class="headerlink" title="3、client"></a>3、client</h4><p><strong>ESClientUtils.java</strong></p>
<pre><code>package com.hsiehchou.es.client;

import com.hsiehchou.common.config.ConfigUtil;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.TransportAddress;
import org.elasticsearch.transport.client.PreBuiltTransportClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.Serializable;
import java.net.InetAddress;
import java.util.Properties;

/**
 * ES 客户端获取
 */
public class ESClientUtils implements Serializable{

    private static Logger LOG = LoggerFactory.getLogger(ESClientUtils.class);
    private volatile static TransportClient esClusterClient;
    private ESClientUtils(){}
    private static Properties properties;
    static {
        properties = ConfigUtil.getInstance().getProperties(&quot;es/es_cluster.properties&quot;);
    }

    public static TransportClient getClient(){
        System.setProperty(&quot;es.set.netty.runtime.available.processors&quot;, &quot;false&quot;);
        String clusterName = properties.getProperty(&quot;es.cluster.name&quot;);
        String clusterNodes1 = properties.getProperty(&quot;es.cluster.nodes1&quot;);
        String clusterNodes2 = properties.getProperty(&quot;es.cluster.nodes2&quot;);
        String clusterNodes3 = properties.getProperty(&quot;es.cluster.nodes3&quot;);
        LOG.info(&quot;clusterName:&quot;+ clusterName);
        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes1);
        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes2);
        LOG.info(&quot;clusterNodes:&quot;+ clusterNodes3);
        if(esClusterClient==null){
            synchronized (ESClientUtils.class){
                if(esClusterClient==null){
                    try{
                        Settings settings = Settings.builder()
                                .put(&quot;cluster.name&quot;, clusterName)
                                //.put(&quot;searchguard.ssl.transport.enabled&quot;, false)
                                //.put(&quot;xpack.security.user&quot;, &quot;sc_xy_mn_es:xy@66812.com&quot;)
                               // .put(&quot;transport.type&quot;,&quot;netty3&quot;)
                               // .put(&quot;http.type&quot;,&quot;netty3&quot;)
                                .put(&quot;client.transport.sniff&quot;,true).build();//开启自动嗅探功能
                        esClusterClient = new PreBuiltTransportClient(settings)
                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes1), 9300))
                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes2), 9300))
                                .addTransportAddress(new TransportAddress(InetAddress.getByName(clusterNodes3), 9300));
                        LOG.info(&quot;esClusterClient========&quot; + esClusterClient.listedNodes());
                    }catch (Exception e){
                        LOG.error(&quot;获取客户端失败&quot;,e);
                    }finally {

                    }
                }
            }
        }
        return esClusterClient;
    }

    public static void main(String[] args) {
        TransportClient client = ESClientUtils.getClient();
        System.out.println(client);
    }
}</code></pre><h4 id="4、jest-service"><a href="#4、jest-service" class="headerlink" title="4、jest/service"></a>4、jest/service</h4><p><strong>IndexTypeUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.jest.service;

import com.hsiehchou.common.config.JsonReader;
import io.searchbox.client.JestClient;

public class IndexTypeUtil {

    public static void main(String[] args) {
        IndexTypeUtil.createIndexAndType(&quot;tanslator&quot;,&quot;es/mapping/tanslator.json&quot;);
       // IndexTypeUtil.createIndexAndType(&quot;task&quot;);
      //  IndexTypeUtil.createIndexAndType(&quot;ability&quot;);
       // IndexTypeUtil.createIndexAndType(&quot;paper&quot;);
    }

    public static void createIndexAndType(String index,String jsonPath){
        try{
            JestClient jestClient = JestService.getJestClient();
            JestService.createIndex(jestClient, index);
            JestService.createIndexMapping(jestClient,index,index,getSourceFromJson(jsonPath));
        }catch (Exception e){
            e.printStackTrace();
            //LOG.error(&quot;创建索引失败&quot;,e);
        }
    }
    public static String getSourceFromJson(String path){
        return JsonReader.readJson(path);
    }

    public static String getSource(String index){
        if(index.equals(&quot;task&quot;)){
            return &quot;{\&quot;_source\&quot;: {\n&quot; +
                    &quot;    \&quot;enabled\&quot;: true\n&quot; +
                    &quot;  },\n&quot; +
                    &quot;  \&quot;properties\&quot;: {\n&quot; +
                    &quot;    \&quot;taskwordcount\&quot;: {\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;taskprice\&quot;: {\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;float\&quot;\n&quot; +
                    &quot;    }\n&quot; +
                    &quot;  }\n&quot; +
                    &quot;}&quot;;
        }

        if(index.equals(&quot;tanslator&quot;)){
            return &quot;{\n&quot; +
                    &quot;  \&quot;_source\&quot;: {\n&quot; +
                    &quot;    \&quot;enabled\&quot;: true\n&quot; +
                    &quot;  },\n&quot; +
                    &quot;  \&quot;properties\&quot;: {\n&quot; +
                    &quot;    \&quot;birthday\&quot;: {\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +
                    &quot;      \&quot;fields\&quot;: {\n&quot; +
                    &quot;        \&quot;keyword\&quot;: {\n&quot; +
                    &quot;          \&quot;ignore_above\&quot;: 256,\n&quot; +
                    &quot;          \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +
                    &quot;        }\n&quot; +
                    &quot;      }\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;createtime\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;updatetime\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;avgcooperation\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;cooperationwordcount\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;cooperation\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;cooperationtime\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;age\&quot;:{\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;    },\n&quot; +
                    &quot;    \&quot;industry\&quot;: {\n&quot; +
                    &quot;      \&quot;type\&quot;: \&quot;nested\&quot;,\n&quot; +
                    &quot;      \&quot;properties\&quot;: {\n&quot; +
                    &quot;        \&quot;industryname\&quot;: {\n&quot; +
                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +
                    &quot;          \&quot;fields\&quot;: {\n&quot; +
                    &quot;            \&quot;keyword\&quot;: {\n&quot; +
                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +
                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +
                    &quot;            }\n&quot; +
                    &quot;          }\n&quot; +
                    &quot;        },\n&quot; +
                    &quot;        \&quot;count\&quot;: {\n&quot; +
                    &quot;          \&quot;type\&quot;: \&quot;long\&quot;\n&quot; +
                    &quot;        },\n&quot; +
                    &quot;        \&quot;industryid\&quot;: {\n&quot; +
                    &quot;          \&quot;type\&quot;: \&quot;text\&quot;,\n&quot; +
                    &quot;          \&quot;fields\&quot;: {\n&quot; +
                    &quot;            \&quot;keyword\&quot;: {\n&quot; +
                    &quot;              \&quot;ignore_above\&quot;: 256,\n&quot; +
                    &quot;              \&quot;type\&quot;: \&quot;keyword\&quot;\n&quot; +
                    &quot;            }\n&quot; +
                    &quot;          }\n&quot; +
                    &quot;        }\n&quot; +
                    &quot;      }\n&quot; +
                    &quot;    }\n&quot; +
                    &quot;\n&quot; +
                    &quot;  }\n&quot; +
                    &quot;}&quot;;
        }
        return &quot;&quot;;
    }
}</code></pre><p><strong>JestService.java</strong></p>
<pre><code>package com.hsiehchou.es.jest.service;

import com.hsiehchou.common.file.FileCommon;
import com.google.gson.GsonBuilder;
import io.searchbox.action.Action;
import io.searchbox.client.JestClient;
import io.searchbox.client.JestClientFactory;
import io.searchbox.client.JestResult;
import io.searchbox.client.config.HttpClientConfig;
import io.searchbox.core.*;
import io.searchbox.indices.CreateIndex;
import io.searchbox.indices.DeleteIndex;
import io.searchbox.indices.IndicesExists;
import io.searchbox.indices.mapping.GetMapping;
import io.searchbox.indices.mapping.PutMapping;
import org.apache.commons.lang.StringUtils;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.aggregations.AggregationBuilder;
import org.elasticsearch.search.aggregations.AggregationBuilders;
import org.elasticsearch.search.builder.SearchSourceBuilder;
import org.elasticsearch.search.sort.SortOrder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.List;
import java.util.Map;

public class JestService {

    private static Logger LOG = LoggerFactory.getLogger(JestService.class);


    /**
     * 获取JestClient对象
     *
     * @return
     */
    public static JestClient getJestClient() {

        JestClientFactory factory = new JestClientFactory();
        factory.setHttpClientConfig(new HttpClientConfig
                .Builder(&quot;http://hadoop1:9200&quot;)
                //.defaultCredentials(&quot;sc_xy_mn_es&quot;,&quot;xy@66812.com&quot;)
                .gson(new GsonBuilder().setDateFormat(&quot;yyyy-MM-dd&#39;T&#39;hh:mm:ss&quot;).create())
                .connTimeout(1500)
                .readTimeout(3000)
                .multiThreaded(true)
                .build());
        return factory.getObject();
    }


    public static void main(String[] args) throws Exception {
        JestClient jestClient = null;
//        Map&lt;String, Long&gt; stringLongMap = null;
        List&lt;Map&lt;String, Object&gt;&gt; maps = null;
        try {
            jestClient = JestService.getJestClient();
           /* SearchResult aggregation = JestService.aggregation(jestClient,
                    &quot;wechat&quot;,
                    &quot;wechat&quot;,
                    &quot;collect_time&quot;);
            stringLongMap = ResultParse.parseAggregation(aggregation);*/
           /* SearchResult search = search(jestClient,
                    &quot;wechat&quot;,
                    &quot;wechat&quot;,
                    &quot;id&quot;,
                    &quot;65a3d548bd3e42b1972191bc2bd2829b&quot;,
                    &quot;collect_time&quot;,
                    &quot;desc&quot;,
                    1,
                    2);*/
            /*SearchResult search = search(jestClient,
                    &quot;&quot;,
                    &quot;&quot;,
                    &quot;phone_mac&quot;,
                    &quot;aa-aa-aa-aa-aa-aa&quot;,
                    &quot;collect_time&quot;,
                    &quot;asc&quot;,
                    1,
                    1000);*/

//            System.out.println(indexExists(jestClient,&quot;wechat&quot;));
            System.out.println(&quot;wechat数据量：&quot;+count(jestClient,&quot;wechat&quot;,&quot;wechat&quot;));
            System.out.println(aggregation(jestClient,&quot;wechat&quot;,&quot;wechat&quot;, &quot;phone&quot;));

            String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};
//            try{
            SearchResult search = JestService.search(jestClient,
                        &quot;&quot;,
                        &quot;&quot;,
                        &quot;phone_mac.keyword&quot;,
                        &quot;aa-aa-aa-aa-aa-aa&quot;,
                        &quot;collect_time&quot;,
                        &quot;asc&quot;,
                        1,
                        2000);
                maps = ResultParse.parseSearchResultOnly(search);
                System.out.println(maps.size());
                System.out.println(maps);
            } catch (Exception e) {
                e.printStackTrace();
            } finally {
                JestService.closeJestClient(jestClient);
            }
        System.out.println(maps);
//        } catch (Exception e) {
//            e.printStackTrace();
//        }finally {
//            JestService.closeJestClient(jestClient);
//        }
//        System.out.println(stringLongMap);
    }


    /**
     * 统计一个索引所有数据
     * @param jestClient
     * @param indexName
     * @param typeName
     * @return
     * @throws Exception
     */

    public static Long count(JestClient jestClient,
                             String indexName,
                             String typeName) throws Exception {
        Count count = new Count.Builder()
                .addIndex(indexName)
                .addType(typeName)
                .build();
        CountResult results = jestClient.execute(count);

        return results.getCount().longValue();
    }


    /**
     * 聚合分组查询
     * @param jestClient
     * @param indexName
     * @param typeName
     * @param field
     * @return
     * @throws Exception
     */
    public static SearchResult  aggregation(JestClient jestClient, String indexName, String typeName, String field) throws Exception {

        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
        //分组聚合API
        AggregationBuilder group1 = AggregationBuilders.terms(&quot;group1&quot;).field(field);
        //group1.subAggregation(AggregationBuilders.terms(&quot;group2&quot;).field(query));
        searchSourceBuilder.aggregation(group1);
        searchSourceBuilder.size(0);
        System.out.println(searchSourceBuilder.toString());
        Search search = new Search.Builder(searchSourceBuilder.toString())
                .addIndex(indexName)
                .addType(typeName).build();
        SearchResult result = jestClient.execute(search);
        return result;
    }


    //基础封装
    public static SearchResult search(
            JestClient jestClient,
            String indexName,
            String typeName,
            String field,
            String fieldValue,
            String sortField,
            String sortValue,
            int pageNumber,
            int pageSize,
            String[] includes) {
        //构造一个查询体  封装的就是查询语句
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        searchSourceBuilder.fetchSource(includes,new String[0]);

        //查询构造器
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        if(StringUtils.isEmpty(field)){
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());
        }else{
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));
        }

        searchSourceBuilder.query(boolQueryBuilder);
        //定义分页
        //从什么时候开始
        searchSourceBuilder.from((pageNumber-1)*pageSize);
        searchSourceBuilder.size(pageSize);

        //设置排序
        if(&quot;desc&quot;.equals(sortValue)){
            searchSourceBuilder.sort(sortField,SortOrder.DESC);
        }else{
            searchSourceBuilder.sort(sortField,SortOrder.ASC);
        }

        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());

        //构造一个查询执行器
        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());
        //设置indexName typeName
        if(StringUtils.isNotBlank(indexName)){
            builder.addIndex(indexName);
        }
        if(StringUtils.isNotBlank(typeName)){
            builder.addType(typeName);
        }

        Search build = builder.build();
        SearchResult searchResult = null;
        try {
            searchResult = jestClient.execute(build);
        } catch (IOException e) {
            LOG.error(&quot;查询失败&quot;,e);
        }
        return searchResult;
    }

    //基础封装
    public static SearchResult search(
            JestClient jestClient,
            String indexName,
            String typeName,
            String field,
            String fieldValue,
            String sortField,
            String sortValue,
            int pageNumber,
            int pageSize) {

        //构造一个查询体  封装的就是查询语句
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
        //查询构造器
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        if(StringUtils.isEmpty(field)){
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());
        }else{
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));
        }

        searchSourceBuilder.query(boolQueryBuilder);
        //定义分页
        //从什么时候开始
        searchSourceBuilder.from((pageNumber-1)*pageSize);
        searchSourceBuilder.size(pageSize);

        //设置排序
        if(&quot;desc&quot;.equals(sortValue)){
            searchSourceBuilder.sort(sortField,SortOrder.DESC);
        }else{
            searchSourceBuilder.sort(sortField,SortOrder.ASC);
        }

        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());

        //构造一个查询执行器
        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());
        //设置indexName typeName
        if(StringUtils.isNotBlank(indexName)){
            builder.addIndex(indexName);
        }
        if(StringUtils.isNotBlank(typeName)){
            builder.addType(typeName);
        }

        Search build = builder.build();
        SearchResult searchResult = null;
        try {
            searchResult = jestClient.execute(build);
        } catch (IOException e) {
            LOG.error(&quot;查询失败&quot;,e);
        }
        return searchResult;
    }


   /* //基础封装
    public static SearchResult search(
            JestClient jestClient,
            String indexName,
            String typeName,
            String field,
            String fieldValue,
            String sortField,
            String sortValue,
            int pageNumber,
            int pageSize) {

        //构造一个查询体  封装的就是查询语句
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        //查询构造器
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        if(StringUtils.isEmpty(field)){
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.matchAllQuery());
        }else{
            boolQueryBuilder = boolQueryBuilder.must(QueryBuilders.termQuery(field,fieldValue));
        }
        searchSourceBuilder.query(boolQueryBuilder);
        //定义分页
        //从什么时候开始
        searchSourceBuilder.from((pageNumber-1)*pageSize);
        searchSourceBuilder.size(pageSize);
        //设置排序
        if(&quot;desc&quot;.equals(sortValue)){
            searchSourceBuilder.sort(sortField,SortOrder.DESC);
        }else{
            searchSourceBuilder.sort(sortField,SortOrder.ASC);
        }


        System.out.println(&quot;sql =====&quot; + searchSourceBuilder.toString());

        //构造一个查询执行器
        Search.Builder builder = new Search.Builder(searchSourceBuilder.toString());

        //设置indexName typeName
        if(StringUtils.isNotBlank(indexName)){
            builder.addIndex(indexName);
        }
        if(StringUtils.isNotBlank(typeName)){
            builder.addType(typeName);
        }

        Search build = builder.build();
        SearchResult searchResult = null;
        try {
            searchResult = jestClient.execute(build);
        } catch (IOException e) {
            LOG.error(&quot;查询失败&quot;,e);
        }
        return searchResult;
    }
*/

    /**
     * 判断索引是否存在
     *
     * @param jestClient
     * @param indexName
     * @return
     * @throws Exception
     */
    public static boolean indexExists(JestClient jestClient, String indexName) {
        JestResult result = null;
        try {
            Action action = new IndicesExists.Builder(indexName).build();
            result = jestClient.execute(action);
        } catch (IOException e) {
            LOG.error(null, e);
        }
        return result.isSucceeded();
    }


    /**
     * 创建索引
     *
     * @param jestClient
     * @param indexName
     * @return
     * @throws Exception
     */
    public static boolean createIndex(JestClient jestClient, String indexName) throws Exception {

        if (!JestService.indexExists(jestClient, indexName)) {
            JestResult jr = jestClient.execute(new CreateIndex.Builder(indexName).build());
            return jr.isSucceeded();
        } else {
            LOG.info(&quot;该索引已经存在&quot;);
            return false;
        }
    }

    public static boolean createIndexWithSettingsMapAndMappingsString(JestClient jestClient, String indexName, String type, String path) throws Exception {

        // String mappingJson = &quot;{\&quot;type1\&quot;: {\&quot;_source\&quot;:{\&quot;enabled\&quot;:false},\&quot;properties\&quot;:{\&quot;field1\&quot;:{\&quot;type\&quot;:\&quot;keyword\&quot;}}}}&quot;;
        String mappingJson = FileCommon.getAbstractPath(path);
        String realMappingJson = &quot;{&quot; + type + &quot;:&quot; + mappingJson + &quot;}&quot;;
        System.out.println(realMappingJson);
        CreateIndex createIndex = new CreateIndex.Builder(indexName)
                .mappings(realMappingJson)
                .build();
        JestResult jr = jestClient.execute(createIndex);
        return jr.isSucceeded();
    }


    /**
     * Put映射
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @param source
     * @return
     * @throws Exception
     */
    public static boolean createIndexMapping(JestClient jestClient, String indexName, String typeName, String source) throws Exception {

        PutMapping putMapping = new PutMapping.Builder(indexName, typeName, source).build();
        JestResult jr = jestClient.execute(putMapping);
        return jr.isSucceeded();
    }

    /**
     * Get映射
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @return
     * @throws Exception
     */
    public static String getIndexMapping(JestClient jestClient, String indexName, String typeName) throws Exception {

        GetMapping getMapping = new GetMapping.Builder().addIndex(indexName).addType(typeName).build();
        JestResult jr = jestClient.execute(getMapping);
        return jr.getJsonString();
    }

    /**
     * 索引文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @return
     * @throws Exception
     */
    public static boolean index(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, Object&gt;&gt; listMaps) throws Exception {

        Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);
        for (Map&lt;String, Object&gt; map : listMaps) {
            if (map != null &amp;&amp; map.containsKey(idField)) {
                Object o = map.get(idField);
                Index index = new Index.Builder(map).id(map.get(idField).toString()).build();
                bulk.addAction(index);
            }
        }
        BulkResult br = jestClient.execute(bulk.build());
        return br.isSucceeded();
    }


    /**
     * 索引文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @return
     * @throws Exception
     */
    public static boolean indexString(JestClient jestClient, String indexName, String typeName, String idField, List&lt;Map&lt;String, String&gt;&gt; listMaps) throws Exception {
        if (listMaps != null &amp;&amp; listMaps.size() &gt; 0) {
            Bulk.Builder bulk = new Bulk.Builder().defaultIndex(indexName).defaultType(typeName);
            for (Map&lt;String, String&gt; map : listMaps) {
                if (map != null &amp;&amp; map.containsKey(idField)) {
                    Index index = new Index.Builder(map).id(map.get(idField)).build();
                    bulk.addAction(index);
                }
            }
            BulkResult br = jestClient.execute(bulk.build());
            return br.isSucceeded();
        } else {
            return false;
        }
    }

    /**
     * 索引文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @return
     * @throws Exception
     */
    public static boolean indexOne(JestClient jestClient, String indexName, String typeName, String id, Map&lt;String, Object&gt; map) {
        Index.Builder builder = new Index.Builder(map);
        builder.id(id);
        builder.refresh(true);
        Index index = builder.index(indexName).type(typeName).build();
        try {
            JestResult result = jestClient.execute(index);
            if (result != null &amp;&amp; !result.isSucceeded()) {
                throw new RuntimeException(result.getErrorMessage() + &quot;插入更新索引失败!&quot;);
            }
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
        return true;
    }


    /**
     * 搜索文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @param query
     * @return
     * @throws Exception
     */
    public static SearchResult search(JestClient jestClient, String indexName, String typeName, String query) throws Exception {

        Search search = new Search.Builder(query)
                .addIndex(indexName)
                .addType(typeName)
                .build();
        return jestClient.execute(search);
    }

    /**
     * Get文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @param id
     * @return
     * @throws Exception
     */
    public static JestResult get(JestClient jestClient, String indexName, String typeName, String id) throws Exception {

        Get get = new Get.Builder(indexName, id).type(typeName).build();
        return jestClient.execute(get);
    }

    /**
     * Delete索引
     *
     * @param jestClient
     * @param indexName
     * @return
     * @throws Exception
     */
    public boolean delete(JestClient jestClient, String indexName) throws Exception {

        JestResult jr = jestClient.execute(new DeleteIndex.Builder(indexName).build());
        return jr.isSucceeded();
    }

    /**
     * Delete文档
     *
     * @param jestClient
     * @param indexName
     * @param typeName
     * @param id
     * @return
     * @throws Exception
     */
    public static boolean delete(JestClient jestClient, String indexName, String typeName, String id) throws Exception {

        DocumentResult dr = jestClient.execute(new Delete.Builder(id).index(indexName).type(typeName).build());
        return dr.isSucceeded();
    }

    /**
     * 关闭JestClient客户端
     *
     * @param jestClient
     * @throws Exception
     */
    public static void closeJestClient(JestClient jestClient) {

        if (jestClient != null) {
            jestClient.shutdownClient();
        }
    }


    public static String query = &quot;{\n&quot; +
            &quot;  \&quot;size\&quot;: 1,\n&quot; +
            &quot;  \&quot;query\&quot;: {\n&quot; +
            &quot;     \&quot;match\&quot;: {\n&quot; +
            &quot;       \&quot;taskexcuteid\&quot;: \&quot;89899143\&quot;\n&quot; +
            &quot;     }\n&quot; +
            &quot;  },\n&quot; +
            &quot;  \&quot;aggs\&quot;: {\n&quot; +
            &quot;    \&quot;count\&quot;: {\n&quot; +
            &quot;      \&quot;terms\&quot;: {\n&quot; +
            &quot;        \&quot;field\&quot;: \&quot;source.keyword\&quot;\n&quot; +
            &quot;      },\n&quot; +
            &quot;      \&quot;aggs\&quot;: {\n&quot; +
            &quot;        \&quot;sum_price\&quot;: {\n&quot; +
            &quot;          \&quot;sum\&quot;: {\n&quot; +
            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +
            &quot;          }\n&quot; +
            &quot;        },\n&quot; +
            &quot;        \&quot;sum_wordcount\&quot;: {\n&quot; +
            &quot;          \&quot;sum\&quot;: {\n&quot; +
            &quot;            \&quot;field\&quot;: \&quot;taskwordcount\&quot;\n&quot; +
            &quot;          }\n&quot; +
            &quot;        },\n&quot; +
            &quot;        \&quot;avg_taskprice\&quot;: {\n&quot; +
            &quot;          \&quot;avg\&quot;: {\n&quot; +
            &quot;            \&quot;field\&quot;: \&quot;taskprice\&quot;\n&quot; +
            &quot;          }\n&quot; +
            &quot;        }\n&quot; +
            &quot;      }\n&quot; +
            &quot;    }\n&quot; +
            &quot;  }\n&quot; +
            &quot;}&quot;;
}</code></pre><p><strong>ResultParse.java</strong></p>
<pre><code>package com.hsiehchou.es.jest.service;

import com.google.gson.Gson;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonPrimitive;
import io.searchbox.client.JestClient;
import io.searchbox.client.JestResult;
import io.searchbox.core.SearchResult;
import io.searchbox.core.search.aggregation.MetricAggregation;
import io.searchbox.core.search.aggregation.TermsAggregation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;

public class ResultParse {

    private static Logger LOG = LoggerFactory.getLogger(ResultParse.class);

    public static void main(String[] args) throws Exception {
        JestClient jestClient = JestService.getJestClient();

        /*long l = System.currentTimeMillis();
        JestClient jestClient = JestClientUtil.getJestClient();
        System.out.println(jestClient);
        String json =&quot;{\n&quot; +
                &quot;  \&quot;size\&quot;: 1, \n&quot; +
                &quot;  \&quot;query\&quot;: {\n&quot; +
                &quot;    \&quot;query_string\&quot;: {\n&quot; +
                &quot;      \&quot;query\&quot;: \&quot;中文\&quot;\n&quot; +
                &quot;    }\n&quot; +
                &quot;  },\n&quot; +
                &quot;  \&quot;highlight\&quot;: {\n&quot; +
                &quot;    \&quot;pre_tags\&quot; : [ \&quot;&lt;red&gt;\&quot; ],\n&quot; +
                &quot;    \&quot;post_tags\&quot; : [ \&quot;&lt;/red&gt;\&quot; ],\n&quot; +
                &quot;    \&quot;fields\&quot;:{\n&quot; +
                &quot;      \&quot;secondlanguage\&quot;: {}\n&quot; +
                &quot;      ,\&quot;firstlanguage\&quot;: {}\n&quot; +
                &quot;    }\n&quot; +
                &quot;  }\n&quot; +
                &quot;}&quot;;
        SearchResult search = JestService.search(jestClient, ES_INDEX.TANSLATOR_TEST, ES_INDEX.TANSLATOR_TEST,json);
        ResultParse.parseSearchResult(search);
        jestClient.shutdownClient();
        long l1 = System.currentTimeMillis();
        System.out.println(l1-l);*/
    }

    public static Map&lt;String,Object&gt; parseGet(JestResult getResult){
        Map&lt;String,Object&gt; map = null;
        JsonObject jsonObject = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);
        if(jsonObject != null){
            map = new HashMap&lt;String,Object&gt;();
            //System.out.println(jsonObject);
            Set&lt;Map.Entry&lt;String, JsonElement&gt;&gt; entries = jsonObject.entrySet();
            for(Map.Entry&lt;String, JsonElement&gt; entry:entries){
                JsonElement value = entry.getValue();
                if(value.isJsonPrimitive()){
                    JsonPrimitive value1 = (JsonPrimitive) value;
                  //  LOG.error(&quot;转换前==========&quot; + value1);
                    if( value1.isString() ){
                       // LOG.error(&quot;转换后==========&quot; + value1.getAsString());
                        map.put(entry.getKey(),value1.getAsString());
                    }else{
                        map.put(entry.getKey(),value1);
                    }
                }else{
                    map.put(entry.getKey(),value);
                }
             }
        }
        return map;
    }

    public static Map&lt;String,Object&gt; parseGet2map(JestResult getResult){

        JsonObject source = getResult.getJsonObject().getAsJsonObject(&quot;_source&quot;);
        Gson gson = new Gson();
        Map map = gson.fromJson(source, Map.class);
        return map;
    }

    /**
     * 解析listMap
     * 结果格式为  {hits=0, total=0, data=[]}
     * @param search
     * @return
     */
    public static List&lt;Map&lt;String,Object&gt;&gt; parseSearchResultOnly(SearchResult search){

        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();
        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);
        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){
            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;
            list.add(source);
        }
        return list;
    }

    /**
     * 解析listMap
     * 结果格式为  {hits=0, total=0, data=[]}
     * @param search
     * @return
     */
    public static Map&lt;String,Long&gt; parseAggregation(SearchResult search){
        Map&lt;String,Long&gt; mapResult = new HashMap&lt;&gt;();
        MetricAggregation aggregations = search.getAggregations();
        TermsAggregation group1 = aggregations.getTermsAggregation(&quot;group1&quot;);
        List&lt;TermsAggregation.Entry&gt; buckets = group1.getBuckets();
        buckets.forEach(x-&gt;{
            String key = x.getKey();
            Long count = x.getCount();
            mapResult.put(key,count);
        });
        return mapResult;
    }

    /**
     * 解析listMap
     * 结果格式为  {hits=0, total=0, data=[]}
     * @param search
     * @return
     */
    public static Map&lt;String,Object&gt; parseSearchResult(SearchResult search){

        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();
        List&lt;Map&lt;String,Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;();

        Long total = search.getTotal();
        map.put(&quot;total&quot;,total);
        List&lt;SearchResult.Hit&lt;Object, Void&gt;&gt; hits = search.getHits(Object.class);
        map.put(&quot;hits&quot;,hits.size());
        for(SearchResult.Hit&lt;Object, Void&gt; hit : hits){
            Map&lt;String, List&lt;String&gt;&gt; highlight = hit.highlight;
            Map&lt;String,Object&gt; source = (Map&lt;String,Object&gt;)hit.source;
            source.put(&quot;highlight&quot;,highlight);
            list.add(source);
        }
        map.put(&quot;data&quot;,list);
        return map;
    }
}</code></pre><h4 id="5、search"><a href="#5、search" class="headerlink" title="5、search"></a>5、search</h4><p><strong>BuilderUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.search;

import org.apache.commons.lang.StringUtils;
import org.elasticsearch.action.search.SearchRequestBuilder;
import org.elasticsearch.client.transport.TransportClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BuilderUtil {

    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);

    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String index, String type){
        SearchRequestBuilder builder = null;
        try {
            if (StringUtils.isNotBlank(index)) {
                builder = client.prepareSearch(index.split(&quot;,&quot;));
            } else {
                builder = client.prepareSearch();
            }
            if (StringUtils.isNotBlank(type)) {
                builder.setTypes(type.split(&quot;,&quot;));
            }
        } catch (Exception e) {
            LOG.error(null, e);
        }
        return builder;
    }

    public static SearchRequestBuilder getSearchBuilder(TransportClient client, String[] indexs, String type){
        SearchRequestBuilder builder = null;
        try {
            if (indexs.length&gt;0) {
                for(String index:indexs){
                    builder = client.prepareSearch(index);
                }
            } else {
                builder = client.prepareSearch();
            }
            if (StringUtils.isNotBlank(type)) {
                builder.setTypes(type);
            }
        } catch (Exception e) {
            LOG.error(null, e);
        }
        return builder;
    }
}</code></pre><p><strong>QueryUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.search;

import com.hsiehchou.es.utils.UnicodeUtil;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.index.query.QueryStringQueryBuilder;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Map;

public class QueryUtil {

    private static Logger LOG = LoggerFactory.getLogger(QueryUtil.class);

    /**
     * EQ   等於
     * NEQ  不等於
     * GE   大于等于
     * GT   大于
     * LE   小于等于
     * LT   小于
     * RANGE 区间范围
     */
    public static enum OPREATOR {EQ, NEQ,WILDCARD, GE, LE, GT, LT, FUZZY, RANGE, IN, PREFIX}

    /**
     * @param paramMap
     * @return
     */
    public static BoolQueryBuilder getSearchParam(Map&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramMap) {

        BoolQueryBuilder qb = QueryBuilders.boolQuery();

        if (null != paramMap &amp;&amp; !paramMap.isEmpty()) {

            for (Map.Entry&lt;OPREATOR, Map&lt;String, Object&gt;&gt; paramEntry : paramMap.entrySet()) {

                OPREATOR key = paramEntry.getKey();
                Map&lt;String, Object&gt; fieldMap = paramEntry.getValue();

                for (Map.Entry&lt;String, Object&gt; fieldEntry : fieldMap.entrySet()) {

                    String field = fieldEntry.getKey();
                    Object value = fieldEntry.getValue();

                    switch (key) {
                        case EQ:/**等於查詢 equale**/
                            qb.must(QueryBuilders.matchPhraseQuery(field, value).slop(0));
                            break;
                        case NEQ:/**不等於查詢 not equale**/
                            qb.mustNot(QueryBuilders.matchQuery(field, value));
                            break;
                        case GE: /**大于等于查詢  great than or equal to**/
                            qb.must(QueryBuilders.rangeQuery(field).gte(value));
                            break;
                        case LE: /**小于等于查詢 less than or equal to**/
                            qb.must(QueryBuilders.rangeQuery(field).lte(value));
                            break;
                        case GT: /**大于查詢**/
                            qb.must(QueryBuilders.rangeQuery(field).gt(value));
                            break;
                        case LT: /**小于查詢**/
                            qb.must(QueryBuilders.rangeQuery(field).lt(value));
                            break;
                        case FUZZY:
                            String text = String.valueOf(value);
                            if (!UnicodeUtil.hasChinese(text)) {
                                text = &quot;*&quot; + text + &quot;*&quot;;
                            }
                            text = QueryParser.escape(text);
                            qb.must(new QueryStringQueryBuilder(text).field(field));
                            break;

                        case RANGE: /**区间查詢**/
                            String[] split = value.toString().split(&quot;,&quot;);
                            if(split.length==2){
                                qb.must(QueryBuilders.rangeQuery(field).from(Long.valueOf(split[0]))
                                        .to(Long.valueOf(split[1])));
                            }
                             /*  if (value instanceof Map) {
                                Map&lt;String, Object&gt; rangMap = (Map&lt;String, Object&gt;) value;
                                qb.must(QueryBuilders.rangeQuery(field).from(rangMap.get(&quot;ge&quot;))
                                        .to(rangMap.get(&quot;le&quot;)));
                            }*/
                            break;

                        case PREFIX: /**前缀查詢**/
                            qb.must(QueryBuilders.prefixQuery(field, String.valueOf(value)));
                            break;

                        case IN:
                            qb.must(QueryBuilders.termsQuery(field, (Object[]) value));
                            break;

                        default:
                            qb.must(QueryBuilders.matchQuery(field, value));
                            break;
                    }
                }
            }
        }
        return qb;
    }
}</code></pre><p><strong>ResponseParse.java</strong></p>
<pre><code>package com.hsiehchou.es.search;

import org.elasticsearch.action.get.GetResponse;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Map;

public class ResponseParse {

    private static Logger LOG = LoggerFactory.getLogger(BuilderUtil.class);

    public static Map&lt;String, Object&gt; parseGetResponse(GetResponse getResponse){
        Map&lt;String, Object&gt; source = null;
        try {
            source = getResponse.getSource();
        } catch (Exception e) {
            LOG.error(null,e);
        }
        return source;
    }
}</code></pre><p><strong>SearchUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.search;

import com.hsiehchou.es.client.ESClientUtils;
import org.elasticsearch.action.get.GetRequestBuilder;
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.action.search.SearchRequestBuilder;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.MatchQueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.SearchHits;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class SearchUtil {

    private static Logger LOG = LoggerFactory.getLogger(SearchUtil.class);

    private static TransportClient client = ESClientUtils.getClient();

    public static void main(String[] args) {
        TransportClient client = ESClientUtils.getClient();
        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client, &quot;wechat&quot;, &quot;wechat&quot;, &quot;phone_mac&quot;, &quot;aa-aa-aa-aa-aa-aa&quot;);
        System.out.println(maps);
        /* long l = System.currentTimeMillis();
        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);
        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - l));

        long lll = System.currentTimeMillis();
        searchSingleData(&quot;tanslator&quot;, &quot;tanslator&quot;,&quot;4e1117d7-c434-48a7-9134-45f7c90f94ee_TR1100397895_2&quot;);
        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - lll));

        long ll = System.currentTimeMillis();
        List&lt;Map&lt;String, Object&gt;&gt; maps = searchSingleData(client,&quot;tanslator&quot;, &quot;tanslator&quot;, &quot;iolid&quot;, &quot;TR1100397895&quot;);
        System.out.println(&quot;消耗时间&quot; + (System.currentTimeMillis() - ll));
        System.out.println(maps);*/
    }

    /**
     * 查询单条数据
     * @param index  索引
     * @param type   表名
     * @param id     字段
     * @return
     */
    public static GetResponse searchSingleData(String index, String type, String id) {
        GetResponse response = null;
        try {
            GetRequestBuilder builder = null;
            builder = client.prepareGet(index, type, id);
            response = builder.execute().actionGet();
        } catch (Exception e) {
            LOG.error(null, e);
        }
        return response;
    }

    /**
     * @param index
     * @param type
     * @param field
     * @param value
     * @return
     */
    public static List&lt;Map&lt;String, Object&gt;&gt; searchSingleData(TransportClient client,String index, String type,String field, String value) {
        List&lt;Map&lt;String, Object&gt;&gt; result = new ArrayList&lt;&gt;();
        try {
            SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);
            MatchQueryBuilder matchQueryBuilder = QueryBuilders.matchQuery(field, value);
            builder.setQuery(matchQueryBuilder).setExplain(false);
            SearchResponse searchResponse = builder.execute().actionGet();
            SearchHits hits = searchResponse.getHits();
            SearchHit[] searchHists = hits.getHits();
            for (SearchHit sh : searchHists) {
                result.add(sh.getSourceAsMap());
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOG.error(null, e);
        }
        return result;
    }

    /**
     * 多条件查詢
     * @param index
     * @param type
     * @param paramMap 组合查询条件
     * @return
     */
    public static SearchResponse searchListData(String index, String type,
                                                Map&lt;QueryUtil.OPREATOR,Map&lt;String,Object&gt;&gt; paramMap) {

        SearchRequestBuilder builder = BuilderUtil.getSearchBuilder(client,index,type);
        builder.setQuery(QueryUtil.getSearchParam(paramMap)).setExplain(false);
        SearchResponse searchResponse = builder.get();

        return searchResponse;
    }

    /**
     * 多条件查詢
     * @param index
     * @param type
     * @param paramMap 组合查询条件
     * @return
     */
    public static SearchResponse searchListData1(String index, String type, Map&lt;String,String&gt; paramMap) {

        BoolQueryBuilder qb = QueryBuilders.boolQuery();
        qb.must(QueryBuilders.matchQuery(&quot;&quot;, &quot;&quot;));

        BoolQueryBuilder qb1 = QueryBuilders.boolQuery();
        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));
        qb1.should(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));

        qb.must(qb1);
        return null;
    }
}</code></pre><h4 id="6、utils"><a href="#6、utils" class="headerlink" title="6、utils"></a>6、utils</h4><p><strong>ESresultUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.utils;

import org.apache.commons.lang.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Map;

public class ESresultUtil {

    private static Logger LOG = LoggerFactory.getLogger(ESresultUtil.class);

    public static Long getLong(Map&lt;String,Object&gt; esMAp,String field){

        Long valueLong = 0L;
        if(esMAp!=null &amp;&amp; esMAp.size()&gt;0){
            if(esMAp.containsKey(field)){
                 Object value = esMAp.get(field);
                 if(value!=null &amp;&amp; StringUtils.isNotBlank(value.toString())){
                     valueLong = Long.valueOf(value.toString());
                 }
            }
        }
        return valueLong;
    }
}</code></pre><p><strong>UnicodeUtil.java</strong></p>
<pre><code>package com.hsiehchou.es.utils;

import java.util.regex.Pattern;

public class UnicodeUtil {

    // 根据Unicode编码完美的判断中文汉字和符号
    private static boolean isChinese(char c) {
        Character.UnicodeBlock ub = Character.UnicodeBlock.of(c);
        if (ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS || ub == Character.UnicodeBlock.CJK_COMPATIBILITY_IDEOGRAPHS
                || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_A || ub == Character.UnicodeBlock.CJK_UNIFIED_IDEOGRAPHS_EXTENSION_B
                || ub == Character.UnicodeBlock.CJK_SYMBOLS_AND_PUNCTUATION || ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS
                || ub == Character.UnicodeBlock.GENERAL_PUNCTUATION) {
            return true;
        }
        return false;
    }

    // 完整的判断中文汉字和符号
    public static boolean isChinese(String strName) {
        char[] ch = strName.toCharArray();
        for (int i = 0; i &lt; ch.length; i++) {
            char c = ch[i];
            if (isChinese(c)) {
                return true;
            }
        }
        return false;
    }

    // 完整的判断中文汉字和符号
    public static boolean hasChinese(String strName) {
        char[] ch = strName.toCharArray();
        for (int i = 0; i &lt; ch.length; i++) {
            char c = ch[i];
            if (isChinese(c)) {
                return true;
            }
        }
        return false;
    }

    // 只能判断部分CJK字符（CJK统一汉字）
    public static boolean isChineseByREG(String str) {
        if (str == null) {
            return false;
        }
        Pattern pattern = Pattern.compile(&quot;[\\u4E00-\\u9FBF]+&quot;);
        return pattern.matcher(str.trim()).find();
    }

    // 只能判断部分CJK字符（CJK统一汉字）
    /*    public static boolean isChineseByName(String str) {
        if (str == null) {
            return false;
        }
        // 大小写不同：\\p 表示包含，\\P 表示不包含
        // \\p{Cn} 的意思为 Unicode 中未被定义字符的编码，\\P{Cn} 就表示 Unicode中已经被定义字符的编码
        String reg = &quot;\\p{InCJK Unified Ideographs}&amp;&amp;\\P{Cn}&quot;;
        Pattern pattern = Pattern.compile(reg);
        return pattern.matcher(str.trim()).find();
    }*/

    public static void main(String[] args) {
        System.out.println(hasChinese(&quot;aa表aa&quot;));
    }
}</code></pre><h4 id="7、V2"><a href="#7、V2" class="headerlink" title="7、V2"></a>7、V2</h4><p><strong>ElasticSearchService.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import com.hsiehchou.es.client.ESClientUtils;
import org.apache.commons.collections.map.HashedMap;
import org.apache.commons.lang.StringUtils;
import org.elasticsearch.action.admin.indices.create.CreateIndexRequest;
import org.elasticsearch.action.admin.indices.delete.DeleteIndexResponse;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;
import org.elasticsearch.action.bulk.BulkRequestBuilder;
import org.elasticsearch.action.search.SearchRequestBuilder;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.update.UpdateRequest;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.text.Text;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.SearchHit;
import org.elasticsearch.search.SearchHits;
import org.elasticsearch.search.aggregations.AggregationBuilder;
import org.elasticsearch.search.aggregations.AggregationBuilders;
import org.elasticsearch.search.aggregations.bucket.terms.Terms;
import org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder;
import org.elasticsearch.search.fetch.subphase.highlight.HighlightField;
import org.elasticsearch.search.sort.SortBuilder;
import org.elasticsearch.search.sort.SortOrder;

import java.util.*;

/**
 *  ES检索封装
 */
public class ElasticSearchService {

    private final static int MAX = 10000;

    private static TransportClient client = ESClientUtils.getClient();

    /**
     * 功能描述：新建索引
     * @param indexName 索引名
     */
    public void createIndex(String indexName) {
        client.admin().indices().create(new CreateIndexRequest(indexName))
                .actionGet();
    }

    /**
     * 功能描述：新建索引
     * @param index 索引名
     * @param type 类型
     */
    public void createIndex(String index, String type) {
        client.prepareIndex(index, type).setSource().get();
    }

    /**
     * 功能描述：删除索引
     * @param index 索引名
     */
    public void deleteIndex(String index) {
        if (indexExist(index)) {
            DeleteIndexResponse dResponse = client.admin().indices().prepareDelete(index)
                    .execute().actionGet();
            if (!dResponse.isAcknowledged()) {

            }
        } else {

        }
    }

    /**
     * 功能描述：验证索引是否存在
     * @param index 索引名
     */
    public boolean indexExist(String index) {
        IndicesExistsRequest inExistsRequest = new IndicesExistsRequest(index);
        IndicesExistsResponse inExistsResponse = client.admin().indices()
                .exists(inExistsRequest).actionGet();
        return inExistsResponse.isExists();
    }

    /**
     * 功能描述：插入数据
     * @param index 索引名
     * @param type 类型
     * @param json 数据
     */
    public void insertData(String index, String type, String json) {
       client.prepareIndex(index, type)
                .setSource(json)
                .get();
    }

    /**
     * 功能描述：插入数据
     * @param index 索引名
     * @param type 类型
     * @param _id 数据id
     * @param json 数据
     */
    public void insertData(String index, String type, String _id, String json) {
        client.prepareIndex(index, type).setId(_id)
                .setSource(json)
                .get();
    }

    /**
     * 功能描述：更新数据
     * @param index 索引名
     * @param type 类型
     * @param _id 数据id
     * @param json 数据
     */
    public void updateData(String index, String type, String _id, String json) throws Exception {
        try {
            UpdateRequest updateRequest = new UpdateRequest(index, type, _id)
                    .doc(json);
            client.update(updateRequest).get();
        } catch (Exception e) {
            //throw new MessageException(&quot;update data failed.&quot;, e);
        }
    }

    /**
     * 功能描述：删除数据
     * @param index 索引名
     * @param type 类型
     * @param _id 数据id
     */
    public void deleteData(String index, String type, String _id) {
        client.prepareDelete(index, type, _id)
                .get();
    }

    /**
     * 功能描述：批量插入数据
     * @param index 索引名
     * @param type 类型
     * @param data (_id 主键, json 数据)
     */
    public void bulkInsertData(String index, String type, Map&lt;String, String&gt; data) {
        BulkRequestBuilder bulkRequest = client.prepareBulk();
        data.forEach((param1, param2) -&gt; {
            bulkRequest.add(client.prepareIndex(index, type, param1)
                    .setSource(param2)
            );
        });
        bulkRequest.get();
    }

    /**
     * 功能描述：批量插入数据
     * @param index 索引名
     * @param type 类型
     * @param jsonList 批量数据
     */
    public void bulkInsertData(String index, String type, List&lt;String&gt; jsonList) {
        BulkRequestBuilder bulkRequest = client.prepareBulk();
        jsonList.forEach(item -&gt; {
            bulkRequest.add(client.prepareIndex(index, type)
                    .setSource(item)
            );
        });
        bulkRequest.get();
    }

    /**
     * 功能描述：查询
     * @param index 索引名
     * @param type 类型
     * @param constructor 查询构造
     */
    public List&lt;Map&lt;String, Object&gt;&gt; search(String index, String type, ESQueryBuilderConstructor constructor) {

        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();
        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);
        //排序
        if (StringUtils.isNotEmpty(constructor.getAsc()))
            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);
        if (StringUtils.isNotEmpty(constructor.getDesc()))
            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);
        //设置查询体
        searchRequestBuilder.setQuery(constructor.listBuilders());
        //返回条目数
        int size = constructor.getSize();
        if (size &lt; 0) {
            size = 0;
        }
        if (size &gt; MAX) {
            size = MAX;
        }
        //返回条目数
        searchRequestBuilder.setSize(size);
        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());
        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
        SearchHits hits = searchResponse.getHits();
        SearchHit[] searchHists = hits.getHits();
        for (SearchHit sh : searchHists) {
            list.add(sh.getSourceAsMap());
        }
        return list;
    }


    /**
     * 功能描述：查询
     * @param index 索引名
     * @param type 类型
     * @param constructor 查询构造
     */
    public Map&lt;String,Object&gt; searchCountAndMessage(String index, String type, ESQueryBuilderConstructor constructor) {
        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();
        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();
        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);
        //排序
        if (StringUtils.isNotEmpty(constructor.getAsc()))
            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);
        if (StringUtils.isNotEmpty(constructor.getDesc()))
            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);
        //设置查询体
        searchRequestBuilder.setQuery(constructor.listBuilders());
        //返回条目数
        int size = constructor.getSize();
        if (size &lt; 0) {
            size = 0;
        }
        if (size &gt; MAX) {
            size = MAX;
        }

        //返回条目数
        searchRequestBuilder.setSize(size);
        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());
        SearchResponse searchResponse = searchRequestBuilder.execute().actionGet();
        long totalHits = searchResponse.getHits().getTotalHits();

        SearchHits hits = searchResponse.getHits();
        SearchHit[] searchHists = hits.getHits();
        for (SearchHit sh : searchHists) {
            list.add(sh.getSourceAsMap());
        }
        map.put(&quot;total&quot;,(long)searchHists.length);
        map.put(&quot;count&quot;,totalHits);
        map.put(&quot;data&quot;,list);
        return map;
    }

    /**
     * 功能描述：查询
     * @param index 索引名
     * @param type 类型
     * @param constructor 查询构造
     */
    public Map&lt;String,Object&gt; searchCountAndMessageNew(String index, String type, ESQueryBuilderConstructorNew constructor) {
        Map&lt;String,Object&gt; map = new HashMap&lt;String,Object&gt;();
        List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;&gt;();
        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);

        //排序
        List&lt;SortBuilder&gt; sortBuilderList = constructor.getSortBuilderList();
        if(sortBuilderList!=null &amp;&amp; sortBuilderList.size()&gt;0){
            sortBuilderList.forEach(sortBuilder-&gt;{
                searchRequestBuilder.addSort(sortBuilder);
            });
        }

        //设置查询体
        searchRequestBuilder.setQuery(constructor.listBuilders());

        //返回条目数
        int size = constructor.getSize();
        if (size &lt; 0) {
            size = 0;
        }
        if (size &gt; MAX) {
            size = MAX;
        }
        //返回条目数
        searchRequestBuilder.setSize(size);
        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());

        //设置高亮
        HighlightBuilder highlightBuilder = new HighlightBuilder();
        List&lt;String&gt; highLighterFields = constructor.getHighLighterFields();
        if(highLighterFields.size()&gt;0){
            highLighterFields.forEach(field -&gt; {
                highlightBuilder.field(field);
            });

        }

        highlightBuilder.preTags(&quot;&lt;font color=\&quot;red\&quot;&gt;&quot;);
        highlightBuilder.postTags(&quot;&lt;/font&gt;&quot;);
        SearchResponse searchResponse = searchRequestBuilder.highlighter(highlightBuilder).execute().actionGet();
        long totalHits = searchResponse.getHits().getTotalHits();

        SearchHits hits = searchResponse.getHits();
        SearchHit[] searchHists = hits.getHits();
        for (SearchHit hit : searchHists) {

            Map&lt;String, Object&gt; sourceAsMap = hit.getSourceAsMap();
            Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields();

            //获取高亮结果
            Set&lt;String&gt; set = highlightFields.keySet();

            for (String str : set) {
                Text[] fragments = highlightFields.get(str).getFragments();
                String st1r=&quot;&quot;;
                for(Text text:fragments){
                    st1r = st1r + text.toString();
                }
                sourceAsMap.put(str,st1r);
                System.out.println(&quot;str(==============&quot; + st1r);
            }

            list.add(sourceAsMap);
        }
        map.put(&quot;total&quot;,(long)searchHists.length);
        map.put(&quot;count&quot;,totalHits);
        map.put(&quot;data&quot;,list);
        return map;
    }

    /**
     * 功能描述：统计查询
     * @param index 索引名
     * @param type 类型
     * @param constructor 查询构造
     * @param groupBy 统计字段
     */
    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, String groupBy) {
        Map&lt;Object, Object&gt; map = new HashedMap();
        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);
        //排序
        if (StringUtils.isNotEmpty(constructor.getAsc()))
            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);
        if (StringUtils.isNotEmpty(constructor.getDesc()))
            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);
        //设置查询体
        if (null != constructor) {
            searchRequestBuilder.setQuery(constructor.listBuilders());
        } else {
            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
        }
        int size = constructor.getSize();
        if (size &lt; 0) {
            size = 0;
        }
        if (size &gt; MAX) {
            size = MAX;
        }
        //返回条目数
        searchRequestBuilder.setSize(size);

        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());
        SearchResponse sr = searchRequestBuilder.addAggregation(
                AggregationBuilders.terms(&quot;agg&quot;).field(groupBy)
        ).get();

        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);

        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();

        while (iter.hasNext()) {
            Terms.Bucket gradeBucket = iter.next();
            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());
        }

        return map;
    }

    /**
     * 功能描述：统计查询
     * @param index 索引名
     * @param type 类型
     * @param constructor 查询构造
     * @param agg 自定义计算
     */
    public Map&lt;Object, Object&gt; statSearch(String index, String type, ESQueryBuilderConstructor constructor, AggregationBuilder agg) {
        if (agg == null) {
            return null;
        }
        Map&lt;Object, Object&gt; map = new HashedMap();
        SearchRequestBuilder searchRequestBuilder = client.prepareSearch(index).setTypes(type);
        //排序
        if (StringUtils.isNotEmpty(constructor.getAsc()))
            searchRequestBuilder.addSort(constructor.getAsc(), SortOrder.ASC);
        if (StringUtils.isNotEmpty(constructor.getDesc()))
            searchRequestBuilder.addSort(constructor.getDesc(), SortOrder.DESC);
        //设置查询体
        if (null != constructor) {
            searchRequestBuilder.setQuery(constructor.listBuilders());
        } else {
            searchRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
        }
        int size = constructor.getSize();
        if (size &lt; 0) {
            size = 0;
        }
        if (size &gt; MAX) {
            size = MAX;
        }
        //返回条目数
        searchRequestBuilder.setSize(size);

        searchRequestBuilder.setFrom(constructor.getFrom() &lt; 0 ? 0 : constructor.getFrom());
        SearchResponse sr = searchRequestBuilder.addAggregation(
                agg
        ).get();

        Terms stateAgg = sr.getAggregations().get(&quot;agg&quot;);
        Iterator&lt;? extends Terms.Bucket&gt; iter = stateAgg.getBuckets().iterator();

        while (iter.hasNext()) {
            Terms.Bucket gradeBucket = iter.next();
            map.put(gradeBucket.getKey(), gradeBucket.getDocCount());
        }
        return map;
    }

    /**
     * 功能描述：关闭链接
     */
    public void close() {
        client.close();
    }

    public static void test() {
        try{
            ElasticSearchService service = new ElasticSearchService();
            ESQueryBuilderConstructorNew constructor = new ESQueryBuilderConstructorNew();
            constructor.must(new ESQueryBuilders().bool(QueryBuilders.boolQuery()));
            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));
            constructor.must(new ESQueryBuilders().match(&quot;secondlanguage&quot;, &quot;4&quot;));
            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));
            constructor.should(new ESQueryBuilders().match(&quot;source&quot;, &quot;5&quot;));
            service.searchCountAndMessageNew(&quot;&quot;, &quot;&quot;, constructor);
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    public static void main(String[] args) {
        try {
            ElasticSearchService service = new ElasticSearchService();
            ESQueryBuilderConstructor constructor = new ESQueryBuilderConstructor();

         /*   constructor.must(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50));

            constructor.should(new ESQueryBuilders().term(&quot;gender&quot;, &quot;f&quot;).range(&quot;age&quot;, 20, 50).fuzzy(&quot;age&quot;, 20));
            constructor.mustNot(new ESQueryBuilders().term(&quot;gender&quot;, &quot;m&quot;));
            constructor.setSize(15);  //查询返回条数，最大 10000
            constructor.setFrom(11);  //分页查询条目起始位置， 默认0
            constructor.setAsc(&quot;age&quot;); //排序

            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;bank&quot;, &quot;account&quot;, constructor);
            Map&lt;Object, Object&gt; map = service.statSearch(&quot;bank&quot;, &quot;account&quot;, constructor, &quot;state&quot;);*/

            constructor.must(new ESQueryBuilders().match(&quot;id&quot;, &quot;WE16000190TR&quot;));
            List&lt;Map&lt;String, Object&gt;&gt; list = service.search(&quot;test01&quot;, &quot;test01&quot;, constructor);
             for(Map&lt;String, Object&gt; map : list){
                 System.out.println(map);
             }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}</code></pre><p><strong>ESCriterion.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import org.elasticsearch.index.query.QueryBuilder;

import java.util.List;

/**
 * 条件接口
 */
public interface ESCriterion {

    public enum Operator {
        PREFIX,             /**根据字段前缀查询**/
        MATCH,              /**匹配查询**/
        MATCH_PHRASE,       /**精确匹配**/
        MULTI_MATCH,        /**多字段匹配**/

        TERM,               /**term查询**/
        TERMS,              /**term查询**/

        RANGE,              /**范围查询**/
        GTE,                 /**大于等于查询**/
        LTE,

        FUZZY,              /**根据字段前缀查询**/
        QUERY_STRING,       /**根据字段前缀查询**/
        MISSING ,           /**根据字段前缀查询**/

        BOOL
    }

    public enum MatchMode {
        START, END, ANYWHERE
    }

    public enum Projection {
        MAX, MIN, AVG, LENGTH, SUM, COUNT
    }

    public List&lt;QueryBuilder&gt; listBuilders();
}</code></pre><p><strong>ESQueryBuilderConstructor.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import org.apache.commons.collections.CollectionUtils;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;

import java.util.ArrayList;
import java.util.List;

/**
 * 查询条件容器
 */
public class ESQueryBuilderConstructor {

    private int size = Integer.MAX_VALUE;

    private int from = 0;

    private String asc;

    private String desc;

    //查询条件容器
    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();
    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();
    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();

    //构造builder
    public QueryBuilder listBuilders() {
        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        QueryBuilder queryBuilder = null;

        if (count &gt;= 1) {
            //must容器
            if (!CollectionUtils.isEmpty(mustCriterions)) {
                for (ESCriterion criterion : mustCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.must(builder);
                    }
                }
            }

            //should容器
            if (!CollectionUtils.isEmpty(shouldCriterions)) {
                for (ESCriterion criterion : shouldCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.should(builder);
                    }
                }
            }

            //must not 容器
            if (!CollectionUtils.isEmpty(mustNotCriterions)) {
                for (ESCriterion criterion : mustNotCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.mustNot(builder);
                    }
                }
            }
            return queryBuilder;
        } else {
            return null;
        }
    }

    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructor must(ESCriterion criterion){
        if(criterion!=null){
            mustCriterions.add(criterion);
        }
        return this;
    }

    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructor should(ESCriterion criterion){
        if(criterion!=null){
            shouldCriterions.add(criterion);
        }
        return this;
    }

    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructor mustNot(ESCriterion criterion){
        if(criterion!=null){
            mustNotCriterions.add(criterion);
        }
        return this;
    }


    public int getSize() {
        return size;
    }

    public void setSize(int size) {
        this.size = size;
    }

    public String getAsc() {
        return asc;
    }

    public void setAsc(String asc) {
        this.asc = asc;
    }

    public String getDesc() {
        return desc;
    }

    public void setDesc(String desc) {
        this.desc = desc;
    }

    public int getFrom() {
        return from;
    }

    public void setFrom(int from) {
        this.from = from;
    }
}</code></pre><p><strong>ESQueryBuilderConstructorNew.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import org.apache.commons.collections.CollectionUtils;
import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.search.sort.SortBuilder;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 查询条件容器
 */
public class ESQueryBuilderConstructorNew {

    private List&lt;String&gt; highLighterFields = new ArrayList&lt;String&gt;();

    private int size = Integer.MAX_VALUE;

    private int from = 0;

    private List&lt;SortBuilder&gt; sortBuilderList;

    public List&lt;SortBuilder&gt; getSortBuilderList() {
        return sortBuilderList;
    }

    public void setSortBuilderList(List&lt;SortBuilder&gt; sortBuilderList) {
        this.sortBuilderList = sortBuilderList;
    }

    private Map&lt;String,List&lt;String&gt;&gt; sortMap;

    //查询条件容器
    private List&lt;ESCriterion&gt; mustCriterions = new ArrayList&lt;ESCriterion&gt;();
    private List&lt;ESCriterion&gt; shouldCriterions = new ArrayList&lt;ESCriterion&gt;();
    private List&lt;ESCriterion&gt; mustNotCriterions = new ArrayList&lt;ESCriterion&gt;();

    //构造builder
    public QueryBuilder listBuilders() {
        int count = mustCriterions.size() + shouldCriterions.size() + mustNotCriterions.size();

        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        QueryBuilder queryBuilder = null;

        if (count &gt;= 1) {
            //must容器
            if (!CollectionUtils.isEmpty(mustCriterions)) {
                for (ESCriterion criterion : mustCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.must(builder);
                    }
                }
            }

            //should容器
            if (!CollectionUtils.isEmpty(shouldCriterions)) {
                for (ESCriterion criterion : shouldCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.should(builder);
                    }
                }
            }

            //must not 容器
            if (!CollectionUtils.isEmpty(mustNotCriterions)) {
                for (ESCriterion criterion : mustNotCriterions) {
                    for (QueryBuilder builder : criterion.listBuilders()) {
                        queryBuilder = boolQueryBuilder.mustNot(builder);
                    }
                }
            }
            return queryBuilder;
        } else {
            return null;
        }
    }

    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructorNew must(ESCriterion criterion){
        if(criterion!=null){
            mustCriterions.add(criterion);
        }
        return this;
    }

    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructorNew should(ESCriterion criterion){
        if(criterion!=null){
            shouldCriterions.add(criterion);
        }
        return this;
    }
    /**
     * 增加简单条件表达式
     */
    public ESQueryBuilderConstructorNew mustNot(ESCriterion criterion){
        if(criterion!=null){
            mustNotCriterions.add(criterion);
        }
        return this;
    }

    public List&lt;String&gt; getHighLighterFields() {
        return highLighterFields;
    }

    public void setHighLighterFields(List&lt;String&gt; highLighterFields) {
        this.highLighterFields = highLighterFields;
    }

    public int getSize() {
        return size;
    }

    public void setSize(int size) {
        this.size = size;
    }

    public Map&lt;String, List&lt;String&gt;&gt; getSortMap() {
        return sortMap;
    }

    public void setSortMap(Map&lt;String, List&lt;String&gt;&gt; sortMap) {
        this.sortMap = sortMap;
    }

    public int getFrom() {
        return from;
    }

    public void setFrom(int from) {
        this.from = from;
    }
}</code></pre><p><strong>ESQueryBuilders.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.NestedQueryBuilder;
import org.elasticsearch.index.query.QueryBuilder;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

/**
 * 条件构造器
 */
public class ESQueryBuilders implements ESCriterion{

    private List&lt;QueryBuilder&gt; list = new ArrayList&lt;QueryBuilder&gt;();

    /**
     * 功能描述：match 查询
     * @param field 字段名
     * @param value 值
     */
    public ESQueryBuilders match(String field, Object value) {
        list.add(new ESSimpleExpression (field, value, Operator.MATCH).toBuilder());
        return this;
    }

    /**
     * 功能描述：match 查询
     * @param field 字段名
     * @param value 值
     */
    public ESQueryBuilders match_phrase(String field, Object value) {
        list.add(new ESSimpleExpression (field, value, Operator.MATCH_PHRASE).toBuilder());
        return this;
    }

    /**
     * 功能描述：match 查询
     * @param fieldNames 字段名
     * @param value 值
     */
    public ESQueryBuilders multi_match(Object value , String... fieldNames ) {
        String[] fields = fieldNames;
        list.add(new ESSimpleExpression (value, Operator.MULTI_MATCH,fields).toBuilder());
        return this;
    }

    /**
     * 功能描述：Term 查询
     * @param field 字段名
     * @param value 值
     */
    public ESQueryBuilders term(String field, Object value) {
        list.add(new ESSimpleExpression (field, value, Operator.TERM).toBuilder());
        return this;
    }

    /**
     * 功能描述：Terms 查询
     * @param field 字段名
     * @param values 集合值
     */
    public ESQueryBuilders terms(String field, Collection&lt;Object&gt; values) {
        list.add(new ESSimpleExpression (field, values).toBuilder());
        return this;
    }

    /**
     * 功能描述：fuzzy 查询
     * @param field 字段名
     * @param value 值
     */
    public ESQueryBuilders fuzzy(String field, Object value) {
        list.add(new ESSimpleExpression (field, value, Operator.FUZZY).toBuilder());
        return this;
    }

    /**
     * 功能描述：Range 查询
     * @param from 起始值
     * @param to 末尾值
     */
    public ESQueryBuilders range(String field, Object from, Object to) {
        list.add(new ESSimpleExpression (field, from, to).toBuilder());
        return this;
    }

    /**
     * 功能描述：GTE 大于等于查询
     * @param
     */
    public ESQueryBuilders gte(String field, Object num) {
        list.add(new ESSimpleExpression (field, num,Operator.GTE).toBuilder());
        return this;
    }

    /**
     * 功能描述：LTE 小于等于查询
     * @param
     */
    public ESQueryBuilders lte(String field, Object num) {
        list.add(new ESSimpleExpression (field, num,Operator.LTE).toBuilder());
        return this;
    }

    /**
     * 功能描述：prefix 查询
     * @param field 字段名
     * @param value 值
     */
    public ESQueryBuilders prefix(String field, Object value) {
        list.add(new ESSimpleExpression (field, value, Operator.PREFIX).toBuilder());
        return this;
    }

    /**
     * 功能描述：Range 查询
     * @param queryString 查询语句
     */
    public ESQueryBuilders queryString(String queryString) {
        list.add(new ESSimpleExpression (queryString, Operator.QUERY_STRING).toBuilder());
        return this;
    }

    /**
     * 功能描述：Range 查询
     * @param
     */
    public ESQueryBuilders bool(BoolQueryBuilder boolQueryBuilder) {
        list.add(boolQueryBuilder);
        return this;
    }

    public ESQueryBuilders nested(NestedQueryBuilder nestedQueryBuilder) {
        list.add(nestedQueryBuilder);
        return this;
    }

    public List&lt;QueryBuilder&gt; listBuilders() {
        return list;
    }
}</code></pre><p><strong>ESSimpleExpression.java</strong></p>
<pre><code>package com.hsiehchou.es.V2;

import org.elasticsearch.index.query.BoolQueryBuilder;
import org.elasticsearch.index.query.QueryBuilder;
import org.elasticsearch.index.query.QueryBuilders;

import java.util.Collection;
import com.hsiehchou.es.V2.ESCriterion.Operator;

import static org.elasticsearch.index.search.MatchQuery.Type.PHRASE;

/**
 * 条件表达式
 */
public class ESSimpleExpression {

    private String[] fieldNames;         //属性名
    private String fieldName;         //属性名
    private Object value;             //对应值
    private Collection&lt;Object&gt; values;//对应值
    private Operator operator;        //计算符
    private Object from;
    private Object to;

    protected  ESSimpleExpression() {
    }

    protected  ESSimpleExpression(Object value, Operator operator,String... fieldNames) {
        this.fieldNames = fieldNames;
        this.value = value;
        this.operator = operator;
    }


    protected  ESSimpleExpression(String fieldName, Object value, Operator operator) {
        this.fieldName = fieldName;
        this.value = value;
        this.operator = operator;
    }

    protected  ESSimpleExpression(String value, Operator operator) {
        this.value = value;
        this.operator = operator;
    }

    protected ESSimpleExpression(String fieldName, Collection&lt;Object&gt; values) {
        this.fieldName = fieldName;
        this.values = values;
        this.operator = Operator.TERMS;
    }

    protected ESSimpleExpression(String fieldName, Object from, Object to) {
        this.fieldName = fieldName;
        this.from = from;
        this.to = to;
        this.operator = Operator.RANGE;
    }

    public BoolQueryBuilder toBoolQueryBuilder(){
        BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();
        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));
        boolQueryBuilder.mustNot(QueryBuilders.matchQuery(&quot;&quot;,&quot;&quot;));

        return null;
    }

    public QueryBuilder toBuilder() {
        QueryBuilder qb = null;
        switch (operator) {
            case MATCH:
                qb = QueryBuilders.matchQuery(fieldName, value);
                break;
            case MATCH_PHRASE:
                qb = QueryBuilders.matchPhraseQuery(fieldName, value);
                break;
            case MULTI_MATCH:
                qb = QueryBuilders.multiMatchQuery(value,fieldNames).type(PHRASE);
                break;
            case TERM:
                qb = QueryBuilders.termQuery(fieldName, value);
                break;
            case TERMS:
                qb = QueryBuilders.termsQuery(fieldName, values);
                break;
            case RANGE:
                qb = QueryBuilders.rangeQuery(fieldName).from(from).to(to).includeLower(true).includeUpper(true);
                break;
            case GTE:
                qb = QueryBuilders.rangeQuery(fieldName).gte(value);
                break;
            case LTE:
                qb = QueryBuilders.rangeQuery(fieldName).lte(value);
                break;
            case FUZZY:
                qb = QueryBuilders.fuzzyQuery(fieldName, value);
                break;
            case PREFIX:
                qb = QueryBuilders.prefixQuery(fieldName, value.toString());
                break;
            case QUERY_STRING:
                qb = QueryBuilders.queryStringQuery(value.toString());
                default:
        }
        return qb;
    }
}</code></pre><h3 id="九、预警"><a href="#九、预警" class="headerlink" title="九、预警"></a>九、预警</h3><p>通过后台或者界面设置规则，保存到mysql，然后同步到redis。</p>
<p>数据量大的话，用mysql是非常慢的，使用内存数据库redis进行规则缓存，使用时直接比对预警。</p>
<p><img src="/medias/%E9%A2%84%E8%AD%A6%E6%B5%81%E7%A8%8B.PNG" alt="预警流程"></p>
<p><img src="/medias/%E9%A2%84%E8%AD%A6%E8%BF%87%E7%A8%8B.PNG" alt="预警过程"></p>
<p>MySQL 需要2张表<br>一张是规则表   用来存储规则<br>一张是消息表   存储告警消息</p>
<h4 id="1、创建规则表（由界面控制规则发布）"><a href="#1、创建规则表（由界面控制规则发布）" class="headerlink" title="1、创建规则表（由界面控制规则发布）"></a>1、创建规则表（由界面控制规则发布）</h4><p>规则首先存放在mysql中，会使用一个定时任务将mysql中的规则同步到redis<br>       直接在test库中创建<br>       创建脚本<br><strong>xz_rule.sql</strong></p>
<pre><code>SET FOREIGN_KEY_CHECKS=0;

DROP TABLE IF EXISTS `xz_rule`;
CREATE TABLE `xz_rule` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `warn_fieldname` varchar(20) DEFAULT NULL,
  `warn_fieldvalue` varchar(255) DEFAULT NULL,
  `publisher` varchar(255) DEFAULT NULL,
  `send_type` varchar(255) CHARACTER SET utf8 DEFAULT NULL,
  `send_mobile` varchar(255) DEFAULT NULL,
  `send_mail` varchar(255) DEFAULT NULL,
  `send_dingding` varchar(255) DEFAULT NULL,
  `create_time` date DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=2 DEFAULT CHARSET=latin1;

INSERT INTO `xz_rule` VALUES (&#39;1&#39;, &#39;phone&#39;, &#39;18609765432&#39;, &#39;?????1&#39;, &#39;2&#39;, &#39;13724536789&#39;, &#39;1782324@qq.com&#39;, &#39;32143243&#39;, &#39;2019-06-28&#39;);</code></pre><h4 id="2、创建消息表"><a href="#2、创建消息表" class="headerlink" title="2、创建消息表"></a>2、创建消息表</h4><ol>
<li>用于存放预警的消息，供界面定时刷新预警消息 或者是滚屏预警</li>
<li>预警消息统计</li>
</ol>
<p><strong>warn_message.sql</strong></p>
<pre><code>SET FOREIGN_KEY_CHECKS=0;

DROP TABLE IF EXISTS `warn_message`;
CREATE TABLE `warn_message` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `alarmRuleid` varchar(255) DEFAULT NULL,
  `alarmType` varchar(255) DEFAULT NULL,
  `sendType` varchar(255) DEFAULT NULL,
  `sendMobile` varchar(255) DEFAULT NULL,
  `sendEmail` varchar(255) DEFAULT NULL,
  `sendStatus` varchar(255) DEFAULT NULL,
  `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,
  `hitTime` datetime DEFAULT NULL,
  `checkinTime` datetime DEFAULT NULL,
  `isRead` varchar(255) DEFAULT NULL,
  `readAccounts` varchar(255) DEFAULT NULL,
  `alarmaccounts` varchar(255) DEFAULT NULL,
  `accountid` varchar(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=31 DEFAULT CHARSET=latin1;</code></pre><h4 id="3、创建数据库连接工具类"><a href="#3、创建数据库连接工具类" class="headerlink" title="3、创建数据库连接工具类"></a>3、创建数据库连接工具类</h4><p><strong>新建com.hsiehchou.common.netb.db包</strong><br><strong>创建DBCommon类</strong></p>
<p><strong>DBCommon.java</strong></p>
<pre><code>package com.hsiehchou.common.netb.db;

import com.hsiehchou.common.config.ConfigUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.*;
import java.util.Properties;

public class DBCommon {

    private static Logger LOG = LoggerFactory.getLogger(DBCommon.class);
    private static String MYSQL_PATH = &quot;common/mysql.properties&quot;;
    private static Properties properties = ConfigUtil.getInstance().getProperties(MYSQL_PATH);

    private static Connection conn ;
    private DBCommon(){}

    public static void main(String[] args) {
        System.out.println(properties);
        Connection xz_bigdata = DBCommon.getConn(&quot;test&quot;);
        System.out.println(xz_bigdata);
    }

    //TODO  配置文件
    private static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;;
    private static final String USER_NAME = properties.getProperty(&quot;user&quot;);
    private static final String PASSWORD = properties.getProperty(&quot;password&quot;);
    private static final String IP = properties.getProperty(&quot;db_ip&quot;);
    private static final String PORT = properties.getProperty(&quot;db_port&quot;);
    private static final String DB_CONFIG = &quot;?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;autoReconnect=true&amp;failOverReadOnly=false&quot;;

    static {
        try {
            Class.forName(JDBC_DRIVER);
        } catch (ClassNotFoundException e) {
            LOG.error(null, e);
        }
    }

    /**
     * 获取数据库连接
     * @param dbName
     * @return
     */
    public static Connection getConn(String dbName) {
        Connection conn = null;
        String  connstring = &quot;jdbc:mysql://&quot;+IP+&quot;:&quot;+PORT+&quot;/&quot;+dbName+DB_CONFIG;
        try {
            conn = DriverManager.getConnection(connstring, USER_NAME, PASSWORD);
        } catch (SQLException e) {
            e.printStackTrace();
            LOG.error(null, e);
        }
        return conn;
    }

    /**
     * @param url eg:&quot;jdbc:oracle:thin:@172.16.1.111:1521:d406&quot;
     * @param driver eg:&quot;oracle.jdbc.driver.OracleDriver&quot;
     * @param user eg:&quot;ucase&quot;
     * @param password eg:&quot;ucase123&quot;
     * @return
     * @throws ClassNotFoundException
     * @throws SQLException
     */
    public static Connection getConn(String url, String driver, String user,
                                     String password) throws ClassNotFoundException, SQLException{
        Class.forName(driver);
        conn = DriverManager.getConnection(url, user, password);
        return  conn;
    }

    public static void close(Connection conn){
        try {
            if( conn != null ){
                conn.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Statement statement){
        try {
            if( statement != null ){
                statement.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Connection conn,PreparedStatement statement){
        try {
            if( conn != null ){
                conn.close();
            }
            if( statement != null ){
                statement.close();
            }
        } catch (SQLException e) {
            LOG.error(null,e);
        }
    }

    public static void close(Connection conn,Statement statement,ResultSet resultSet) throws SQLException{

        if( resultSet != null ){
            resultSet.close();
        }
        if( statement != null ){
            statement.close();
        }
        if( conn != null ){
            conn.close();
        }
    }
}</code></pre><p><strong>引入maven依赖</strong></p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;
    &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;
    &lt;version&gt;${commons-dbutils.version}&lt;/version&gt;
&lt;/dependency&gt;</code></pre><h4 id="4、创建实体类和dao"><a href="#4、创建实体类和dao" class="headerlink" title="4、创建实体类和dao"></a>4、创建实体类和dao</h4><p><strong>新建com.hsiehchou.spark.warn.domain包</strong><br><strong>新建 XZ_RuleDomain，WarningMessage</strong></p>
<p><strong>XZ_RuleDomain.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.domain;

import java.sql.Date;

public class XZ_RuleDomain {

    private int id;
    private String warn_fieldname;   //预警字段
    private String warn_fieldvalue; //预警内容
    private String publisher;       //发布者
    private String send_type;       //消息接收方式
    private String send_mobile;     //接收手机号
    private String send_mail;       //接收邮箱
    private String send_dingding;   //接收钉钉
    private Date create_time;       //创建时间


    public int getId() {
        return id;
    }

    public void setId(int id) {
        this.id = id;
    }

    public String getWarn_fieldname() {
        return warn_fieldname;
    }

    public void setWarn_fieldname(String warn_fieldname) {
        this.warn_fieldname = warn_fieldname;
    }

    public String getWarn_fieldvalue() {
        return warn_fieldvalue;
    }

    public void setWarn_fieldvalue(String warn_fieldvalue) {
        this.warn_fieldvalue = warn_fieldvalue;
    }

    public String getPublisher() {
        return publisher;
    }

    public void setPublisher(String publisher) {
        this.publisher = publisher;
    }

    public String getSend_type() {
        return send_type;
    }

    public void setSend_type(String send_type) {
        this.send_type = send_type;
    }

    public String getSend_mobile() {
        return send_mobile;
    }

    public void setSend_mobile(String send_mobile) {
        this.send_mobile = send_mobile;
    }

    public String getSend_mail() {
        return send_mail;
    }

    public void setSend_mail(String send_mail) {
        this.send_mail = send_mail;
    }

    public String getSend_dingding() {
        return send_dingding;
    }

    public void setSend_dingding(String send_dingding) {
        this.send_dingding = send_dingding;
    }

    public Date getCreate_time() {
        return create_time;
    }

    public void setCreate_time(Date create_time) {
        this.create_time = create_time;
    }
}</code></pre><p><strong>WarningMessage.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.domain;

import java.sql.Date;

public class WarningMessage {
    private String id;            //主键id
    private String alarmRuleid;   //规则id
    private String alarmType;     //告警类型
    private String sendType;      //发送方式
    private String sendMobile;    //发送至手机
    private String sendEmail;     //发送至邮箱
    private String sendStatus;    //发送状态
    private String senfInfo;      //发送内容
    private Date hitTime;         //命中时间
    private Date checkinTime;     //入库时间
    private String isRead;        //是否已读
    private String readAccounts;  //已读用户
    private String alarmaccounts;
    private String accountid;

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public String getAlarmRuleid() {
        return alarmRuleid;
    }

    public void setAlarmRuleid(String alarmRuleid) {
        this.alarmRuleid = alarmRuleid;
    }

    public String getAlarmType() {
        return alarmType;
    }

    public void setAlarmType(String alarmType) {
        this.alarmType = alarmType;
    }

    public String getSendType() {
        return sendType;
    }

    public void setSendType(String sendType) {
        this.sendType = sendType;
    }

    public String getSendMobile() {
        return sendMobile;
    }

    public void setSendMobile(String sendMobile) {
        this.sendMobile = sendMobile;
    }

    public String getSendEmail() {
        return sendEmail;
    }

    public void setSendEmail(String sendEmail) {
        this.sendEmail = sendEmail;
    }

    public String getSendStatus() {
        return sendStatus;
    }

    public void setSendStatus(String sendStatus) {
        this.sendStatus = sendStatus;
    }

    public String getSenfInfo() {
        return senfInfo;
    }

    public void setSenfInfo(String senfInfo) {
        this.senfInfo = senfInfo;
    }

    public Date getHitTime() {
        return hitTime;
    }

    public void setHitTime(Date hitTime) {
        this.hitTime = hitTime;
    }

    public Date getCheckinTime() {
        return checkinTime;
    }

    public void setCheckinTime(Date checkinTime) {
        this.checkinTime = checkinTime;
    }

    public String getIsRead() {
        return isRead;
    }

    public void setIsRead(String isRead) {
        this.isRead = isRead;
    }

    public String getReadAccounts() {
        return readAccounts;
    }

    public void setReadAccounts(String readAccounts) {
        this.readAccounts = readAccounts;
    }

    public String getAlarmaccounts() {
        return alarmaccounts;
    }

    public void setAlarmaccounts(String alarmaccounts) {
        this.alarmaccounts = alarmaccounts;
    }

    public String getAccountid() {
        return accountid;
    }

    public void setAccountid(String accountid) {
        this.accountid = accountid;
    }

    @Override
    public String toString() {
        return &quot;WarningMessage{&quot; +
                &quot;id=&#39;&quot; + id + &#39;\&#39;&#39; +
                &quot;, alarmRuleid=&#39;&quot; + alarmRuleid + &#39;\&#39;&#39; +
                &quot;, alarmType=&#39;&quot; + alarmType + &#39;\&#39;&#39; +
                &quot;, sendType=&#39;&quot; + sendType + &#39;\&#39;&#39; +
                &quot;, sendMobile=&#39;&quot; + sendMobile + &#39;\&#39;&#39; +
                &quot;, sendEmail=&#39;&quot; + sendEmail + &#39;\&#39;&#39; +
                &quot;, sendStatus=&#39;&quot; + sendStatus + &#39;\&#39;&#39; +
                &quot;, senfInfo=&#39;&quot; + senfInfo + &#39;\&#39;&#39; +
                &quot;, hitTime=&quot; + hitTime +
                &quot;, checkinTime=&quot; + checkinTime +
                &quot;, isRead=&#39;&quot; + isRead + &#39;\&#39;&#39; +
                &quot;, readAccounts=&#39;&quot; + readAccounts + &#39;\&#39;&#39; +
                &quot;, alarmaccounts=&#39;&quot; + alarmaccounts + &#39;\&#39;&#39; +
                &quot;, accountid=&#39;&quot; + accountid + &#39;\&#39;&#39; +
                &#39;}&#39;;
    }
}</code></pre><p><strong>新建com.hsiehchou.spark.warn.dao包</strong><br><strong>新建 XZ_RuleDao，WarningMessageDao</strong></p>
<p><strong>XZ_RuleDao.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.dao;

import com.hsiehchou.common.netb.db.DBCommon;
import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;
import org.apache.commons.dbutils.QueryRunner;
import org.apache.commons.dbutils.handlers.BeanListHandler;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.Connection;
import java.sql.SQLException;
import java.util.List;

public class XZ_RuleDao {

    private static final Logger LOG = LoggerFactory.getLogger(XZ_RuleDao.class);

    /**
     *  获取所有的规则
     * @return
     */
    public static List&lt;XZ_RuleDomain&gt; getRuleList(){
        List&lt;XZ_RuleDomain&gt; listRules = null;

        //获取连接
        Connection conn = DBCommon.getConn(&quot;test&quot;);

        //执行器
        QueryRunner query = new QueryRunner();
        String sql = &quot;select * from xz_rule&quot;;
        try {
            listRules = query.query(conn,sql,new BeanListHandler&lt;&gt;(XZ_RuleDomain.class));
        } catch (SQLException e) {
            LOG.error(null,e);
        }finally {
            DBCommon.close(conn);
        }
        return listRules;
    }

    public static void main(String[] args) {
        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();
        System.out.println(ruleList.size());
        ruleList.forEach(x-&gt;{
            System.out.println(x);
        });
    }
}</code></pre><p><strong>WarningMessageDao.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.dao;

import com.hsiehchou.common.netb.db.DBCommon;
import com.hsiehchou.spark.warn.domain.WarningMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.*;

public class WarningMessageDao {

    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageDao.class);

    /**
     * 写入消息到mysql
     * @param warningMessage
     * @return
     */
    public static Integer insertWarningMessageReturnId(WarningMessage warningMessage) {
        Connection conn= DBCommon.getConn(&quot;test&quot;);
        String sql=&quot;insert into warn_message(alarmruleid,sendtype,senfinfo,hittime,sendmobile,alarmtype) &quot; +
                &quot;values(?,?,?,?,?,?)&quot;;

        PreparedStatement stmt=null;
        ResultSet resultSet=null;
        int id=-1;
        try{
            stmt = conn.prepareStatement(sql);
            stmt.setString(1,warningMessage.getAlarmRuleid());
            stmt.setInt(2,Integer.valueOf(warningMessage.getSendType()));
            stmt.setString(3,warningMessage.getSenfInfo());
            stmt.setTimestamp(4,new Timestamp(System.currentTimeMillis()));
            stmt.setString(5,warningMessage.getSendMobile());
            stmt.setInt(6,Integer.valueOf(warningMessage.getAlarmType()));
            stmt.executeUpdate();
        }catch(Exception e) {
            LOG.error(null,e);
        }finally {
            try {
                DBCommon.close(conn,stmt,resultSet);
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
        return id;
    }
}</code></pre><h4 id="5、告警工具类"><a href="#5、告警工具类" class="headerlink" title="5、告警工具类"></a>5、告警工具类</h4><p><strong>新建com.hsiehchou.spark.warn.service包</strong><br><strong>新建 BlackRuleWarning，WarningMessageSendUtil</strong></p>
<p><strong>BlackRuleWarning.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.service;

import com.hsiehchou.spark.warn.dao.WarningMessageDao;
import com.hsiehchou.spark.warn.domain.WarningMessage;
import org.apache.commons.lang3.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import redis.clients.jedis.Jedis;

import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class BlackRuleWarning {
    private static final Logger LOG = LoggerFactory.getLogger(BlackRuleWarning.class);
    //可以通过数据库，配置文件加载

    //为了遍历所有预警字段
    private static List&lt;String&gt; listWarnFields = new ArrayList&lt;&gt;();

    static {
        listWarnFields.add(&quot;phone&quot;);
        listWarnFields.add(&quot;mac&quot;);
    }

    /**
     * 预警流程处理
     * @param map
     * @param jedis15
     */
    public static void blackWarning(Map&lt;String, Object&gt; map, Jedis jedis15) {

        listWarnFields.forEach(warnField -&gt; {
            if (map.containsKey(warnField) &amp;&amp; StringUtils.isNotBlank(map.get(warnField).toString())) {

                //获取预警字段核预警值  相当于手机号
                String warnFieldValue = map.get(warnField).toString();

                //去redis中进行比对
                //数据中  通过   &quot;字段&quot; + &quot;字段值&quot; 去拼接key
                //            phone       :    186XXXXXX
                String key = warnField + &quot;:&quot; + warnFieldValue;

                //redis中的key是   phone:18609765435
                System.out.println(&quot;拼接数据流中的key=======&quot; + key);
                if (jedis15.exists(key)) {
                    //对比命中之后 就可以发送消息提醒
                    System.out.println(&quot;命中REDIS中的&quot; + key + &quot;===========开始预警&quot;);
                    beginWarning(jedis15, key);
                } else {
                    //直接过
                    System.out.println(&quot;未命中&quot; + key + &quot;===========不进行预警&quot;);
                }
            }
        });
    }

    /**
     * 规则已经命中，开始预警
     * @param jedis15
     * @param key
     */
    private static void beginWarning( Jedis jedis15, String key) {

        System.out.println(&quot;============MESSAGE -1- =========&quot;);
        //封装告警  信息及告警消息
        WarningMessage warningMessage = getWarningMessage(jedis15, key);


        System.out.println(&quot;============MESSAGE -4- =========&quot;);
        if (warningMessage != null) {
            //将预警信息写入预警信息表
            WarningMessageDao.insertWarningMessageReturnId(warningMessage);
            //String accountid = warningMessage.getAccountid();
            //String readAccounts = warningMessage.getAlarmaccounts();
            // WarnService.insertRead_status(messageId, accountid);
            if (warningMessage.getSendType().equals(&quot;2&quot;)) {
                //手机短信告警 默认告警方式
                WarningMessageSendUtil.messageWarn(warningMessage);
            }
        }
    }

    /**
     * 封装告警信息及告警消息
     * @param jedis15
     * @param key
     * @return
     */
    private static WarningMessage getWarningMessage(Jedis jedis15, String key) {
        System.out.println(&quot;============MESSAGE -2- =========&quot;);
        //封装消息
        String[] split = key.split(&quot;:&quot;);
        if (split.length == 2) {
            WarningMessage warningMessage = new WarningMessage();
            String time = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).toString();
            String clew_type = split[0];//告警字段
            String rulecontent = split[1];//告警字段值

            //从redis中获取消息信息进行封装
            Map&lt;String, String&gt; valueMap = jedis15.hgetAll(key);

            //规则ID (是哪条规则命中的)
            warningMessage.setAlarmRuleid(valueMap.get(&quot;id&quot;));

            //预警方式
            warningMessage.setSendType(valueMap.get(&quot;send_type&quot;));//告警方式，0：界面 1：邮件 2：短信 3：邮件+短信

            //预警信息接收手机号
            warningMessage.setSendMobile(valueMap.get(&quot;send_mobile&quot;));

            //arningMessage.setSendEmail(valueMap.get(&quot;sendemail&quot;));
            /*arningMessage.setAlarmaccounts(valueMap.get(&quot;alarmaccounts&quot;));*/
            //规则发布人
            warningMessage.setAccountid(valueMap.get(&quot;publisher&quot;));
            warningMessage.setAlarmType(&quot;2&quot;);
            StringBuffer warn_content = new StringBuffer();

            //预警内容 信息   时间  地点  人物
            //预警字段来进行设置  phone
            //我们有手机号


            //数据关联
            // 手机  MAC  身份证， 车牌  人脸。。URL 姓名
            // 全部设在推送消息里面
            warn_content.append(&quot;【网络告警】：手机号为:&quot; + &quot;[&quot; + rulecontent + &quot;]在时间&quot; + time + &quot;出现在&quot; + &quot;&gt;附近,设备号&quot;
            );
            String content = warn_content.toString();
            warningMessage.setSenfInfo(content);
            System.out.println(&quot;============MESSAGE -3- =========&quot;);
            return warningMessage;
        } else {
            return null;
        }
    }
}</code></pre><p><strong>WarningMessageSendUtil.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.service;

import com.hsiehchou.common.regex.Validation;
import com.hsiehchou.spark.warn.domain.WarningMessage;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class WarningMessageSendUtil {
    private static final Logger LOG = LoggerFactory.getLogger(WarningMessageSendUtil.class);

    public static void messageWarn(WarningMessage warningMessage) {

        String[] mobiles = warningMessage.getSendMobile().split(&quot;,&quot;);

        for(String phone:mobiles){
            if(Validation.isMobile(phone)){
                System.out.println(&quot;开始向手机号为&quot; + phone + &quot;发送告警消息====&quot; + warningMessage);
                StringBuffer sb= new StringBuffer();
                String content=warningMessage.getSenfInfo().toString();
                //TODO  调用短信接口发送消息
                //TODO  怎么通过短信发送  这个是需要公司开通接口
                //TODO  DINGDING
                // 专门的接口
             /*   sb.append(ClusterProperties.https_url + &quot;username=&quot; + ClusterProperties.https_username +
                        &quot;&amp;password=&quot; + ClusterProperties.https_password + &quot;&amp;mobile=&quot; + phone +
                        &quot;&amp;apikey=&quot; + ClusterProperties.https_apikey+
                        &quot;&amp;content=&quot; + URLEncoder.encode(content));*/
               // sendMessage(sb.toString());
            }
        }
    }
}</code></pre><h4 id="6、创建redis子项目"><a href="#6、创建redis子项目" class="headerlink" title="6、创建redis子项目"></a>6、创建redis子项目</h4><p><strong>操作redis 使用</strong></p>
<p><strong>新建xz_bigdata_redis子模块</strong></p>
<p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_redis&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_redis&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;jedis.version&gt;2.7.0&lt;/jedis.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;redis.clients&lt;/groupId&gt;
            &lt;artifactId&gt;jedis&lt;/artifactId&gt;
            &lt;version&gt;${jedis.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;</code></pre><p><strong>新建com.hsiehchou.redis.client包</strong><br> <strong>创建redis连接类—JedisSingle</strong></p>
<p><strong>JedisSingle.java</strong></p>
<pre><code>package com.hsiehchou.redis.client;

import com.hsiehchou.common.config.ConfigUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import redis.clients.jedis.Jedis;
import redis.clients.jedis.exceptions.JedisConnectionException;

import java.net.SocketTimeoutException;
import java.util.Map;
import java.util.Properties;

public class JedisSingle {

    private static final Logger LOG = LoggerFactory.getLogger(JedisSingle.class);
    private static Properties redisConf;

    /**
     * 读取redis配置文件
     * redis.hostname = 192.168.247.103
     * redis.port  = 6379
     */
    static {
        redisConf = ConfigUtil.getInstance().getProperties(&quot;redis/redis.properties&quot;);
        System.out.println(redisConf);
    }

    public static Jedis getJedis(int db){
        Jedis jedis = JedisSingle.getJedis();
        if(jedis!=null){
            jedis.select(db);
        }
        return jedis;
    }

    public static void main(String[] args) {
        Jedis jedis = JedisSingle.getJedis(15);
        Map&lt;String, String&gt; Map = jedis.hgetAll(&quot;phone:18609765435&quot;);
        System.out.println(Map.toString());
    }

    public static Jedis getJedis(){
        int timeoutCount = 0;
        while (true) {// 如果是网络超时则多试几次
            try
            {
                 Jedis jedis = new Jedis(redisConf.get(&quot;redis.hostname&quot;).toString(),
                         Integer.valueOf(redisConf.get(&quot;redis.port&quot;).toString()));
                return jedis;
            } catch (Exception e)
            {
                if (e instanceof JedisConnectionException || e instanceof SocketTimeoutException)
                {
                    timeoutCount++;
                    LOG.warn(&quot;获取jedis连接超时次数:&quot; +timeoutCount);
                    if (timeoutCount &gt; 4)
                    {
                        LOG.error(&quot;获取jedis连接超时次数a:&quot; +timeoutCount);
                        LOG.error(null,e);
                        break;
                    }
                }else
                {
                    LOG.error(&quot;getJedis error&quot;, e);
                    break;
                }
            }
        }
        return null;
    }

    public static void close(Jedis jedis){
        if(jedis!=null){
            jedis.close();
        }
    }
}</code></pre><h4 id="7、创建定时任务，将规则同步到redis"><a href="#7、创建定时任务，将规则同步到redis" class="headerlink" title="7、创建定时任务，将规则同步到redis"></a>7、创建定时任务，将规则同步到redis</h4><p><strong>新建 com.hsiehchou.spark.warn.timer 包</strong><br><strong>新建 SyncRule2Redis，WarnHelper</strong></p>
<p><strong>SyncRule2Redis.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.timer;

import java.util.TimerTask;

public class SyncRule2Redis extends TimerTask {
    @Override
    public void run() {
        //这里定义同步方法
        //就是读取mysql的数据 然后写入到redis中
        System.out.println(&quot;========开始同步MYSQL规则到redis=======&quot;);
        WarnHelper.syncRuleFromMysql2Redis();
        System.out.println(&quot;============开始同步规则成功===========&quot;);
    }
}</code></pre><p><strong>WarnHelper.java</strong></p>
<pre><code>package com.hsiehchou.spark.warn.timer;

import com.hsiehchou.redis.client.JedisSingle;
import com.hsiehchou.spark.warn.dao.XZ_RuleDao;
import com.hsiehchou.spark.warn.domain.XZ_RuleDomain;
import org.apache.commons.lang3.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import redis.clients.jedis.Jedis;

import java.util.List;

public class WarnHelper {

    private static final Logger LOG = LoggerFactory.getLogger(WarnHelper.class);

    /**
     * 同步mysql规则数据到redis
     */
    public static void syncRuleFromMysql2Redis(){
        //获取所有的规则
        List&lt;XZ_RuleDomain&gt; ruleList = XZ_RuleDao.getRuleList();
        Jedis jedis = null;
        try {
            //获取redis 客户端
            jedis = JedisSingle.getJedis(15);
            for (int i = 0; i &lt; ruleList.size(); i++) {
                XZ_RuleDomain rule = ruleList.get(i);
                String id = rule.getId()+&quot;&quot;;
                String publisher = rule.getPublisher();
                String warn_fieldname = rule.getWarn_fieldname();
                String warn_fieldvalue = rule.getWarn_fieldvalue();
                String send_mobile = rule.getSend_mobile();
                String send_type = rule.getSend_type();

                //拼接redis key值
                String redisKey = warn_fieldname +&quot;:&quot; + warn_fieldvalue;

                //通过redis hash结构   hashMap
                jedis.hset(redisKey,&quot;id&quot;,StringUtils.isNoneBlank(id) ? id : &quot;&quot;);
                jedis.hset(redisKey,&quot;publisher&quot;,StringUtils.isNoneBlank(publisher) ? publisher : &quot;&quot;);
                jedis.hset(redisKey,&quot;warn_fieldname&quot;,StringUtils.isNoneBlank(warn_fieldname) ? warn_fieldname : &quot;&quot;);
                jedis.hset(redisKey,&quot;warn_fieldvalue&quot;,StringUtils.isNoneBlank(warn_fieldvalue) ? warn_fieldvalue : &quot;&quot;);
                jedis.hset(redisKey,&quot;send_mobile&quot;,StringUtils.isNoneBlank(send_mobile) ? send_mobile : &quot;&quot;);
                jedis.hset(redisKey,&quot;send_type&quot;,StringUtils.isNoneBlank(send_type) ? send_type : &quot;&quot;);
            }
        } catch (Exception e) {
           LOG.error(&quot;同步规则到es失败&quot;,e);
        } finally {
            JedisSingle.close(jedis);
        }
    }

    public static void main(String[] args)
    {
        WarnHelper.syncRuleFromMysql2Redis();
    }
}</code></pre><h4 id="8、创建streaming流任务"><a href="#8、创建streaming流任务" class="headerlink" title="8、创建streaming流任务"></a>8、创建streaming流任务</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/warn</strong><br><strong>WarningStreamingTask.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.warn

import java.util.Timer

import com.hsiehchou.redis.client.JedisSingle
import com.hsiehchou.spark.common.SparkContextFactory
import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtil
import com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfig
import com.hsiehchou.spark.warn.service.BlackRuleWarning
import com.hsiehchou.spark.warn.timer.SyncRule2Redis
import org.apache.spark.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.kafka.KafkaManager
import redis.clients.jedis.Jedis

object WarningStreamingTask extends Serializable with Logging{

  def main(args: Array[String]): Unit = {

     //定义一个定时器去定时同步 MYSQL到REDIS
     val timer : Timer = new Timer

    //SyncRule2Redis 任务类
    //0 第一次开始执行
    //1*60*1000  隔多少时间执行一次
    timer.schedule(new SyncRule2Redis,0,1*60*1000)

     //从kafka中获取数据流
     //val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)
     //kafka topic
     val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)

     //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;WarningStreamingTask1&quot;, java.lang.Long.valueOf(10),1)
     val ssc:StreamingContext = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2esStreaming&quot;, java.lang.Long.valueOf(10))

    //构建kafkaManager
    val kafkaManager = new KafkaManager(
      Spark_Kafka_ConfigUtil.getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;), &quot;WarningStreamingTask111&quot;)
    )
    //使用kafkaManager创建DStreaming流
    val kafkaDS = kafkaManager.createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)
      //添加一个日期分组字段
      //如果数据其他的转换，可以先在这里进行统一转换
       .persist(StorageLevel.MEMORY_AND_DISK)


    kafkaDS.foreachRDD(rdd=&gt;{

      //流量预警
      //if(!rdd.isEmpty()){
/*      val count_flow = rdd.map(x=&gt;{
          val flow = java.lang.Long.valueOf(x.get(&quot;collect_time&quot;))
          flow
        }).reduce(_+_)
      if(count_flow &gt; 1719179595L){
        println(&quot;流量预警: 阈值[1719179595L] 实际值:&quot;+ count_flow)
      }*/
      //}

      //客户端连接之类的 最好不要放在RDD外面，因为在处理partion时，数据需要分发到各个节点上去
      //数据分发必须需要序列化才可以，如果不能序列化，分发会报错
      //如果这个数据 包括他里面的内容 都可以序列化，那么可以直接放在RDD外面
      var jedis:Jedis = null
      try {
        //jedis = JedisSingle.getJedis(15)
        rdd.foreachPartition(partion =&gt; {
          jedis = JedisSingle.getJedis(15)
          while (partion.hasNext) {
            val map = partion.next()
            val table = map.get(&quot;table&quot;)
            val mapObject = map.asInstanceOf[java.util.Map[String,Object]]
            println(table)
            //开始比对
            BlackRuleWarning.blackWarning(mapObject,jedis)
          }
        })
      } catch {
        case e =&gt; e.printStackTrace()
      } finally {
        JedisSingle.close(jedis)
      }


 /*       rdd.foreachPartition(partion =&gt; {
          var jedis: Jedis = null
          try {
            jedis = JedisSingle.getJedis(15)
            while (partion.hasNext) {
              val map = partion.next()
              val mapObject = map.asInstanceOf[java.util.Map[String, Object]]
              //开始比对
              BlackRuleWarning.blackWarning(mapObject, jedis)
            }
          } catch {
            case e =&gt; logError(null,e)
          }finally {
            JedisSingle.close(jedis)
          }
        })*/

    })

    ssc.start()
    ssc.awaitTermination()
  }
}</code></pre><h4 id="9、执行"><a href="#9、执行" class="headerlink" title="9、执行"></a>9、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>
<h4 id="10、截图"><a href="#10、截图" class="headerlink" title="10、截图"></a>10、截图</h4><p><img src="/medias/redis%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F.PNG" alt="redis安装成功"></p>
<p><img src="/medias/%E9%A2%84%E8%AD%A6.PNG" alt="预警"></p>
<p><img src="/medias/RedisManager.PNG" alt="RedisManager"></p>
<p><img src="/medias/mysql-xz_rule.PNG" alt="mysql-xz_rule"></p>
<p><img src="/medias/%E5%8F%91%E9%80%81%E9%A2%84%E8%AD%A6.PNG" alt="发送预警"></p>
<h4 id="11、redis安装"><a href="#11、redis安装" class="headerlink" title="11、redis安装"></a>11、redis安装</h4><p>解压：tar -zxvf redis-3.0.5.tar.gz<br>cd redis-3.0.5/<br>make<br>make PREFIX=/opt/software/redis install</p>
<p><strong>redis-benchmark</strong> ： Redis提供的压力测试工具。模拟产生客户端的压力<br><strong>redis-check-aof</strong> ： 检查aof日志文件<br><strong>redis-check-dump</strong> ： 检查rdb文件<br><strong>redis-cli</strong> ： Redis客户端脚本<br><strong>redis-sentinel</strong> ： 哨兵<br><strong>redis-server</strong> ： Redis服务器脚本</p>
<p><strong>核心配置文件:redis.conf</strong><br>[root@hsiehchou202 redis-3.0.5]# cp redis.conf /opt/software/redis<br>[root@hsiehchou202 redis]# mkdir conf<br>[root@hsiehchou202 redis]# mv redis.conf conf/<br>[root@hsiehchou202 conf]# vi redis.conf</p>
<p>42行 <strong>daemonize yes //后台方式运行</strong><br>50行 <strong>port 6379</strong></p>
<p>启动<strong>redis ./bin/redis-server conf/redis.conf</strong></p>
<p><strong>检测是否启动好</strong><br>[root@hsiehchou202 redis]# <strong>bin/redis-server conf/redis.conf</strong></p>
<h3 id="十、Spark—kafka2hive"><a href="#十、Spark—kafka2hive" class="headerlink" title="十、Spark—kafka2hive"></a>十、Spark—kafka2hive</h3><h4 id="1、CDH启用Hive-on-spark"><a href="#1、CDH启用Hive-on-spark" class="headerlink" title="1、CDH启用Hive on spark"></a>1、CDH启用Hive on spark</h4><p><strong>设置 hive on spark 参数</strong><br>原来的HIVE执行引擎使用的hadoop的mapreduce，Hive on Spark 就是讲执行引擎换为spark 引擎</p>
<h4 id="2、hive配置文件"><a href="#2、hive配置文件" class="headerlink" title="2、hive配置文件"></a>2、hive配置文件</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs/</strong></p>
<p><strong>HiveConfig.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfs

import java.util

import org.apache.commons.configuration.{CompositeConfiguration, ConfigurationException, PropertiesConfiguration}
import org.apache.spark.Logging
import org.apache.spark.sql.types.{StringType, StructField, StructType}

import scala.collection.mutable.ArrayBuffer
import scala.collection.JavaConversions._

object HiveConfig extends Serializable with Logging {

  //HIVE 文件根目录
  var hive_root_path = &quot;/apps/hive/warehouse/external/&quot;
  var hiveFieldPath = &quot;es/mapping/fieldmapping.properties&quot;

  var config: CompositeConfiguration = null

  //所有的表
  var tables: util.List[_] = null

  //表对应所有的字段映射,可以通过table名获取 这个table的所有字段
  var tableFieldsMap: util.Map[String, util.HashMap[String, String]] = null

  //StructType
  var mapSchema: util.Map[String, StructType] = null

  //建表语句
  var hiveTableSQL: util.Map[String, String] = null

  /**
    * 主要就是创建mapSchema  和  hiveTableSQL
    */
  initParams()

  def main(args: Array[String]): Unit = {
  }

  /**
    * 初始化HIVE参数
    */
  def initParams(): Unit = {
    //加载es/mapping/fieldmapping.properties 配置文件
    config = HiveConfig.readCompositeConfiguration(hiveFieldPath)
    println(&quot;==========================config====================================&quot;)

    config.getKeys.foreach(key =&gt; {
      println(key + &quot;:&quot; + config.getProperty(key.toString))
    })
    println(&quot;==========================tables====================================&quot;)
    //wechat,mail,qq
    tables = config.getList(&quot;tables&quot;)

    tables.foreach(table =&gt; {
      println(table)
    })

    var tables1 = config.getProperty(&quot;tables&quot;)

    println(&quot;======================tableFieldsMap================================&quot;)
    //(qq,{qq.imsi=string, qq.id=string, qq.send_message=string, qq.filename=string})
    tableFieldsMap = HiveConfig.getKeysByType()
    tableFieldsMap.foreach(x =&gt; {
      println(x)
    })
    println(&quot;=========================mapSchema===================================&quot;)
    mapSchema = HiveConfig.createSchema()
    mapSchema.foreach(x =&gt; {
//      val structType = x._2
//      println(&quot;-----------&quot;)
//      println(structType)
//
//
//      val names = structType.fieldNames
//      names.foreach(field =&gt; {
//        println(field)
//      })
      println(x)
    })
    println(&quot;=========================hiveTableSQL===================================&quot;)
    hiveTableSQL = HiveConfig.getHiveTables()
    hiveTableSQL.foreach(x =&gt; {
      println(x)
    })
  }

  /**
    * 读取hive 字段配置文件
    * @param path
    * @return
    */
  def readCompositeConfiguration(path: String): CompositeConfiguration = {
    logInfo(&quot;加载配置文件 &quot; + path)
    //多配置工具
    val compositeConfiguration = new CompositeConfiguration
    try {
      val configuration = new PropertiesConfiguration(path)
      compositeConfiguration.addConfiguration(configuration)
    } catch {
      case e: ConfigurationException =&gt; {
        logError(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e)
      }
    }
    logInfo(&quot;加载配置文件&quot; + path + &quot;成功。 &quot;)
    compositeConfiguration
  }

  /**
    * 获取table-字段 对应关系
    * 使用 util.Map[String,util.HashMap[String, String结构保存
    * @return
    */
  def getKeysByType(): util.Map[String, util.HashMap[String, String]] = {

    val map = new util.HashMap[String, util.HashMap[String, String]]()
    println(&quot;__________________tables_____________________&quot;+tables)
    //wechat, mail, qq
    val iteratorTable = tables.iterator()

    //对每个表进行遍历
    while (iteratorTable.hasNext) {

      //使用一个MAP保存一种对应关系
      val fieldMap = new util.HashMap[String, String]()

      //获取一个表
      val table: String = iteratorTable.next().toString
      //获取这个表的所有字段
      val fields = config.getKeys(table)
      //获取通用字段  这里暂时没有
      val commonKeys: util.Iterator[String] = config.getKeys(&quot;common&quot;).asInstanceOf[util.Iterator[String]]

      //将通用字段放到map结构中去
      while (commonKeys.hasNext) {
        val key = commonKeys.next()
        fieldMap.put(key.replace(&quot;common&quot;, table), config.getString(key))
      }

      //将每种表的私有字段放到map中去
      while (fields.hasNext) {
        val field = fields.next().toString
        fieldMap.put(field, config.getString(field))
        println(&quot;__________________field_____________________&quot;+&quot;\n&quot;+field)
      }
      map.put(table, fieldMap)
    }
    map
  }

  /**
    * 构建建表语句
    * 例如CREATE external TABLE IF NOT EXISTS qq (imei string,imsi string,longitude string,latitude string,phone_mac string,device_mac string,device_number string,collect_time string,username string,phone string,object_username string,send_message string,accept_message string,message_time string,id string,table string,filename string,absolute_filename string)
    * @return
    */
  def getHiveTables(): util.Map[String, String] = {

    val hiveTableSqlMap: util.Map[String, String] = new util.HashMap[String, String]()

    //获取没中数据的建表语句
    tables.foreach(table =&gt; {

      var sql: String = s&quot;CREATE external TABLE IF NOT EXISTS ${table} (&quot;

      val tableFields = config.getKeys(table.toString)
      tableFields.foreach(tableField =&gt; {
        //qq.imsi=string, qq.id=string, qq.send_message=string
        val fieldType = config.getProperty(tableField.toString)
        val field = tableField.toString.split(&quot;\\.&quot;)(1)
        sql = sql + field
        fieldType match {
          //就是将配置中的类型映射为HIVE 建表语句中的类型
          case &quot;string&quot; =&gt; sql = sql + &quot; string,&quot;
          case &quot;long&quot; =&gt; sql = sql + &quot; string,&quot;
          case &quot;double&quot; =&gt; sql = sql + &quot; string,&quot;
          case _ =&gt; println(&quot;Nothing Matched!!&quot; + fieldType)
        }
      })
      sql = sql.substring(0, sql.length - 1)
      //sql = sql + s&quot;)STORED AS PARQUET location &#39;${hive_root_path}${table}&#39;&quot;
      sql = sql + s&quot;) partitioned by(year string,month string,day string) STORED AS PARQUET &quot; + s&quot;location &#39;${hive_root_path}${table}&#39;&quot;
      hiveTableSqlMap.put(table.toString, sql)
    })
    hiveTableSqlMap
  }

  /**
    * 使用tableFieldsMap
    * 对每种类型数据创建对应的Schema
    * @return
    */
  def createSchema(): util.Map[String, StructType] = {
    // schema  表结构
    /*   CREATE TABLE `warn_message` (
         //arrayStructType
         `id` int(11) NOT NULL AUTO_INCREMENT,
         `alarmRuleid` varchar(255) DEFAULT NULL,
         `alarmType` varchar(255) DEFAULT NULL,
         `sendType` varchar(255) DEFAULT NULL,
         `sendMobile` varchar(255) DEFAULT NULL,
         `sendEmail` varchar(255) DEFAULT NULL,
         `sendStatus` varchar(255) DEFAULT NULL,
         `senfInfo` varchar(255) CHARACTER SET utf8 DEFAULT NULL,
         `hitTime` datetime DEFAULT NULL,
         `checkinTime` datetime DEFAULT NULL,
         `isRead` varchar(255) DEFAULT NULL,
         `readAccounts` varchar(255) DEFAULT NULL,
         `alarmaccounts` varchar(255) DEFAULT NULL,
         `accountid` varchar(11) DEFAULT NULL,
         PRIMARY KEY (`id`)
       ) ENGINE=MyISAM AUTO_INCREMENT=528 DEFAULT CHARSET=latin1;*/

    val mapStructType: util.Map[String, StructType] = new util.HashMap[String, StructType]()

    for (table &lt;- tables) {
      //通过tableFieldsMap 拿到这个表的所有字段
      val tableFields = tableFieldsMap.get(table)
      //对这个字段进行遍历
      val keyIterator = tableFields.keySet().iterator()
      //创建ArrayBuffer
      var arrayStructType = ArrayBuffer[StructField]()
      while (keyIterator.hasNext) {
        val key = keyIterator.next()
        val value = tableFields.get(key)

        //将key拆分 获取 &quot;.&quot;后面的部分作为数据字段
        val field = key.split(&quot;\\.&quot;)(1)
        value match {
          /* case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)
           case &quot;long&quot;   =&gt; arrayStructType += StructField(field, LongType, true)
           case &quot;double&quot;   =&gt; arrayStructType += StructField(field, DoubleType, true)*/
          case &quot;string&quot; =&gt; arrayStructType += StructField(field, StringType, true)
          case &quot;long&quot; =&gt; arrayStructType += StructField(field, StringType, true)
          case &quot;double&quot; =&gt; arrayStructType += StructField(field, StringType, true)
          case _ =&gt; println(&quot;Nothing Matched!!&quot; + value)
        }
      }
      val schema = StructType(arrayStructType)
      mapStructType.put(table.toString, schema)
    }
    mapStructType
  }
}</code></pre><h4 id="3、kafka写hdfs和创建hive表"><a href="#3、kafka写hdfs和创建hive表" class="headerlink" title="3、kafka写hdfs和创建hive表"></a>3、kafka写hdfs和创建hive表</h4><p><strong>Kafka2HiveTest.scala</strong> </p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfs

import java.util

import com.hsiehchou.hdfs.HdfsAdmin
import com.hsiehchou.hive.HiveConf
import com.hsiehchou.spark.common.{SparkContextFactory}
import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtil
import com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming.kafkaConfig
import org.apache.hadoop.fs.Path
import org.apache.spark.{Logging}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.{DataFrame, Row, SaveMode}
import org.apache.spark.sql.types.StructType
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kafka.KafkaManager

import scala.collection.JavaConversions._

object Kafka2HiveTest extends Serializable with Logging{

  val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)

  //获取所有数据类型
  //获取所有数据的Schema
  def main(args: Array[String]): Unit = {
    //val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;XZ_kafka2es&quot;, java.lang.Long.valueOf(10),1)

    val ssc = SparkContextFactory.newSparkStreamingContext(&quot;Kafka2HiveTest&quot;, java.lang.Long.valueOf(10))

    //1.创建HIVE表  hiveSQL已經創建好了
    val sc = ssc.sparkContext
    val hiveContext: HiveContext = HiveConf.getHiveContext(sc)
    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;)
    createHiveTable(hiveContext)

    //kafka拿到流数据
    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil
                                    .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),
                                      &quot;Kafka2HiveTest&quot;))
                                    .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)
                                    .persist(StorageLevel.MEMORY_AND_DISK)

    HiveConfig.tables.foreach(table=&gt;{
      //过滤出单一数据类型(获取和table相同类型的所有数据)
       val tableDS = kafkaDS.filter(x =&gt; {table.equals(x.get(&quot;table&quot;))})

      //获取数据类型的schema 表结构
      val schema = HiveConfig.mapSchema.get(table)

      //获取这个表的所有字段
      val schemaFields: Array[String] = schema.fieldNames
      tableDS.foreachRDD(rdd=&gt;{

        //TODO 数据写入HDFS
        /* val sc = rdd.sparkContext
        val hiveContext = HiveConf.getHiveContext(sc)
        hiveContext.sql(s&quot;USE DEFAULT&quot;)*/

        //将RDD转为DF   原因：要加字段描述，写比较方便
        val tableDF = rdd2DF(rdd,schemaFields,hiveContext,schema)

        //多种数据一起处理
        val path_all = s&quot;hdfs://hadoop1:8020${HiveConfig.hive_root_path}${table}&quot;
        val exists = HdfsAdmin.get().getFs.exists(new Path(path_all))

        //2.写到HDFS   不管存不存在我们都要把数据写入进去 通过追加的方式
        //每10秒写一次，写一次会生成一个文件
        tableDF.write.mode(SaveMode.Append).parquet(path_all)

        //3.加载数据到HIVE
        if (!exists) {
          //如果不存在 进行首次加载
          System.out.println(&quot;===================开始加载数据到分区=============&quot;)
          hiveContext.sql(s&quot;ALTER TABLE ${table} LOCATION &#39;${path_all}&#39;&quot;)
        }
      })
    })
    ssc.start()
    ssc.awaitTermination()
  }

  /**
    * 创建HIVE表
    * @param hiveContext
    */
  def createHiveTable(hiveContext: HiveContext): Unit ={
    val keys = HiveConfig.hiveTableSQL.keySet()
    keys.foreach(key=&gt;{
      val sql = HiveConfig.hiveTableSQL.get(key)
      //通过hiveContext 和已经创建好的SQL语句去创建HIVE表
      hiveContext.sql(sql)
      println(s&quot;创建表${key}成功&quot;)
    })
  }

  /**
    * 将RDD转为DF
    * @param rdd
    * @param schemaFields
    * @param hiveContext
    * @param schema
    * @return
    */
  def rdd2DF(rdd:RDD[util.Map[String,String]],
             schemaFields: Array[String],
             hiveContext:HiveContext,
             schema:StructType): DataFrame ={

      //将RDD[Map[String,String]]转为RDD[ROW]
      val rddRow = rdd.map(recourd =&gt; {
        val listRow: util.ArrayList[Object] = new util.ArrayList[Object]()
          for (schemaField &lt;- schemaFields) {
            listRow.add(recourd.get(schemaField))
          }
          Row.fromSeq(listRow)
          //所有分区合并成一个
      }).repartition(1)
    //构建DF
    //def createDataFrame(rowRDD: RDD[Row], schema: StructType)
    val typeDF = hiveContext.createDataFrame(rddRow, schema)
    typeDF
  }
}</code></pre><h4 id="4、Kafka2HiveTest-执行"><a href="#4、Kafka2HiveTest-执行" class="headerlink" title="4、Kafka2HiveTest 执行"></a>4、Kafka2HiveTest 执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>
<p><img src="/medias/%E5%AD%98%E5%88%B0hdfs%E4%B8%AD.PNG" alt="存到hdfs中"></p>
<p><img src="/medias/hive%E6%9F%A5%E8%AF%A21.PNG" alt="hive查询1"></p>
<h4 id="5、xz-bigdata-spark-src-java"><a href="#5、xz-bigdata-spark-src-java" class="headerlink" title="5、xz_bigdata_spark/src/java/"></a>5、xz_bigdata_spark/src/java/</h4><p><strong>com/hsiehchou/hdfs</strong><br><strong>HdfsAdmin.java—HDFS 文件操作类</strong></p>
<pre><code>package com.hsiehchou.hdfs;

import com.hsiehchou.common.adjuster.StringAdjuster;
import com.hsiehchou.common.file.FileCommon;
import com.google.common.base.Preconditions;
import com.google.common.collect.Lists;
import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.log4j.Logger;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.lang.reflect.Array;
import java.util.Collection;
import java.util.List;

/**
 * HDFS 文件操作类
 */
public class HdfsAdmin {

    private static Logger LOG;
    private static final String HDFS_SITE = &quot;/hadoop/hdfs-site.xml&quot;;
    private static final String CORE_SITE = &quot;/hadoop/core-site.xml&quot;;

    private volatile static HdfsAdmin hdfsAdmin;

    private  FileSystem fs;

    private HdfsAdmin(Configuration conf, Logger logger){
        try {
            if(conf == null) conf = newConf();
            conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop1:8020&quot;);
            fs = FileSystem.get(conf);
        } catch (IOException e) {
            LOG.error(&quot;获取 hdfs的FileSystem出现异常。&quot;, e);
        }
        Preconditions.checkNotNull(fs, &quot;没有获取到可用的Hdfs的FileSystem&quot;);
        this.LOG = logger;
        if(this.LOG == null)
            this.LOG = Logger.getLogger(HdfsAdmin.class);
    }

    private Configuration newConf(){

        Configuration conf = new Configuration();
        if(FileCommon.exist(HDFS_SITE)) conf.addResource(HDFS_SITE);
        if(FileCommon.exist(CORE_SITE)) conf.addResource(CORE_SITE);
        return conf;
    }

    public static HdfsAdmin get(){
        return get(null);
    }

    /**
     * 获取hdfsAdmin
     * @param logger
     * @return
     */
    public static HdfsAdmin get(Logger logger){
        if(hdfsAdmin == null){
            synchronized (HdfsAdmin.class){
                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(null, logger);
            }
        }
        return hdfsAdmin;
    }

    public static HdfsAdmin get(Configuration conf, Logger logger){
        if(hdfsAdmin == null){
            synchronized (HdfsAdmin.class){
                if(hdfsAdmin == null) hdfsAdmin = new HdfsAdmin(conf, logger);
            }
        }
        return hdfsAdmin;
    }

    public FileStatus getFileStatus(String dir) {
        FileStatus fileStatus = null;
        try {
            fileStatus = fs.getFileStatus(new Path(dir));
        } catch (IOException e) {
            LOG.error(String.format(&quot;获取文件 %s信息失败。&quot;, dir), e);
        }
        return fileStatus;
    }

    public void createFile(String dst , byte[] contents){
        //目标路径
        Path dstPath = new Path(dst);

        //打开一个输出流
        FSDataOutputStream outputStream;
        try {
            outputStream = fs.create(dstPath);
            outputStream.write(contents);
            outputStream.flush();
            outputStream.close();
        } catch (IOException e) {
            LOG.error(String.format(&quot;创建文件 %s 失败。&quot;, dst), e);
        }
        LOG.info(String.format(&quot;文件: %s 创建成功！&quot;, dst));
    }

    //上传本地文件
    public void uploadFile(String src,String dst){
        //原路径
        Path srcPath = new Path(src);

        //目标路径
        Path dstPath = new Path(dst);

        //调用文件系统的文件复制函数,前面参数是指是否删除原文件，true为删除，默认为false
        try {
            fs.copyFromLocalFile(false,srcPath, dstPath);
        } catch (IOException e) {
            LOG.error(String.format(&quot;上传文件 %s 到 %s 失败。&quot;, src, dst), e);
        }
        //打印文件路径
        LOG.info(String.format(&quot;上传文件 %s 到 %s 完成。&quot;, src, dst));
    }

    public void downloadFile(String src , String dst){
        Path dstPath = new Path(dst) ;
        try {
            fs.copyToLocalFile(false, new Path(src), dstPath);
        } catch (IOException e) {
            LOG.error(String.format(&quot;下载文件 %s 到 %s 失败。&quot;, src, dst), e);
        }
        LOG.info(String.format(&quot;下载文件 %s 到 %s 完成&quot;, src, dst));
    }

    //文件重命名
    public void rename(String oldName,String newName){

        Path oldPath = new Path(oldName);
        Path newPath = new Path(newName);
        boolean isok = false;
        try {
            isok = fs.rename(oldPath, newPath);
        } catch (IOException e) {
            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName), e);
        }
        if(isok){
            LOG.info(String.format(&quot;重命名文件 %s 为 %s 完成。&quot;, oldName, newName));
        }else{
            LOG.error(String.format(&quot;重命名文件 %s 为 %s 失败。&quot;, oldName, newName));
        }
    }

    public void delete(String path){
        delete(path, true);
    }

    //删除文件
    public void delete(String path, boolean recursive){

        Path deletePath = new Path(path);
        boolean isok = false;
        try {
            isok = fs.delete(deletePath, recursive);
        } catch (IOException e) {
            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path), e);
        }
        if(isok){
            LOG.info(String.format(&quot;删除文件 %s 完成。&quot;, path));
        }else{
            LOG.error(String.format(&quot;删除文件 %s 失败。&quot;, path));
        }
    }

    //创建目录
    public void mkdir(String path){

        Path srcPath = new Path(path);
        boolean isok = false;
        try {
            isok = fs.mkdirs(srcPath);
        } catch (IOException e) {
            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path), e);
        }
        if(isok){
            LOG.info(String.format(&quot;创建目录 %s 完成。&quot;, path));
        }else{
            LOG.error(String.format(&quot;创建目录 %s 失败。&quot;, path));
        }
    }

    //读取文件的内容
    public InputStream readFile(String filePath){
        Path srcPath = new Path(filePath);
        InputStream in = null;
        try {
           in = fs.open(srcPath);
        } catch (IOException e) {
            LOG.error(String.format(&quot;读取文件  %s 失败。&quot;, filePath), e);
        }
        return in;
    }

    public &lt;T&gt; void readFile(String filePath, StringAdjuster&lt;T&gt; adjuster, Collection&lt;T&gt; result){
        InputStream inputStream = readFile(filePath);
        if(inputStream != null){
            InputStreamReader reader = new InputStreamReader(inputStream);
            BufferedReader bufferedReader = new BufferedReader(reader);
            String line;
            try {
                T t;
                while((line = bufferedReader.readLine()) != null){
                    t = adjuster.doAdjust(line);
                    if(t != null)result.add(t);
                }
            } catch (IOException e) {
                LOG.error(String.format(&quot;利用缓冲流读取文件  %s 失败。&quot;, filePath), e);
            }finally {
                IOUtils.closeQuietly(bufferedReader);
                IOUtils.closeQuietly(reader);
                IOUtils.closeQuietly(inputStream);
            }
        }
    }

    public List&lt;String&gt; readLines(String filePath){
        return readLines(filePath, &quot;UTF-8&quot;);
    }

    public  List&lt;String&gt; readLines(String filePath, String encoding){
        InputStream inputStream = readFile(filePath);
        List&lt;String&gt; lines = null;
        if(inputStream != null) {
            try {
                lines = IOUtils.readLines(inputStream, encoding);
            } catch (IOException e) {
                LOG.error(String.format(&quot;按行读取文件 %s 失败。&quot;, filePath), e);
            }finally {
                IOUtils.closeQuietly(inputStream);
            }
        }
        return lines;
    }

    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,
                                                final boolean onlyFile, final boolean onlyDir){
       return findNewFileOrDirInDir(dir, filter, onlyFile, onlyDir, false);
    }

    public List&lt;FileStatus&gt; findNewFileOrDirInDir(String dir, HdfsFileFilter filter,
                          final boolean onlyFile, final boolean onlyDir, boolean recursive){
        if(onlyFile &amp;&amp; onlyDir){
            FileStatus fileStatus = getFileStatus(dir);
            if(fileStatus == null)return Lists.newArrayList();
            if(isAccepted(fileStatus,filter)){
                return Lists.newArrayList(fileStatus);
            }
            return Lists.newArrayList();
        }

       if(onlyFile){
           return findNewFileInDir(dir, filter, recursive);
       }

       if(onlyDir){
           return findNewDirInDir(dir, filter, recursive);
       }
       return Lists.newArrayList();
    }

    /**
     * 查找一个文件夹中 新建的目录
     * @param dir
     * @param filter
     * @return
     */
    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter){
        return findNewDirInDir(new Path(dir), filter, false);
    }
    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter){
        return findNewDirInDir(path, filter, false);
    }

    public List&lt;FileStatus&gt; findNewDirInDir(String dir, HdfsFileFilter filter, boolean recursive){
        return findNewDirInDir(new Path(dir), filter, recursive);
    }

    public List&lt;FileStatus&gt; findNewDirInDir(Path path, HdfsFileFilter filter, boolean recursive){
        FileStatus[] files = null;
        try {
            files = fs.listStatus(path);
        } catch (IOException e) {
            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);
        }
        if(files == null)return Lists.newArrayList();

        List&lt;FileStatus&gt; paths = Lists.newArrayList();
        List&lt;String&gt; res = Lists.newArrayList();
        for(FileStatus fileStatus : files){
            if (fileStatus.isDirectory()) {
                if (isAccepted(fileStatus, filter)) {
                    paths.add(fileStatus);
                    res.add(fileStatus.getPath().toString());
                }else if(recursive){
                    paths.addAll(findNewDirInDir(fileStatus.getPath(), filter, recursive));
                }
            }
        }
        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;,
                path, filter,res.size(), res));
        return paths;
    }

    /**
     * 查找一个文件夹中 新建的文件
     * @param dir
     * @param filter
     * @return
     */
    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter){
        return  findNewFileInDir(new Path(dir), filter, false);
    }

    public List&lt;FileStatus&gt; findNewFileInDir(String dir, HdfsFileFilter filter, boolean recursive){
        return  findNewFileInDir(new Path(dir), filter, recursive);
    }

    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter){
        return  findNewFileInDir(path, filter, false);
    }

    public List&lt;FileStatus&gt; findNewFileInDir(Path path, HdfsFileFilter filter, boolean recursive){

        FileStatus[] files = null;
        try {
            files = fs.listStatus(path);
        } catch (IOException e) {
            LOG.error(String.format(&quot;获取目录 %s下的文件列表失败。&quot;, path), e);
        }
        if(files == null)return Lists.newArrayList();

        List&lt;FileStatus&gt; paths = Lists.newArrayList();
        List&lt;String&gt; res = Lists.newArrayList();
        for(FileStatus fileStatus : files){
            if (fileStatus.isFile()) {
                if (isAccepted(fileStatus, filter)) {
                    paths.add(fileStatus);
                    res.add(fileStatus.getPath().toString());
                }
            }else if(recursive){
                paths.addAll(findNewFileInDir(fileStatus.getPath(), filter, recursive));
            }
        }
        LOG.info(String.format(&quot;从目录%s 找到满足条件%s 有如下 %s 个文件： %s&quot;, path, filter,res.size(), res));

        return paths;
    }

    private boolean isAccepted(String file, HdfsFileFilter filter) {
        if(filter == null) return true;
        FileStatus fileStatus = getFileStatus(file);
        if(fileStatus == null)return false;
        return isAccepted(fileStatus, filter);
    }

    private boolean isAccepted(FileStatus fileStatus, HdfsFileFilter filter) {
        return  filter == null ? true : filter.filter(fileStatus);
    }

    public long getModificationTime(Path path){
        try {
            FileStatus status = fs.getFileStatus(path);
            return status.getModificationTime();
        } catch (IOException e) {
            LOG.error(String.format(&quot;获取路径 %s信息失败。&quot;, path), e);
        }
        return -1L;
    }

    public FileSystem getFs() {
        return fs;
    }

    public static void main(String[] args) throws Exception {
        // HdfsAdmin hdfsAdmin = HdfsAdmin.get();
       // hdfsAdmin.mkdir(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);
        //System.out.println(hdfsAdmin.getFs().exists(new Path(&quot;hdfs://hdp04.ultiwill.com:8020/test&quot;)));
        //hdfsAdmin.delete(&quot;hdfs://hdp04.ultiwill.com:8020/test1111&quot;);
        //System.out.println(&quot;hdfsAdmin = &quot; + );
       // List&lt;FileStatus&gt; status = hdfsAdmin.findNewDirInDir(&quot;hdfs://hdp04.ultiwill.com:50070/hdp&quot;, null);
        //System.out.println(&quot;status = &quot; + status.size());
    }
}</code></pre><p><strong>HdfsFileFilter.java</strong></p>
<pre><code>package com.hsiehchou.hdfs;

import com.hsiehchou.common.filter.Filter;
import org.apache.hadoop.fs.FileStatus;

public abstract class HdfsFileFilter implements Filter&lt;FileStatus&gt; {

}</code></pre><p><strong>com/hsiehchou/hive</strong><br><strong>HiveConf.java</strong></p>
<pre><code>package com.hsiehchou.hive;

import org.apache.hadoop.conf.Configuration;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.hive.HiveContext;

import java.util.Iterator;
import java.util.Map;

public class HiveConf {

    //private static String DEFUALT_CONFIG = &quot;spark/hive/hive-server-config&quot;;
    private static HiveConf hiveConf;
    private static HiveContext hiveContext;

    private HiveConf(){

    }

    public static HiveConf getHiveConf(){
        if(hiveConf==null){
            synchronized (HiveConf.class){
                if(hiveConf==null){
                    hiveConf=new  HiveConf();
                }
            }
        }
        return hiveConf;
    }

    public static HiveContext getHiveContext(SparkContext sparkContext){
        if(hiveContext==null){
            synchronized (HiveConf.class){
                if(hiveContext==null){
                    hiveContext = new  HiveContext(sparkContext);
                    Configuration conf = new Configuration();
                    conf.addResource(&quot;spark/hive/hive-site.xml&quot;);
                    Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = conf.iterator();
                    while (iterator.hasNext()) {
                        Map.Entry&lt;String, String&gt; next = iterator.next();
                        hiveContext.setConf(next.getKey(), next.getValue());
                    }
                    hiveContext.setConf(&quot;spark.sql.parquet.mergeSchema&quot;, &quot;true&quot;);
                }
            }
        }
        return hiveContext;
    }
}</code></pre><h4 id="6、小文件合并"><a href="#6、小文件合并" class="headerlink" title="6、小文件合并"></a>6、小文件合并</h4><p><strong>scala/com/hsiehchou/spark/streaming/kafka/kafka2hdfs</strong></p>
<p><strong>CombineHdfs.scala—合并HDFS小文件任务</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hdfs

import com.hsiehchou.hdfs.HdfsAdmin
import com.hsiehchou.spark.common.SparkContextFactory
import org.apache.hadoop.fs.{FileSystem, FileUtil, Path}
import org.apache.spark.Logging
import org.apache.spark.sql.{SQLContext, SaveMode}

import scala.collection.JavaConversions._

/**
  * 合并HDFS小文件任务
  */
object CombineHdfs extends Serializable with Logging{

  def main(args: Array[String]): Unit = {
    //  val sparkContext = SparkContextFactory.newSparkBatchContext(&quot;CombineHdfs&quot;)

    val sparkContext = SparkContextFactory.newSparkLocalBatchContext(&quot;CombineHdfs&quot;)

    //创建一个 sparkSQL
    val sqlContext: SQLContext = new SQLContext(sparkContext)

    //遍历表 就是遍历HIVE表
    HiveConfig.tables.foreach(table=&gt;{

      //获取HDFS文件目录
      //apps/hive/warehouse/external/mail类似
      //apps/hive/warehouse/external/mail
      val table_path =s&quot;${HiveConfig.hive_root_path}$table&quot; 

      //通过sparkSQL 加载 这些目录的文件
      val tableDF = sqlContext.read.load(table_path)

      //先获取原来数据种的所有文件  HDFS文件 API
      val fileSystem:FileSystem = HdfsAdmin.get().getFs

      //通过globStatus 获取目录下的正则匹配文件
      //fileSystem.listFiles()
      val arrayFileStatus = fileSystem.globStatus(new Path(table_path+&quot;/part*&quot;))

      //stat2Paths将文件状态转为文件路径   这个文件路径是用来删除的
      val paths = FileUtil.stat2Paths(arrayFileStatus)

      //写入合并文件   //repartition 需要根据生产中实际情况去定义
      tableDF.repartition(1).write.mode(SaveMode.Append).parquet(table_path)
      println(&quot;写入&quot; + table_path +&quot;成功&quot;)

      //删除小文件
      paths.foreach(path =&gt;{
        HdfsAdmin.get().getFs.delete(path)
        println(&quot;删除文件&quot; + path + &quot;成功&quot;)
      })
    })
  }
}</code></pre><h4 id="7、定时任务"><a href="#7、定时任务" class="headerlink" title="7、定时任务"></a>7、定时任务</h4><p><strong>命令行输入：crontab -e</strong></p>
<p><strong>内容：</strong><br><code>0 1 * * *</code> spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>
<p><strong>说明：</strong><br><code>* * * * *</code> 执行的任务</p>
<table>
<thead>
<tr>
<th align="center">项目</th>
<th align="center">含义</th>
<th align="center">范围</th>
</tr>
</thead>
<tbody><tr>
<td align="center">第一个“*”</td>
<td align="center">一小时当中的第几分钟（分）</td>
<td align="center">0-59</td>
</tr>
<tr>
<td align="center">第二个“*”</td>
<td align="center">一天当中的第几小时（时）</td>
<td align="center">0-23</td>
</tr>
<tr>
<td align="center">第三个“*”</td>
<td align="center">一个月当中的第几天（天）</td>
<td align="center">1-31</td>
</tr>
<tr>
<td align="center">第四个“*”</td>
<td align="center">一年当中的第几月（月）</td>
<td align="center">1-12</td>
</tr>
<tr>
<td align="center">第五个“*”</td>
<td align="center">一周当中的星期几（周）</td>
<td align="center">0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<h4 id="8、合并小文件截图"><a href="#8、合并小文件截图" class="headerlink" title="8、合并小文件截图"></a>8、合并小文件截图</h4><p><img src="/medias/%E5%90%88%E5%B9%B6%E5%B0%8F%E6%96%87%E4%BB%B6.PNG" alt="合并小文件">        </p>
<h4 id="9、hive命令"><a href="#9、hive命令" class="headerlink" title="9、hive命令"></a>9、hive命令</h4><p>show tales;</p>
<p>hdfs dfs -ls /apps/hive/warehouse/external</p>
<p>hdfs dfs -rm -r /apps/hive/warehouse/external/mail</p>
<p>drop table mail;</p>
<p>desc qq;</p>
<p>select * from qq limit 1;<br>select count(*) from qq;</p>
<p>/usr/bin下面的启动zookeeper客户端<br>zookeeper-client</p>
<p>删除zookeeper里面的消费者数据<br>rmr /consumers/WarningStreamingTask2/offsets</p>
<p>rmr /consumers/Kafka2HiveTest/offsets</p>
<p>rmr /consumers/DataRelationStreaming1/offsets</p>
<h3 id="十一、Spark—Kafka2Hbase"><a href="#十一、Spark—Kafka2Hbase" class="headerlink" title="十一、Spark—Kafka2Hbase"></a>十一、Spark—Kafka2Hbase</h3><h4 id="1、数据关联"><a href="#1、数据关联" class="headerlink" title="1、数据关联"></a>1、数据关联</h4><p><strong>（1）为什么需要关联</strong><br><strong>问题</strong>：我们不能充分了解数据之间的关联关系。</p>
<p><strong>公司中应用的非常多</strong><br><strong>离线关联</strong>，传通数据 mysql 通过关联字段去关联。<br>但是，如果数据量非常大，关联表非常多。处理不了。</p>
<p>数据零散，只能从单一维度去看数据，看的面比较窄。<br>如果需要从多个维度分析，关联成本比较大。</p>
<p>建立数据之间的关联关系，实现<strong>关联查询</strong>的<strong>毫秒级响应</strong>；<br>另一个方面，可以为数据挖掘，机器学习<strong>提供训练数据</strong>。</p>
<p>后面进行机器学习的时候，都需要从<strong>多维度</strong>对数据进行<strong>分析和建模</strong>。</p>
<p><strong>（2）HBASE 只要rowkey一样，那么他们就是一条数据</strong><br>QQ<br>aa-aa-aa-aa-aa-aa 666666</p>
<p>微信<br>aa-aa-aa-aa-aa-aa weixin</p>
<p>邮箱<br>aa-aa-aa-aa-aa-aa <a href="mailto:666666@qq.com">666666@qq.com</a></p>
<p><strong>（3）如何关联</strong><br>一对一的情况 :<br><a href="https://blog.csdn.net/shujuelin/article/details/83657485" target="_blank" rel="noopener">https://blog.csdn.net/shujuelin/article/details/83657485</a></p>
<p><strong>使用HBASE写入特性</strong><br>比如 MAC1  1789932321<br> MAC1  <a href="mailto:88888@qq.com">88888@qq.com</a><br> MAC1  88888 </p>
<p>一对多的情况怎么处理<br> <strong>使用多版本</strong><br>aa-aa-aa-aa-aa-aa 666666<br>aa-aa-aa-aa-aa-aa 777777</p>
<p><strong>（4）一对多</strong><br>使用多版本存一堆多的关系<br>多版本 插入了一个777777 一个版本<br>再插入一个777777   一个版本</p>
<p>所以需要自定义版本号 确定版本唯一<br>通过 “888888”.hashCode() &amp; Integer.MAX_VALUE</p>
<p><strong>（5）如果实现hbase多字段查询</strong><br>往主关联表 test:relation 里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1637094383 类型phone_mac value=&gt;aa-aa-aa-aa-aa-aa<br>往二级索表 test:phone_mac里面写入数据  rowkey=&gt;aa-aa-aa-aa-aa-aa version=&gt;1736188717 value=&gt;aa-aa-aa-aa-aa-aa</p>
<p><img src="/medias/Hbase%E5%85%B3%E8%81%94.PNG" alt="Hbase关联"></p>
<p>查询不直接查主关联表，因为查询字段不在主键里面，没办法查或者性能非常低下。</p>
<p>查询是分2步rowkey查询<br>第一步， 通过查询字段取对应的二级索引表里面去找主关联表的ROWKEY<br>第二步， 通过主关联表的ROWKEY 获取HBASE中的全量数据</p>
<p> WIFI 已经入库的情况下，手机号也必须已经入库了，才能找到<br> 加入WIFI的手机号还没有入库</p>
<p>如果是基础数据先过来   没有mac 没有主键</p>
<table>
<thead>
<tr>
<th align="center">Card</th>
<th align="center">phone</th>
</tr>
</thead>
<tbody><tr>
<td align="center">400000000000000</td>
<td align="center">18612345678</td>
</tr>
</tbody></table>
<p>关联</p>
<table>
<thead>
<tr>
<th align="center">Phone</th>
<th align="center">value （识别这个字段是身份证才可以）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">18612345678</td>
<td align="center">400000000000000</td>
</tr>
</tbody></table>
<p>1）因为检索的时候都是通过索引表直接找MAC，混入了身份证<br>2）要进行一个合并</p>
<p><strong>（6）关联及二级索引示意</strong></p>
<p><img src="/medias/%E5%85%B3%E8%81%94%E5%8F%8A%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E7%A4%BA%E6%84%8F.PNG" alt="关联及二级索引示意"></p>
<p><img src="/medias/Hbase%E5%85%B3%E8%81%94%E8%A1%A8%E7%A4%BA%E6%84%8F%E5%9B%BE.PNG" alt="Hbase关联表示意图"></p>
<p><strong>（7）如果使用ES建立二级索引</strong></p>
<p><img src="/medias/%E4%BD%BF%E7%94%A8ES%E5%BB%BA%E7%AB%8B%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95.PNG" alt="使用ES建立二级索引"></p>
<p>如果hbase 里面有100个字段，存放的是全量信息，但是只有20个字段参与查询、检索，那么我们可以把这个20个字段单独提出来存放到es中，因为ES是对对字段，多条件查询非常灵活。所以我们可以先在ES中对条件进行检索，根据检索的结果拿到hbaSe的rowkey，然后再通过rowkey到hbase里面获取全量信息。      </p>
<p><strong>（8）Hbase 预分区</strong><br>主要是根据rowkey分布来进行预分区</p>
<p>分区主要是为了防止热点问题</p>
<p>relation表为例<br>这个表的rowkey 是不是就是 mac</p>
<p>phone_mac 都是以0-9  a-f开头的<br>device_mac 都是以0-9  a-z开头的<br>Hbase 是按字典序排序</p>
<p><strong>（9）自定义版本号</strong><br>通过这样的一个转换我们可以精确定位数据的多版本号，，然后可以根据版本号对数据进行多版本删除。<br>156511 aaaaaaaa</p>
<h4 id="2、DataRelationStreaming—数据关联"><a href="#2、DataRelationStreaming—数据关联" class="headerlink" title="2、DataRelationStreaming—数据关联"></a>2、DataRelationStreaming—数据关联</h4><p><strong>DataRelationStreaming.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka.kafka2hbase

import java.util.Properties

import com.hsiehchou.common.config.ConfigUtil
import com.hsiehchou.hbase.config.HBaseTableUtil
import com.hsiehchou.hbase.insert.HBaseInsertHelper
import com.hsiehchou.hbase.spilt.SpiltRegionUtil
import com.hsiehchou.spark.common.SparkContextFactory
import com.hsiehchou.spark.streaming.kafka.Spark_Kafka_ConfigUtil
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.kafka.KafkaManager

object DataRelationStreaming extends Serializable with Logging{

  // 读取需要关联的配置文件字段
  // phone_mac,phone,username,send_mail,imei,imsi
  val relationFields = ConfigUtil.getInstance()
    .getProperties(&quot;spark/relation.properties&quot;)
    .get(&quot;relationfield&quot;)
    .toString
    .split(&quot;,&quot;)
  def main(args: Array[String]): Unit = {

    //初始化hbase表
    //initRelationHbaseTable(relationFields)

    val ssc = SparkContextFactory.newSparkLocalStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10),1)
    //  val ssc = SparkContextFactory.newSparkStreamingContext(&quot;DataRelationStreaming&quot;, java.lang.Long.valueOf(10))

    val kafkaConfig: Properties = ConfigUtil.getInstance().getProperties(&quot;kafka/kafka-server-config.properties&quot;)
    val topics = &quot;chl_test7&quot;.split(&quot;,&quot;)
    val kafkaDS = new KafkaManager(Spark_Kafka_ConfigUtil
      .getKafkaParam(kafkaConfig.getProperty(&quot;metadata.broker.list&quot;),
        &quot;DataRelationStreaming2&quot;))
      .createJsonToJMapStringDirectStreamWithOffset(ssc, topics.toSet)
      .persist(StorageLevel.MEMORY_AND_DISK)

    kafkaDS.foreachRDD(rdd=&gt;{

      rdd.foreachPartition(partion=&gt;{
        //对partion进行遍历
        while (partion.hasNext){

          //获取每一条流数据
          val map = partion.next()

          //获取mac 主键
          var phone_mac:String = map.get(&quot;phone_mac&quot;)

          //获取所有关联字段 //phone_mac,phone,username,send_mail,imei,imsi
          relationFields.foreach(relationFeild =&gt;{
            //relationFields 是关联字段，需要进行关联处理的，所有判断
            //map中是不是包含这个字段，如果包含的话，取出来进行处理
            if(map.containsKey(relationFeild)){
              //创建主关联，并遍历关联字段进行关联
              val put = new Put(phone_mac.getBytes())

              //取关联字段的值
              //TODO  到这里  主关联表的 主键和值都有了  然后封装成PUT写入hbase主关联表就行了
              val value = map.get(relationFeild)

              //自定义版本号  通过 (表字段名 + 字段值 取hashCOde)
              //因为值有可能是字符串，但是版本号必须是long类型，所以这里我们需要
              //将字符串影射唯一数字，而且必须是正整数
              val versionNum = (relationFeild+value).hashCode() &amp; Integer.MAX_VALUE
              put.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(relationFeild),versionNum ,Bytes.toBytes(value.toString))
              HBaseInsertHelper.put(&quot;test:relation&quot;,put)
              println(s&quot;往主关联表 test:relation 里面写入数据  rowkey=&gt;${phone_mac} version=&gt;${versionNum} 类型${relationFeild} value=&gt;${value}&quot;)

              // 建立二级索引
              // 使用关联字段的值最为二级索引的rowkey
              // 二级索引就是把这个字段的值作为索引表rowkey
              // 把这个字段的mac做为索引表的值
              val put_2 = new Put(value.getBytes())//把这个字段的值作为索引表rowkey
              val table_name = s&quot;test:${relationFeild}&quot;//往索引表里面取写
              //使用主表的rowkey  就是 取hash作为二级索引的版本号
              val versionNum_2 = phone_mac.hashCode() &amp; Integer.MAX_VALUE
              put_2.addColumn(&quot;cf&quot;.getBytes(), Bytes.toBytes(&quot;phone_mac&quot;),versionNum_2 ,Bytes.toBytes(phone_mac.toString))
              HBaseInsertHelper.put(table_name,put_2)
              println(s&quot;往二级索表 ${table_name}里面写入数据  rowkey=&gt;${value} version=&gt;${versionNum_2} value=&gt;${phone_mac}&quot;)
            }
          })
        }
      })
    })
    ssc.start()
    ssc.awaitTermination()
  }

  def initRelationHbaseTable(relationFields:Array[String]): Unit ={
    //初始化总关联表
    val relation_table = &quot;test:relation&quot;
    HBaseTableUtil.createTable(relation_table,
      &quot;cf&quot;,
      true,
      -1,
      100,
      SpiltRegionUtil.getSplitKeysBydinct)
    //HBaseTableUtil.deleteTable(relation_table)

    //遍历所有关联字段，根据字段创建二级索引表
    relationFields.foreach(field=&gt;{
      val hbase_table = s&quot;test:${field}&quot;
      HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 100, SpiltRegionUtil.getSplitKeysBydinct)
      // HBaseTableUtil.deleteTable(hbase_table)
    })
  }
}</code></pre><h4 id="3、com-hsiehchou-spark-streaming"><a href="#3、com-hsiehchou-spark-streaming" class="headerlink" title="3、com.hsiehchou.spark.streaming"></a>3、com.hsiehchou.spark.streaming</h4><p><strong>common/SparkContextFactory.scala</strong></p>
<pre><code>package com.hsiehchou.spark.common

import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{Accumulator, SparkContext}

object SparkContextFactory {

  def newSparkBatchContext(appName:String = &quot;sparkBatch&quot;) : SparkContext = {
    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)
    new SparkContext(sparkConf)
  }

  def newSparkLocalBatchContext(appName:String = &quot;sparkLocalBatch&quot; , threads : Int = 2) : SparkContext = {
    val sparkConf = SparkConfFactory.newSparkLoalConf(appName, threads)
    sparkConf.set(&quot;&quot;,&quot;&quot;)
    new SparkContext(sparkConf)
  }

  def getAccumulator(appName:String = &quot;sparkBatch&quot;) : Accumulator[Int] = {
    val sparkConf = SparkConfFactory.newSparkBatchConf(appName)
    val accumulator: Accumulator[Int] = new SparkContext(sparkConf).accumulator(0,&quot;&quot;)
    accumulator
  }

  /**
    * 创建本地流streamingContext
    * @param appName             appName
    * @param batchInterval      多少秒读取一次
    * @param threads            开启多少个线程
    * @return
    */
  def newSparkLocalStreamingContext(appName:String = &quot;sparkStreaming&quot; ,
                                    batchInterval:Long = 30L ,
                                    threads : Int = 4) : StreamingContext = {
    val sparkConf =  SparkConfFactory.newSparkLocalConf(appName, threads)
    // sparkConf.set(&quot;spark.streaming.receiver.maxRate&quot;,&quot;10000&quot;)
    sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;,&quot;1&quot;)
    new StreamingContext(sparkConf, Seconds(batchInterval))
  }

  /**
    * 创建集群模式streamingContext
    * 这里不设置线程数，在submit中指定
    * @param appName
    * @param batchInterval
    * @return
    */
  def newSparkStreamingContext(appName:String = &quot;sparkStreaming&quot; , batchInterval:Long = 30L) : StreamingContext = {
    val sparkConf = SparkConfFactory.newSparkStreamingConf(appName)
    new StreamingContext(sparkConf, Seconds(batchInterval))
  }

  def startSparkStreaming(ssc:StreamingContext){
    ssc.start()
      ssc.awaitTermination()
      ssc.stop()
  }
}</code></pre><p><strong>streaming/kafka/Spark_Kafka_ConfigUtil.scala</strong></p>
<pre><code>package com.hsiehchou.spark.streaming.kafka

import org.apache.spark.Logging

object Spark_Kafka_ConfigUtil extends Serializable with Logging{

  def getKafkaParam(brokerList:String,groupId : String): Map[String,String]={
    val kafkaParam=Map[String,String](
      &quot;metadata.broker.list&quot; -&gt; brokerList,
      &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot;,
      &quot;group.id&quot; -&gt; groupId,
      &quot;refresh.leader.backoff.ms&quot; -&gt; &quot;1000&quot;,
      &quot;num.consumer.fetchers&quot; -&gt; &quot;8&quot;)
    kafkaParam
  }
}</code></pre><h4 id="4、com-hsiehchou-common-config-ConfigUtil"><a href="#4、com-hsiehchou-common-config-ConfigUtil" class="headerlink" title="4、com/hsiehchou/common/config/ConfigUtil"></a>4、com/hsiehchou/common/config/ConfigUtil</h4><p><strong>ConfigUtil.java</strong></p>
<pre><code>package com.hsiehchou.common.config;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

public class ConfigUtil {

    private static Logger LOG = LoggerFactory.getLogger(ConfigUtil.class);

    private static ConfigUtil configUtil;

    public static ConfigUtil getInstance(){

        if(configUtil == null){
            configUtil = new ConfigUtil();
        }
        return configUtil;
    }

    public Properties getProperties(String path){
        Properties properties = new Properties();
        try {
            LOG.info(&quot;开始加载配置文件&quot; + path);
            InputStream insss = this.getClass().getClassLoader().getResourceAsStream(path);
            properties = new Properties();
            properties.load(insss);
        } catch (IOException e) {
            LOG.info(&quot;加载配置文件&quot; + path + &quot;失败&quot;);
            LOG.error(null,e);
        }

        LOG.info(&quot;加载配置文件&quot; + path + &quot;成功&quot;);
        System.out.println(&quot;文件内容：&quot;+properties);
        return properties;
    }

    public static void main(String[] args) {
        ConfigUtil instance = ConfigUtil.getInstance();
        Properties properties = instance.getProperties(&quot;common/datatype.properties&quot;);
        //Properties properties = instance.getProperties(&quot;spark/relation.properties&quot;);

       // properties.get(&quot;relationfield&quot;);
        System.out.println(properties);
    }
}</code></pre><h4 id="5、构建模块—xz-bigdata-hbase"><a href="#5、构建模块—xz-bigdata-hbase" class="headerlink" title="5、构建模块—xz_bigdata_hbase"></a>5、构建模块—xz_bigdata_hbase</h4><p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata2&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_hbase&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
        &lt;hbase.version&gt;1.2.0&lt;/hbase.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_resources&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;guava&lt;/artifactId&gt;
                    &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
                &lt;/exclusion&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;
                    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;
            &lt;version&gt;${hbase.version}-${cdh.version}&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;servlet-api-2.5&lt;/artifactId&gt;
                    &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

&lt;/project&gt;</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseConf.java</strong></p>
<pre><code>package com.hsiehchou.hbase.config;

import com.hsiehchou.hbase.spilt.SpiltRegionUtil;
import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.BufferedMutator;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.log4j.Logger;

import java.io.IOException;
import java.io.Serializable;

public class HBaseConf implements Serializable {

    private static final long serialVersionUID = 1L;
    private static final Logger LOG = Logger.getLogger(HBaseConf.class);

    private static final String HBASE_SERVER_CONFIG = &quot;hbase/hbase-server-config.properties&quot;;
    private static final String HBASE_SITE = &quot;hbase/hbase-site.xml&quot;;

    private volatile static HBaseConf hbaseConf;
    private CompositeConfiguration hbase_server_config;

    public CompositeConfiguration getHbase_server_config() {

        return hbase_server_config;
    }

    public void setHbase_server_config(CompositeConfiguration hbase_server_config) {
        this.hbase_server_config = hbase_server_config;
    }

    //hbase 配置文件
    private  Configuration configuration;
    //hbase 连接
    private volatile transient Connection conn;

    /**
     * 初始化HBaseConf的时候加载配置文件
     */
    private HBaseConf() {
        hbase_server_config = new CompositeConfiguration();
        //加载配置文件
        loadConfig(HBASE_SERVER_CONFIG,hbase_server_config);
        //初始化连接
        getHconnection();
    }

    //获取连接
    public Configuration getConfiguration(){
        if(configuration==null){
            configuration = HBaseConfiguration.create();
            configuration.addResource(HBASE_SITE);
            LOG.info(&quot;加载配置文件&quot; + HBASE_SITE + &quot;成功&quot;);
        }
        return configuration;
    }

    public BufferedMutator getBufferedMutator(String tableName) throws IOException {
        return getHconnection().getBufferedMutator(TableName.valueOf(tableName));
    }

    public Connection getHconnection(){

        if(conn==null){
            //获取配置文件
            getConfiguration();
            synchronized (HBaseConf.class) {
                if (conn == null) {
                    try {
                        conn = ConnectionFactory.createConnection(configuration);
                    } catch (IOException e) {
                        LOG.error(String.format(&quot;获取hbase的连接失败  参数为： %s&quot;, toString()), e);
                    }
                }
            }
        }
        return conn;
    }

    /**
     * 加载配置文件
     * @param path
     * @param configuration
     */
    private void loadConfig(String path,CompositeConfiguration configuration) {
        try {
            LOG.info(&quot;加载配置文件 &quot; + path);
            configuration.addConfiguration(new PropertiesConfiguration(path));
            LOG.info(&quot;加载配置文件&quot; + path +&quot;成功。 &quot;);
        } catch (ConfigurationException e) {
            LOG.error(&quot;加载配置文件 &quot; + path + &quot;失败&quot;, e);
        }
    }

    /**
     * 单例 初始化HBaseConf
     * @return
     */
    public static HBaseConf getInstance() {
        if (hbaseConf == null) {
            synchronized (HBaseConf.class) {
                if (hbaseConf == null) {
                    hbaseConf = new HBaseConf();
                }
            }
        }
        return hbaseConf;
    }

    public static void main(String[] args) {
        String hbase_table = &quot;test:chl_test2&quot;;
        HBaseTableUtil.createTable(hbase_table, &quot;cf&quot;, true, -1, 1, SpiltRegionUtil.getSplitKeysBydinct());

      /*  Connection hconnection = HBaseConf.getInstance().getHconnection();
        Connection hconnection1 = HBaseConf.getInstance().getHconnection();
        System.out.println(hconnection);
        System.out.println(hconnection1);*/
    }
}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableFactory.java</strong></p>
<pre><code>package com.hsiehchou.hbase.config;

import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.BufferedMutator;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Table;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.Serializable;

public class HBaseTableFactory implements Serializable {

    private static final long serialVersionUID = -1071596337076137201L;

    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableFactory.class);

    private HBaseConf conf;
    private transient Connection conn  ;
    private boolean isReady = true;

    public HBaseTableFactory(){

        conf = HBaseConf.getInstance();
        if(true){
            conn = conf.getHconnection();
        }else{
            isReady = false;
            LOG.warn(&quot;HBase 连接没有启动。&quot;);
        }
    }

    public HBaseTableFactory(Connection conn){
        this.conn = conn;
    }

    /**
      * 根据表名创建 表的实例
      * @param tableName
      * @return
      * @throws IOException
      * HTableInterface
     */
    public Table getHBaseTableInstance(String tableName) throws IOException{

        if(conn == null){
            if(conf == null){
                conf = HBaseConf.getInstance();
                isReady = true;
                LOG.warn(&quot;HBaseConf为空，重新初始化。&quot;);
            }
            synchronized (HBaseTableFactory.class) {
                if(conn == null) {
                    conn = conf.getHconnection();
                    LOG.warn(&quot;初始 hbase Connection 为空 ， 获取  Connection成功。&quot;);
                }
            }
        }
        return  isReady ? conn.getTable(TableName.valueOf(tableName)) : null;
    }

    public HTable getHTable(String tableName) throws IOException{

        return  (HTable) getHBaseTableInstance(tableName);
    }

    public BufferedMutator getBufferedMutator(String tableName) throws IOException {
        return getConf().getBufferedMutator(tableName);
    }

    public boolean isReady() {
        return isReady;
    }

    private HBaseConf getConf(){
        if(conf == null){
            conf = HBaseConf.getInstance();
        }
        return conf;
    }

    public void close() throws IOException{
        conn.close();
        conn = null;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/config/HBaseTableUtil</strong></p>
<pre><code>package com.hsiehchou.hbase.config;

import com.google.common.collect.Sets;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.io.compress.Compression;
import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
import org.apache.hadoop.hbase.regionserver.BloomType;
import org.apache.hadoop.hbase.util.Bytes;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.*;

import static com.google.common.base.Preconditions.checkArgument;

public class HBaseTableUtil {

    private static final Logger LOG = LoggerFactory.getLogger(HBaseTableUtil.class);
    private static final String COPROCESSORCLASSNAME =  &quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;;
    private static HBaseConf conf = HBaseConf.getInstance() ;

    private HBaseTableUtil(){}

    /**
     * 获取hbase 表连接
     * @param tableName
     * @return
     */
    public static Table getTable(String tableName){
        Table table =null;
        if(tableExists(tableName)){
            try {
                table = conf.getHconnection().getTable(TableName.valueOf(tableName));
            } catch (IOException e) {
                LOG.error(null,e);
            }
        }
        return table;
    }

    public static void close(Table table){
        if(table != null) {
            try {
                table.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * 判断   HBase中是否存在  名为  tableName 的表
     * @param tableName
     * @return  boolean
     */
    public static boolean tableExists(String tableName){

        boolean  isExists = false;
        try {
            isExists = conf.getHconnection().getAdmin().tableExists(TableName.valueOf(tableName));
        } catch (MasterNotRunningException e) {
            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);
        } catch (ZooKeeperConnectionException e) {
            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);
        } catch (IOException e) {
            LOG.error(&quot;&quot;, e);
        }
        return isExists;
    }

    /**
     * 删除表
     * @param tableName
     * @return
     */
    public static boolean deleteTable(String tableName){

        boolean status = false;
        TableName name = TableName.valueOf(tableName);
        try {
            Admin admin = conf.getHconnection().getAdmin();
            if(admin.tableExists(name)){
                if(!admin.isTableDisabled(name)){
                    admin.disableTable(name);
                }
                admin.deleteTable(name);
            }else{
                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);
            }
            admin.close();
            status = true;
        } catch (MasterNotRunningException e) {
            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);
        } catch (ZooKeeperConnectionException e) {
            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);
        } catch (IOException e) {
            LOG.error(&quot;&quot;, e);
        }
        return status;
    }

    /**
     * 清空表
     * @param tableName
     * @return
     */
    public static boolean truncateTable(String tableName){

        boolean status = false;
        TableName name = TableName.valueOf(tableName);

        try {
            Admin admin = conf.getHconnection().getAdmin();
            if(admin.tableExists(name)){
                if(admin.isTableAvailable(name)){
                    admin.disableTable(name);
                }
                admin.truncateTable(name, true);
            }else{
                LOG.warn(&quot; HBase中不存在 表 &quot; + tableName);
            }
            admin.close();
            status = true;
        } catch (MasterNotRunningException e) {
            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);
        } catch (ZooKeeperConnectionException e) {
            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);
        } catch (IOException e) {
            LOG.error(&quot;&quot;, e);
        }
        return status;
    }

    /**
     * 创建HBase表
     * @param tableName
     * @param cf       列族名
     * @param inMemory
     * @param ttl    ttl &lt; 0     则为永久保存
     */
    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion){

        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);

        return createTable(htd);
    }

    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY){

        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY , COPROCESSORCLASSNAME);

        return createTable(htd);
    }

    public static boolean createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion,  boolean useSNAPPY, byte[][] splits){

        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, COPROCESSORCLASSNAME);
        return createTable(htd , splits);

    }

    /**
     * @param tableName    表名
     * @param cf           列簇
     * @param inMemory     是否存在内存
     * @param ttl          数据过期时间
     * @param maxVersion   最大版本
     * @param splits       分区
     * @return
     */
    public static boolean createTable(String tableName,
                                      String cf,
                                      boolean inMemory,
                                      int ttl,
                                      int maxVersion,
                                      byte[][] splits){
        //返回表说明
        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, COPROCESSORCLASSNAME);
        //通过HTableDescriptor 和 splits 分区策略来定义表
        return createTable(htd , splits);
    }

    public static List&lt;String&gt; listTables(){

        List&lt;String&gt; list = new ArrayList&lt;String&gt;();
        Admin admin = null;

        try {
            admin = conf.getHconnection().getAdmin();
            TableName[] listTableNames = admin.listTableNames();
            for( TableName t :  listTableNames ){
                list.add( t.getNameAsString() );
            }
        } catch(IOException e )  {
            LOG.error(&quot;创建HBase表失败。&quot;, e);
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return list;
    }

    /**
     * 列出所有表
     * @param reg
     * @return
     */
    public static List&lt;String&gt; listTables(String reg){
        List&lt;String&gt; list = new ArrayList&lt;String&gt;();
        Admin admin = null;

        try {
            admin = conf.getHconnection().getAdmin();
            TableName[] listTableNames = admin.listTableNames(reg);
            for(TableName t :  listTableNames){
                list.add(t.getNameAsString());
            }
        } catch(IOException e)  {
            LOG.error(&quot;创建HBase表失败。&quot;, e);
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return list;
    }

    /**
     * 创建HBase表
     * @param tableName
     * @param cf       列族名
     * @param inMemory
     * @param ttl      ttl &lt; 0     则为永久保存
     */
    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl , int maxVersion, String ... coprocessorClassNames){
        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);
        return createTable(htd);
    }

    public static boolean  createTable( String tableName, String cf, boolean inMemory, int ttl, int maxVersion, boolean useSNAPPY, String ... coprocessorClassNames){
        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY, coprocessorClassNames);
        return createTable(htd);
    }

    public static boolean  createTable( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,  boolean useSNAPPY ,byte[][] splits, String ... coprocessorClassNames){
        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, useSNAPPY ,coprocessorClassNames);
        return createTable(htd,splits );
    }
    public static boolean  createTable(String tableName, String cf, boolean inMemory, int ttl, int maxVersion, byte[][] splits, String ... coprocessorClassNames){

        HTableDescriptor htd = createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, coprocessorClassNames);
        return createTable(htd,splits );
    }

    /**
     * 通过HTableDescriptor 和 分区 来构建hbase
     * @param htd
     * @param splits
     * @return
     */
    public static boolean createTable(HTableDescriptor htd, byte[][] splits){
        Admin admin = null;
        try {
            admin = conf.getHconnection().getAdmin();
            TableName tableName = htd.getTableName();
            boolean exist = admin.tableExists(tableName);
            if(exist){
                LOG.error(&quot;表&quot;+tableName.getNameAsString() + &quot;已经存在&quot;);
            }else{
                //使用Admin进行创建表
                admin.createTable(htd, splits);
            }
        } catch(IOException e )  {
            LOG.error(&quot;创建HBase表失败。&quot;, e);
            return false;
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return true;
    }

    public static boolean createTable(HTableDescriptor htd){
        Admin admin = null;
        try {
            admin = conf.getHconnection().getAdmin();
            if(admin.tableExists(htd.getTableName())){
                LOG.info(&quot;表&quot; + htd.getTableName() + &quot;已经存在&quot;);
            }else{
                admin.createTable(htd);
            }
        } catch(IOException e )  {
            LOG.error(&quot;创建HBase表失败。&quot;, e);
            return false;
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return true;
    }

    /**
     * 创建命名空间
     * @param nameSpace
     * @return
     */
    public static boolean createNameSpace(String nameSpace){

        Admin admin = null;
        try {
            admin = conf.getHconnection().getAdmin();
            NamespaceDescriptor[] listNamespaceDescriptors = admin.listNamespaceDescriptors();
            boolean exist = false;
            for(NamespaceDescriptor namespaceDescriptor : listNamespaceDescriptors){
                if(namespaceDescriptor.getName().equals(nameSpace)){
                    exist = true;
                }
            }
            if(!exist) admin.createNamespace(NamespaceDescriptor.create(nameSpace).build());
        } catch(IOException e )  {
            LOG.error(&quot;创建HBase命名空间失败。&quot;, e);
            return false;
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return true;
    }

    /**
     * 为 HBase中的表  tableName添加 协处理器  coprocessorClassName
     * @param tableName
     * @param coprocessorClassName    必须是已经存在与HBase集群中
     * @return  boolean
     */
    public static boolean addCoprocessorClassForTable(String tableName,String coprocessorClassName){

        boolean status = false;
        TableName name = TableName.valueOf(tableName);
        Admin admin = null;
        try {
            admin = conf.getHconnection().getAdmin();
            HTableDescriptor htd = admin.getTableDescriptor(name);
            if(!htd.hasCoprocessor(coprocessorClassName)){

                htd.addCoprocessor(coprocessorClassName);

                admin.disableTable(name);
                admin.modifyTable(name, htd);
                admin.enableTable(name);
            }else{
                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));
            }
            status = true;
        } catch (MasterNotRunningException e) {
            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);
        } catch (ZooKeeperConnectionException e) {
            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);
        } catch (IOException e) {
            LOG.error(&quot;&quot;, e);
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return status;
    }

    /**
     * 为HBase中的表 tableName添加指定位置的 协处理器 jar
     * @param tableName
     * @param coprocessorClassName   jar中的具体的协处理器
     * @param jarPath     hdfs的路径
     * @param level       执行级别
     * @param kvs         运行参数    可以为 null
     * @return   boolean
     */
    public static boolean addCoprocessorJarForTable(String  tableName, String coprocessorClassName,String jarPath,int level ,Map&lt;String, String&gt; kvs ){
        boolean status = false;
        TableName name = TableName.valueOf(tableName);
        Admin admin = null;
        try {
            admin = conf.getHconnection().getAdmin();
            HTableDescriptor htd = admin.getTableDescriptor(name);
            if(!htd.hasCoprocessor(coprocessorClassName)){
                admin.disableTable(name);
                htd.addCoprocessor(coprocessorClassName, new Path(jarPath), level, kvs);
                admin.modifyTable(name, htd);
                admin.enableTable(name);
            }else{
                LOG.warn(String.format(&quot;表 %s中已经存在协处理器%s&quot;, tableName, coprocessorClassName));
            }
            status = true;
        } catch (MasterNotRunningException e) {
            LOG.error(&quot;HBase  master  未运行 。 &quot;, e);
        } catch (ZooKeeperConnectionException e) {
            LOG.error(&quot;zooKeeper 连接异常。 &quot;, e);
        } catch (IOException e) {
            LOG.error(&quot;&quot;, e);
        }finally{
            try {
                if(admin!=null){
                    admin.close();
                }
            } catch (IOException e) {
                LOG.error(&quot;&quot;, e);
            }
        }
        return status;
    }

    /**
     * @param tableName
     * @param cf
     * @param inMemory
     * @param ttl
     * @param maxVersion
     * @param coprocessorClassNames
     * @return
     */
    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion ,String ... coprocessorClassNames ){
        return createHTableDescriptor(tableName, cf, inMemory, ttl, maxVersion, true , COPROCESSORCLASSNAME);
    }

    /**
     * @param tableName
     * @param cf
     * @param inMemory
     * @param ttl
     * @param maxVersion
     * @param useSNAPPY
     * @param coprocessorClassNames
     * @return
     */
    public static HTableDescriptor createHTableDescriptor( String tableName,String cf,boolean inMemory, int ttl ,int maxVersion , boolean useSNAPPY , String ... coprocessorClassNames ){

        // 1.创建命名空间
        String[] split = tableName.split(&quot;:&quot;);
        if(split.length==2){
            createNameSpace(split[0]);
        }

        // 2.添加协处理器
        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));
        for( String coprocessorClassName : coprocessorClassNames ){

            try {
                htd.addCoprocessor(coprocessorClassName);
            } catch (IOException e1) {
                LOG.error(&quot;为表&quot; + tableName + &quot; 添加协处理器失败。 &quot;, e1);
            }
        }

        // 创建HColumnDescriptor
        HColumnDescriptor hcd = new HColumnDescriptor(cf);
        if( maxVersion &gt; 0 )
            //定义最大版本号
            hcd.setMaxVersions(maxVersion);

        /**
         * 设置布隆过滤器
         * 默认是NONE 是否使用布隆过虑及使用何种方式
         * 布隆过滤可以每列族单独启用
         * Default = ROW 对行进行布隆过滤。
         * 对 ROW，行键的哈希在每次插入行时将被添加到布隆。
         * 对 ROWCOL，行键 + 列族 + 列族修饰的哈希将在每次插入行时添加到布隆
         * 使用方法: create ‘table’,{BLOOMFILTER =&gt;’ROW’}
         * 启用布隆过滤可以节省读磁盘过程，可以有助于降低读取延迟
         * */
        hcd.setBloomFilterType(BloomType.ROWCOL);

        /**
         * hbase在LRU缓存基础之上采用了分层设计，整个blockcache分成了三个部分，分别是single、multi和inMemory。三者区别如下：
         * single：如果一个block第一次被访问，放在该优先队列中；
         * multi：如果一个block被多次访问，则从single队列转移到multi队列
         * inMemory：优先级最高，常驻cache，因此一般只有hbase系统的元数据，如meta表之类的才会放到inMemory队列中。普通的hbase列族也可以指定IN_MEMORY属性，方法如下：
         * create &#39;table&#39;, {NAME =&gt; &#39;f&#39;, IN_MEMORY =&gt; true}
         * 修改上表的inmemory属性，方法如下：
         * alter &#39;table&#39;,{NAME=&gt;&#39;f&#39;,IN_MEMORY=&gt;true}
         * */
        hcd.setInMemory(inMemory);
        hcd.setScope(1);

        /**
         * 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，
         * 哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，
         * 性价比最高。
         **/
        if(useSNAPPY)hcd.setCompressionType(Compression.Algorithm.SNAPPY);

        //默认为NONE
        //如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率
        //如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。
        //在任何情况下开启PREFIX_TREE编码都是安全的
        //不要同时开启PREFIX_TREE和SNAPPY
        //通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果
        //hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);

        //默认为64k     65536
        //随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，
        //64k大小比16k大小的吞吐量大约下降13%，延迟增大13%
        //128k大小比64k大小的吞吐量大约下降22%，延迟增大27%
        //对于随机读取为主的业务，可以考虑调低blocksize的大小

        //随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，
        //64k大小比16k大小的吞吐量大约增加33%，延迟降低24%
        //128k大小比64k大小的吞吐量大约增加7%，延迟降低7%
        //对于scan为主的业务，可以考虑调大blocksize的大小

        //如果业务请求以Get为主，则可以适当的减小blocksize的大小
        //如果业务是以scan请求为主，则可以适当的增大blocksize的大小
        //系统默认为64k, 是一个scan和get之间取的平衡值
        //hcd.setBlocksize(s)

        //设置表中数据的存储生命期，过期数据将自动被删除，
        // 例如如果只需要存储最近两天的数据，
        // 那么可以设置setTimeToLive(2 * 24 * 60 * 60)
        if( ttl &lt; 0 ) ttl = HConstants.FOREVER;
        hcd.setTimeToLive(ttl);

        htd.addFamily( hcd);

        return htd;
    }

    public static boolean createTable(HBaseTableParam param){

        String nameSpace = param.getNameSpace();
        if(!&quot;default&quot;.equalsIgnoreCase(nameSpace)){
            checkArgument(createNameSpace(nameSpace), String.format(&quot;创建命名空间%s失败。&quot;, nameSpace));
        }

        HTableDescriptor desc = createHTableDescriptor(param);
        byte[][] splits = param.getSplits();
        if(splits == null){
            return createTable(desc);
        }else{
            return createTable(desc, splits);
        }

    }

    public static HTableDescriptor createHTableDescriptor(HBaseTableParam param){

        String tableName = String.format(&quot;%s:%s&quot;, param.getNameSpace(), param.getTableName());
        HTableDescriptor htd = new HTableDescriptor(TableName.valueOf(tableName));

        for(String coprocessorClassName : param.getCoprocessorClazz()){
            try {
                htd.addCoprocessor(coprocessorClassName);
            } catch (IOException e) {
                LOG.error(String.format(&quot;为表  %s 添加协处理器失败。&quot;, tableName), e);
            }
        }

        HColumnDescriptor hcd = new HColumnDescriptor(param.getCf());
        hcd.setBloomFilterType(param.getBloomType());
        hcd.setMaxVersions(param.getMaxVersions());
        hcd.setScope(param.getReplicationScope());
        hcd.setBlocksize(param.getBlocksize());
        hcd.setInMemory(param.isInMemory());
        hcd.setTimeToLive(param.getTtl());

        /* 数据量大，边压边写也会提升性能的，毕竟IO是大数据的最严重的瓶颈，哪怕使用了SSD也是一样。众多的压缩方式中，推荐使用SNAPPY。从压缩率和压缩速度来看，性价比最高。  */
        if(param.isUsePrefix_tree())hcd.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);
        if(param.isUseSnappy())hcd.setCompressionType(Compression.Algorithm.SNAPPY);

        htd.addFamily( hcd);

        return htd;
    }

    public static void closeTable( Table table ){

        if( table != null ){
            try {
                table.close();
            } catch (IOException e) {
                LOG.error(&quot; &quot;, e);
            }
            table = null;
        }
    }

    public static byte[][] getSplitKeys() {
        //String[] keys = new String[]{&quot;50|&quot;};
        //String[] keys = new String[]{&quot;25|&quot;,&quot;50|&quot;,&quot;75|&quot;};
        //String[] keys = new String[]{&quot;13|&quot;,&quot;26|&quot;,&quot;39|&quot;, &quot;52|&quot;,&quot;65|&quot;,&quot;78|&quot;,&quot;90|&quot;};
        String[] keys = new String[]{ &quot;06|&quot;,&quot;13|&quot;,&quot;20|&quot;, &quot;26|&quot;,&quot;33|&quot;, &quot;39|&quot;,&quot;46|&quot;, &quot;52|&quot;,&quot;58|&quot;, &quot;65|&quot;,&quot;72|&quot;,&quot;78|&quot;, &quot;84|&quot;,&quot;90|&quot;,&quot;95|&quot;};
        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};
        byte[][] splitKeys = new byte[keys.length][];
        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序
        for (int i = 0; i &lt; keys.length; i++) {
            rows.add(Bytes.toBytes(keys[i]));
        }
        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();
        int i = 0;
        while (rowKeyIter.hasNext()) {
            byte[] tempRow = rowKeyIter.next();
            rowKeyIter.remove();
            splitKeys[i] = tempRow;
            i++;
        }
        return splitKeys;
    }

    public static class HBaseTableParam{

        private final String nameSpace; //命名空间
        private final String tableName; //表名
        private final String cf;        //列簇
        private Set&lt;String&gt;  coprocessorClazz = Sets.newHashSet(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;);
        private int maxVersions = 1;    //版本号 默认为1
        private BloomType bloomType = BloomType.ROWCOL;
        private boolean inMemory = false;
        private int replicationScope = 1;
        private boolean useSnappy = false; //默认不使用压缩
        private boolean usePrefix_tree = false;
        private int blocksize = 65536;
        private int ttl = HConstants.FOREVER;

        private byte[][] splits;

        public HBaseTableParam(String nameSpace, String tableName, String cf) {
            super();
            this.nameSpace = nameSpace == null ? &quot;default&quot; : nameSpace;
            this.tableName = tableName;
            this.cf = cf;
        }

        public String getNameSpace() {
            return nameSpace;
        }

        public String getTableName() {
            return tableName;
        }

        public String getCf() {
            return cf;
        }

        public Set&lt;String&gt; getCoprocessorClazz() {
            return coprocessorClazz;
        }

        public void clearCoprocessor(){
            coprocessorClazz.clear();
        }
        public void addCoprocessorClazz(String clazz) {
            this.coprocessorClazz.add(clazz);
        }

        public void addCoprocessorClazz(String ... clazz) {
            addCoprocessorClazz(Arrays.asList(clazz));
        }

        public void addCoprocessorClazz(Collection&lt;String&gt;  clazz) {
            this.coprocessorClazz.addAll(clazz);
        }

        public int getMaxVersions() {
            return maxVersions;
        }

        public void setMaxVersions(int maxVersions) {
            this.maxVersions = maxVersions &lt;= 0 ? 1 : maxVersions;
        }

        public BloomType getBloomType() {
            return bloomType;
        }

        public void setBloomType(BloomType bloomType) {
            this.bloomType = bloomType == null ? BloomType.ROWCOL : bloomType;
        }

        public boolean isInMemory() {
            return inMemory;
        }

        public void setInMemory(boolean inMemory) {
            this.inMemory = inMemory;
        }

        public int getReplicationScope() {
            return replicationScope;
        }

        public void setReplicationScope(int replicationScope) {
            this.replicationScope = replicationScope &lt; 0 ? 1 : replicationScope;
        }

        public boolean isUseSnappy() {
            return useSnappy;
        }

        /**
         * 控制是否使用 snappy 压缩数据， 默认是不启用
         * @param useSnappy
         */
        public void setUseSnappy(boolean useSnappy) {
            this.useSnappy = useSnappy;
        }

        public boolean isUsePrefix_tree() {
            return usePrefix_tree;
        }

        /**
         * 控制是否使用数据编码，默认是不使用
         *
         * 如果数据存储时设置了编码， 在缓存到内存中的时候是不会解码的，这样和不编码的情况相比，相同的数据块，编码后占用的内存更小， 即提高了内存的使用率
         * 如果设置了编码，用户必须在取数据的时候进行解码， 因此在内存充足的情况下会降低读写性能。
         * 在任何情况下开启PREFIX_TREE编码都是安全的
         * 不要同时开启PREFIX_TREE和SNAPPY
         * 通常情况下 SNAPPY并不能比 PREFIX_TREE取得更好的优化效果
         */
        public void setUsePrefix_tree(boolean usePrefix_tree) {
            this.usePrefix_tree = usePrefix_tree;
        }

        public int getBlocksize() {
            return blocksize;
        }

        /**
         *默认为64k     65536
         *随着blocksize的增大， 系统随机读的吞吐量不断的降低，延迟也不断的增大，
         *64k大小比16k大小的吞吐量大约下降13%，延迟增大13%
         *128k大小比64k大小的吞吐量大约下降22%，延迟增大27%
         *对于随机读取为主的业务，可以考虑调低blocksize的大小
         *
         *随着blocksize的增大， scan的吞吐量不断的增大，延迟也不断降低，
         *64k大小比16k大小的吞吐量大约增加33%，延迟降低24%
         *128k大小比64k大小的吞吐量大约增加7%，延迟降低7%
         *对于scan为主的业务，可以考虑调大blocksize的大小
         *
         *如果业务请求以Get为主，则可以适当的减小blocksize的大小
         *如果业务是以scan请求为主，则可以适当的增大blocksize的大小
         *系统默认为64k, 是一个scan和get之间取的平衡值
         *
         */
        public void setBlocksize(int blocksize) {
            this.blocksize = blocksize &lt;= 0 ? 65536 : blocksize;
        }

        public int getTtl() {
            return ttl;
        }

        /**
         * 默认是永久保存
         * @param ttl  大于 零的整数，  &lt;= 0 ? tt 为  永久保存
         */
        public void setTtl(int ttl) {
            this.ttl = ttl &lt;= 0 ? HConstants.FOREVER : ttl;
        }

        public byte[][] getSplits() {
            return splits;
        }

        /*
         * 预分区的rowKey范围配置
         * @param splits
         */
        /*
        public void setSplits(byte[][] splits) {
            this.splits = splits;
        }*/
    }

    public static void main(String[] args) throws Exception{
        Admin admin = conf.getHconnection().getAdmin();
        System.out.println(admin);
        //deleteTable(&quot;test:user&quot;);
        // HBaseTableUtil.createTable(&quot;aaaaa&quot;,&quot;info1&quot;,true,-1,1);
        //  HBaseTableUtil.truncateTable(&quot;aaaaa&quot;);
     /*   boolean b = tableExists(&quot;test:user2&quot;);
        Table table = getTable(&quot;test:user2&quot;);
        System.out.println(&quot;==================&quot;+table);
        System.out.println(&quot;==================&quot;+table.getName());*/

        //HBaseTableUtil.deleteTable(&quot;aaaaa&quot;);

       /* Table table = HBaseTableUtil.getTable(&quot;countform:typecount&quot;);
        System.out.println(table);*/
/*
        boolean b = HBaseTableUtil.tableExists(&quot;countform:typecount&quot;);
        System.out.println(b);*/

        HBaseTableUtil.deleteTable(&quot;tanslator&quot;);
        HBaseTableUtil.deleteTable(&quot;ability&quot;);
        HBaseTableUtil.deleteTable(&quot;task&quot;);
        HBaseTableUtil.deleteTable(&quot;paper&quot;);

        //  HbaseSearchService hbaseSearchService=new HbaseSearchService();
        //  Map&lt;String, String&gt; stringStringMap = hbaseSearchService.get(&quot;countform:bsid&quot;,&quot;&quot;, new BaseMapRowExtrator());
        // Map&lt;String, String&gt; aaaaa = hbaseSearchService.get(&quot;countform:bsid&quot;, &quot;aaaaa&quot;, new BaseMapRowExtrator());
        // System.out.println(aaaaa);
    }
}</code></pre><p><strong>com/hsiehchou/hbase/entity/AbstractRow.java</strong></p>
<pre><code>package com.hsiehchou.hbase.entity;

import com.google.common.collect.HashMultimap;
import com.google.common.collect.Sets;

import java.util.Collection;
import java.util.Map;
import java.util.Set;

public abstract class AbstractRow&lt;T extends HBaseCell&gt; {

    protected String rowKey;
    protected HashMultimap&lt;String, T&gt; cells;

    protected Set&lt;String&gt; fields;
    protected long maxCapTime;

    public AbstractRow(String rowKey){
        this.rowKey = rowKey;
        cells = HashMultimap.create();
        fields = Sets.newHashSet();
    }

    public boolean addCell(String field, String value, long capTime){

        return addCell(field, createCell(field, value, capTime));
    }

    public boolean addCell(String field, T cell){

        fields.add(cell.getField());

        if(cell.getCapTime() &gt; maxCapTime)
            maxCapTime = cell.getCapTime();

        return cells.put(field, cell);
    }

    public boolean[] addCell(String field, Collection&lt;T&gt; cells){

        boolean[] status = new boolean[cells.size()];
        int n = 0;
        for(T cell : cells){
            status[n] = addCell(field, cell);
            n++;
        }
        return status;
    }

    public String getRowKey() {
        return rowKey;
    }

    protected abstract T createCell(String field, String value, long capTime);

    public Map&lt;String, Collection&lt;T&gt;&gt; getCell() {
        return cells.asMap();
    }

    public Collection&lt;T&gt; getCellByField(String field){
        return cells.get(field);
    }

    public Set&lt;Map.Entry&lt;String, T&gt;&gt; entries(){
        return  cells.entries();
    }

    @Override
    public String toString() {
        return &quot;AbstractRow [rowKey=&quot; + rowKey + &quot;, cells=&quot; + cells + &quot;]&quot;;
    }

    public boolean equals(Object obj) {

       if(this == obj)return true ;
       if(!(obj instanceof AbstractRow))return false ;

       @SuppressWarnings(&quot;unchecked&quot;)
       AbstractRow&lt;T&gt; row = (AbstractRow&lt;T&gt;) obj;
       if(rowKey.equals(row.getRowKey()))return true;
       return false;
    }

    public int hashCode(){
        return this.rowKey.hashCode();
    }

    public long getMaxCapTime() {
        return maxCapTime;
    }

    public Set&lt;String&gt; getFields() {
        return Sets.newHashSet(fields);
    }
}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseCell.java</strong></p>
<pre><code>package com.hsiehchou.hbase.entity;

public class HBaseCell implements Comparable&lt;HBaseCell&gt;{

    protected String field;           
    protected String value;
    protected Long capTime;

    public HBaseCell(String field, String value, long capTime){

        this.field = field;
        this.capTime = capTime;
        this.value = value;
    }

    public String getField(){
        return field;
    }

    public String getValue(){
        return value;
    }

    public void setCapTime(long capTime) {
        this.capTime = capTime;
    }

    public Long getCapTime() {
        return capTime;
    }

    public String toString(){
        return String.format(&quot;%s_[%s]_%s&quot;, field, capTime, value);
    }

    public int compareTo(HBaseCell o) {
        return o.getCapTime().compareTo(this.capTime);
    }

    public boolean equals(Object obj) {

       if(this == obj)return true ;
       if(!(obj instanceof HBaseCell))return false ;

       HBaseCell cell = (HBaseCell)obj;
       if(field.equals(cell.getField()) &amp;&amp; value.equals(cell.getValue())){
           if(cell.getCapTime() &lt; capTime){
               cell.setCapTime(this.capTime);
           }
           return true;
       }
       return false;
    }

    public int hashCode(){
        return this.field.hashCode() +  31*this.value.hashCode();
    }

}</code></pre><p><strong>com/hsiehchou/hbase/entity/HBaseRow.java</strong></p>
<pre><code>package com.hsiehchou.hbase.entity;

public class HBaseRow extends AbstractRow&lt;HBaseCell&gt; {

    public HBaseRow(String rowKey){
        super(rowKey);
    }

    public boolean[] addCell(String field, HBaseCell ... cells){

        boolean[] status = new boolean[cells.length];
        for(int i = 0; i &lt; cells.length; i++){
            status[i] = addCell(field, cells[i]);
        }
        return status;
    }

    protected HBaseCell createCell(String field, String value, long capTime) {
        return new HBaseCell(field, value, capTime);
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseListRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class BaseListRowExtrator implements RowExtractor&lt;List&lt;String&gt;&gt;{

    private List&lt;String&gt; row;

    public Long lastcjtime = 0l;

    public Long firstcjtime = 0l;

    @Override
    public List&lt;String&gt; extractRowData(Result result, int rowNum)
            throws IOException {

        row = new ArrayList&lt;String&gt;();
        for(Cell cell :  result.listCells()) {
            String column = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());
            String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
            if(column.equalsIgnoreCase(&quot;cjtime&quot;)) {
                Long v = Long.parseLong(value);
                if(lastcjtime &lt; v) {
                    lastcjtime = v;
                }else if(firstcjtime &gt; v) {
                    firstcjtime = v;
                }
            }
            row.add(value);
        }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class BaseMapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {

    private Map&lt;String,String&gt; row;

    private List&lt;byte[]&gt; rows;
    private String longTimeField;
    private SimpleDateFormat format;

    private String field;
    private String value;

    private long time;

    public BaseMapRowExtrator(){}

    /**
     * @param rows   需要提取 所有的 rowKey  , null 则不提取
     */
    public BaseMapRowExtrator(List&lt;byte[]&gt; rows){
        this.rows = rows;
    }

    /**
     * @param rows             需要提取 所有的 rowKey  , null 则不提取
     * @param longTimeField    long类型的时间字段   表示需要将其转换称 String 类型
     */
    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField){
        this.rows = rows;
        this.longTimeField = longTimeField;
    }

    /**
     * @param rows                  需要提取 所有的 rowKey  , null 则不提取
     * @param longTimeField         long类型的时间字段
     * @param timePattern           表示需要已该指定的格式  将时间字段的值转换成字符串
     */
    public BaseMapRowExtrator(List&lt;byte[]&gt; rows,String longTimeField,String timePattern){
        this.rows = rows;
        this.longTimeField = longTimeField;
        if(StringUtils.isNotBlank(timePattern)){
            format = new SimpleDateFormat(timePattern);
        }
    }

    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {

            row = new HashMap&lt;String,String&gt;();

            List&lt;Cell&gt; cells = result.listCells();
            for(Cell cell :  cells) {
                field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());
                if( field.equals(longTimeField)  ){
                    time = Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
                    if( format != null ){
                        value = format.format(new Date(time));
                    }else{
                        value = String.valueOf(time);
                    }
                }else{
                    value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
                }
                row.put(field,value);
            }

            if( rows != null ){
                rows.add(result.getRow());
            }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BaseMapWithRowKeyExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

public class BaseMapWithRowKeyExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt; {

    private Map&lt;String,String&gt; row;

    /* (non-Javadoc)
     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)
     */
    @Override
    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)
            throws IOException {

        row = new HashMap&lt;String,String&gt;();
        row.put(&quot;rowKey&quot;, Bytes.toString( result.getRow() ));

        for(Cell cell :  result.listCells()) {
            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
        }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/BeanRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import com.google.common.collect.Maps;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.lang.reflect.Field;
import java.util.Map;

public class BeanRowExtrator&lt;T&gt; implements RowExtractor&lt;T&gt; {

    private static final Logger LOG = LoggerFactory.getLogger(BeanRowExtrator.class);

    private Class&lt;T&gt; clazz;
    private Map&lt;String,Field&gt; fieldMap;

    public BeanRowExtrator(Class&lt;T&gt; clazz){
        this.clazz = clazz;
        this.fieldMap = getDeclaredFields(clazz);
    }

    public T extractRowData(Result result, int rowNum) throws IOException {
        return resultReflectToClass(result, rowNum);
    }

    private T resultReflectToClass(Result result, int rowNum){
        String column = null;
        Field field = null;
        T obj = null;
        try {
            obj = clazz.newInstance();
            for(Cell cell : result.listCells()){
                column = Bytes.toString(cell.getQualifierArray(),
                        cell.getQualifierOffset(), cell.getQualifierLength());
                /*检查该列是否在实体类中存在对应的属性,若存在则 为其赋值*/
                if((field = fieldMap.get(column.toLowerCase())) != null){
                    field.set(obj, Bytes.toString(cell.getValueArray(),
                            cell.getValueOffset(), cell.getValueLength()));
                }
            }
        } catch (InstantiationException e) {
            LOG.error(String.format(&quot;解析第%个满足条件的记录%s失败。&quot;, rowNum, result), e);
        } catch (IllegalAccessException e) {
            LOG.error(String.format(&quot;解析第%s个满足条件的记录%s失败。&quot;, rowNum, result), e);
        }
        return obj;
    }

    private  Map&lt;String,Field&gt;  getDeclaredFields(Class&lt;?&gt; clazz){
        Field[] fields = clazz.getDeclaredFields();
        Field field = null;
        Map&lt;String,Field&gt; fieldMap = Maps.newHashMapWithExpectedSize(fields.length);

        for(int i = 0; i &lt; fields.length; i++){
            field = fields[i];
            if(field.getModifiers() == 2){
                field.setAccessible(true);
                fieldMap.put(field.getName().toLowerCase(), field);
            }
        }
        fields = null;

        return fieldMap;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/CellNumExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;

import java.io.IOException;

public class CellNumExtrator implements RowExtractor&lt;Integer&gt; {

    public Integer extractRowData(Result result, int rowNum) throws IOException {
        return  result.listCells().size();
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapLongRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

public class MapLongRowExtrator implements RowExtractor&lt;Map&lt;String,Long&gt;&gt; {

    private Map&lt;String,Long&gt; row;

    @Override
    public Map&lt;String, Long&gt; extractRowData(Result result, int rowNum) throws IOException {

        row = new HashMap&lt;String,Long&gt;();

        for(Cell cell :  result.listCells()) {
            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
        }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MapRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.io.Serializable;
import java.util.HashMap;
import java.util.Map;

public class MapRowExtrator implements RowExtractor&lt;Map&lt;String,String&gt;&gt;,Serializable {

    private static final long serialVersionUID = 1543027485077396235L;

    private Map&lt;String,String&gt; row;

    /* (non-Javadoc)
     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)
     */
    @Override
    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum) throws IOException {

        row = new HashMap&lt;String,String&gt;();

        for(Cell cell :  result.listCells()) {
            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
        }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/MultiVersionRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import com.hsiehchou.hbase.entity.HBaseRow;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class MultiVersionRowExtrator implements RowExtractor&lt;HBaseRow&gt;{

    private HBaseRow row;

    public HBaseRow extractRowData(Result result, int rowNum) throws IOException {

        row = new HBaseRow(Bytes.toString(result.getRow()));

        String field = null;
        String value = null;
        long capTime = 0L;
        for(Cell cell : result.listCells()){
            field = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());
            value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
            capTime = cell.getTimestamp();

            row.addCell(field, value, capTime);
        }
        return  row ;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowByteExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;

import java.io.IOException;
import java.io.Serializable;

public class OneColumnRowByteExtrator implements RowExtractor&lt;byte[]&gt; ,Serializable{

    private static final long serialVersionUID = -3420092335124240222L;

    private byte[] cf;
    private byte[] cl;

    public OneColumnRowByteExtrator( byte[] cf,byte[] cl ){
        this.cf = cf;
        this.cl = cl;
    }

    public byte[] extractRowData(Result result, int rowNum) throws IOException {
        return result.getValue(cf, cl);
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OneColumnRowStringExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.io.Serializable;

public class OneColumnRowStringExtrator implements RowExtractor&lt;String&gt;  , Serializable{

    private static final long serialVersionUID = -8585637277902568648L;

    private byte[] cf ;
    private byte[] cl ;

    public OneColumnRowStringExtrator( byte[] cf , byte[] cl ){
        this.cf = cf;
        this.cl = cl;
    }

    /* (non-Javadoc)
     * @see com.bh.d406.bigdata.hbase.extractor.RowExtractor#extractRowData(org.apache.hadoop.hbase.client.Result, int)
     */
    @Override
    public String extractRowData(Result result, int rowNum) throws IOException {

        byte[] value = result.getValue(cf, cl);
        if( value == null ) return null;

        return  Bytes.toString( value ) ;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;

import java.io.IOException;

public class OnlyRowKeyExtrator implements RowExtractor&lt;byte[]&gt; {

    @Override
    public byte[] extractRowData(Result result, int rowNum) throws IOException {
        // TODO Auto-generated method stub
        return result.getRow();
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/OnlyRowKeyStringExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class OnlyRowKeyStringExtrator implements RowExtractor&lt;String&gt; {

    public String extractRowData(Result result, int rowNum) throws IOException {
        return Bytes.toString( result.getRow() );
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/RowExtractor.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.client.Result;

import java.io.IOException;

public interface RowExtractor&lt;T&gt;  {

    /**
      * description:
      * @param result  result解析器
      * @param rowNum  
      * @return
      * @throws Exception
      * T
     */
    T extractRowData(Result result, int rowNum) throws IOException;
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/SingleColumnMultiVersionRowExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.Set;

public class SingleColumnMultiVersionRowExtrator implements RowExtractor&lt;Set&lt;String&gt;&gt;{

    private Set&lt;String&gt; values;
    private byte[] cf;
    private byte[] cl;


    /**
     * 单列解析器  获取hbase 单列多版本数据
     * @param cf     列簇
     * @param cl     列
     * @param values 返回值
     */
    public SingleColumnMultiVersionRowExtrator(byte[] cf, byte[] cl, Set&lt;String&gt; values){
        this.cf = cf;
        this.cl = cl;
        this.values = values;
    }

    public Set&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {

        for(Cell cell : result.getColumnCells(cf, cl)){
            values.add(Bytes.toString(cell.getValueArray(),cell.getValueOffset(), cell.getValueLength()));
        }
        return values;
    }

}</code></pre><p><strong>com/hsiehchou/hbase/extractor/StrToByteExtrator.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.io.Serializable;
import java.util.HashMap;
import java.util.Map;

public class StrToByteExtrator implements RowExtractor&lt;Map&lt;String,byte[]&gt;&gt; ,Serializable {

    private static final long serialVersionUID = 4633698173362569711L;

    private Map&lt;String,byte[]&gt; row;

    @Override
    public Map&lt;String, byte[]&gt; extractRowData(Result result, int rowNum) throws IOException {

        row = new HashMap&lt;String,byte[]&gt;();

        for(Cell cell :  result.listCells()) {
            row.put(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()),
                    Bytes.copy(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
        }
        return row;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowList.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;

/**
 * Hbase数据库中数据提取接口实现：
 * 提取result的rowKey，和每个cell的值作为一行数据，
 * 一个cell=(row, family:qualifier:value, version)
 *
 * &lt;p&gt;
 * 每行数据的格式为：{rowKey column${separator}value column${separator}value ...}
 * 其中，不同的列之间用空格分隔，同样列元素的描述符与值之间用${separator}分隔
 */
public class ToRowList implements RowExtractor&lt;List&lt;String&gt;&gt; {

    private Boolean currentVersion; //currentVersion为true:只取当前最新版本，false:取所有版本
    private char separator; //不同元素之间拼接时的分隔符，默认为`#`

    private ToRowList(Boolean currentVersion, char separator) {
        this.separator = separator;
        this.currentVersion = currentVersion;
    }

    public ToRowList(Boolean currentVersion) {
        this(currentVersion, &#39;#&#39;);
    }

    public ToRowList() {
        this(true, &#39;#&#39;);
    }

    /**
      * 对{当前版本}存放在list[0] = {rowKey` `column`#`value` `column`#`value ...}
      * 多版本的时候list({rowKey`#`version1` `column`#`value` `column`#`value ...},
      * {rowKey`#`version2` `column`#`value` `column`#`value ...})
      */
    @Override
    public List&lt;String&gt; extractRowData(Result result, int rowNum) throws IOException {
        if(result == null || result.isEmpty()) return null;

        final char SPACE = &#39; &#39;;

        List&lt;String&gt; rows = new LinkedList&lt;&gt;();

        //一个result是同一个rowKey的所有cells集合
        String rowKey = Bytes.toString(result.getRow());

        //build rowKey` `column`#`value` `column`#`value ...
        StringBuilder row = new StringBuilder();
        row.append(rowKey).append(SPACE);

        //用于处理不同版本的映射
        Map&lt;Long, String&gt; version2qualifiersAndValues = new HashMap&lt;&gt;();

        List&lt;Cell&gt; cells = result.listCells();
        for (Cell cell : cells) {
            String value = Bytes.toString(cell.getValueArray(),
                    cell.getValueOffset(), cell.getValueLength());
            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));

            if (currentVersion) {
                row.append(qualifier).append(separator).append(value).append(SPACE);
            } else {
                Long version = cell.getTimestamp();
                String tmp = version2qualifiersAndValues.get(version);
                version2qualifiersAndValues.put(version,
                        StringUtils.isNotBlank(tmp) ? tmp + &quot; &quot; + qualifier + separator + value
                                : rowKey + separator + version + &quot; &quot; + qualifier + separator + value);
            }
        }

        if (currentVersion) {
            rows.add(row.toString());
        } else {
            for (String v : version2qualifiersAndValues.values()) {
                rows.add(v);
            }
        }

        return rows;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/extractor/ToRowMap.java</strong></p>
<pre><code>package com.hsiehchou.hbase.extractor;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 * currentVersion 标识是否取多版本的数据，默认取当前版本
 * 对当前版本，返回row`#`qualifier-&gt;value的映射
 * 对多个版本，返回row`#`version`#`qualifier-&gt;value的映射
 */
public class ToRowMap implements RowExtractor&lt;Map&lt;String, String&gt;&gt; {

    private Boolean currentVersion;

    public ToRowMap() {
        this(true);
    }

    private ToRowMap(Boolean currentVersion) {
        this.currentVersion = currentVersion;
    }

    @Override
    public Map&lt;String, String&gt; extractRowData(Result result, int rowNum)
            throws IOException {
        if(result == null || result.isEmpty()) return null;

        final char HashTag = &#39;#&#39;;

        HashMap&lt;String, String&gt; col2value = new HashMap&lt;&gt;();

        String rowKey = Bytes.toString(result.getRow());

        for (Cell cell : result.listCells()) {
            String value = Bytes.toString(cell.getValueArray(),
                    cell.getValueOffset(), cell.getValueLength());
            String qualifier = Bytes.toString(CellUtil.cloneQualifier(cell));
            if (currentVersion)
                col2value.put(rowKey + HashTag + qualifier, value);
            else {
                long version = cell.getTimestamp();
                col2value.put(rowKey + HashTag + version + HashTag + qualifier, value);
            }
        }

        return col2value;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertException.java</strong></p>
<pre><code>package com.hsiehchou.hbase.insert;

import java.util.Iterator;

public class HBaseInsertException extends Exception{
    public HBaseInsertException(String message) {
        super(message);
    }

    public final synchronized void addSuppresseds(Iterable&lt;Exception&gt; exceptions){

        if(exceptions != null){
            Iterator&lt;Exception&gt; iterator = exceptions.iterator();
            while (iterator.hasNext()){
                addSuppressed(iterator.next());
            }
        }
    }
}</code></pre><p><strong>com/hsiehchou/hbase/insert/HBaseInsertHelper.java</strong></p>
<pre><code>package com.hsiehchou.hbase.insert;

import com.hsiehchou.hbase.config.HBaseTableUtil;
import com.google.common.collect.Lists;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * 添加HBASE 插入数据类
 */
public class HBaseInsertHelper implements Serializable{

    private HBaseInsertHelper(){}

    public static void put(String tableName, Put put) throws Exception {
        put(tableName, Lists.newArrayList(put));
    }

    public static void put(String tableName, List&lt;Put&gt; puts) throws Exception {
        if(!puts.isEmpty()){
            Table table = HBaseTableUtil.getTable(tableName);
            try {
                table.put(puts);
            }catch (Exception e){
                e.printStackTrace();
            }finally {
                HBaseTableUtil.close(table);
            }
        }
     }

    public static void put(final String tableName, List&lt;Put&gt; puts, int perThreadPutSize) throws Exception {

        int size = puts.size();
        if(size &gt; perThreadPutSize){

            int threadNum = (int)Math.ceil(size / (double)perThreadPutSize);
            ExecutorService executorService = Executors.newFixedThreadPool(threadNum);

            final CountDownLatch  cdl = new CountDownLatch(threadNum);
            final List&lt;Exception&gt;  es = Collections.synchronizedList(new ArrayList&lt;Exception&gt;());

            try {
                for(int i = 0; i &lt; threadNum; i++){
                    final List&lt;Put&gt; tmp;
                    if(i == (threadNum - 1)){
                        tmp = puts.subList(perThreadPutSize*i, size);
                    }else{
                        tmp = puts.subList(perThreadPutSize*i, perThreadPutSize*(i + 1));
                    }
                    executorService.execute(new Runnable() {
                        public void run() {
                            try {
                                if(es.isEmpty()) put(tableName, tmp);
                            } catch (Exception e) {
                                es.add(e);
                            }finally {
                                cdl.countDown();
                            }
                        }
                    });
                }
                cdl.await();
            }finally {
                executorService.shutdown();
            }
            if(es.size() &gt; 0){
                HBaseInsertException insertException = new HBaseInsertException(String.format(&quot;put数据到表%s失败。&quot;));
                insertException.addSuppresseds(es);
                throw insertException;
            }
        }else {
            put(tableName, puts);
        }
    }

    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,
                                   byte[] value, Put put) throws Exception {
        checkAndPut(tableName, row, family, qualifier, null, value, put);
    }

    public static void checkAndPut(String tableName, byte[] row, byte[] family, byte[] qualifier,
                                   CompareOp compareOp, byte[] value, Put put) throws Exception {

        if(!put.isEmpty() ){
            Table table = HBaseTableUtil.getTable(tableName);
            try {
                if(compareOp == null){
                    table.checkAndPut(row, family, qualifier, value, put);
                }else{
                    table.checkAndPut(row, family, qualifier, compareOp, value, put);
                }
            }finally{
                HBaseTableUtil.close(table);
            }
        }
    }
}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchService.java</strong></p>
<pre><code>package com.hsiehchou.hbase.search;

import com.hsiehchou.hbase.extractor.RowExtractor;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Scan;

import java.io.IOException;
import java.util.List;
import java.util.Map;


public interface HBaseSearchService {



    /**
      *  根据  用户 给定的解析类  解析  查询结果
      * @param tableName
      * @param scan
      * @param extractor  用户自定义的 结果解析 类
      * @return
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException;

    /**
      * 当存在多个  scan时  采用多线程查询
      * @param tableName
      * @param scans
      * @param extractor  用户自定义的 结果解析 类
      * @return
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException;

    /**
      * 采用多线程  同时查询多个表
      * @param more
      * @return
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; Map&lt;String,List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException;

    /**
      * 利用反射  自动封装实体类
      * @param tableName
      * @param scan    
      * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写
      * @return
      * @throws IOException
      * @throws InstantiationException
      * @throws IllegalAccessException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;

    /**
      * 当存在多个 scan 时  采用多线程查询
      * @param tableName
      * @param scans
      * @param cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写
      * @return
      * @throws IOException
      * @throws InstantiationException
      * @throws IllegalAccessException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;


    /**
      * 批量 get 查询  并按自定义的方式解析结果集
      * @param tableName
      * @param gets
      * @param extractor  用户自定义的 结果解析 类
      * @return
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;

    /**
      * 多线程批量get, 并按自定义的方式解析结果集
      * 建议 : perThreadExtractorGetNum &gt;= 100
      * @param tableName
      * @param gets
      * @param perThreadExtractorGetNum    每个线程处理的 get的个数 
      * @param extractor  用户自定义的 结果解析 类
      * @return
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException;

    /**
      * 批量 get 查询  并利用反射 封装到指定的实体类中
      * @param tableName
      * @param gets
      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写
      * @return      
      * @throws IOException
      * @throws InstantiationException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;

    /**
      * 多线程批量 get 查询  并利用反射 封装到指定的实体类中
      * 建议 : perThreadExtractorGetNum &gt;= 100
      * @param tableName
      * @param gets
      * @param perThreadExtractorGetNum  每个线程处理的 get的个数 
      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写
      * @return
      * @throws IOException
      * @throws InstantiationException
      * @throws IllegalAccessException
      * List&lt;T&gt;
     */
    &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;

    /**
      * get 查询  并按自定义的方式解析结果集
      * @param tableName
      * @param extractor   用户自定义的 结果解析 类
      * @return     如果 查询不到  则 返回  null
      * @throws IOException
      * List&lt;T&gt;
     */
    &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException;

    /**
      * get 查询  并利用反射 封装到指定的实体类中
      * @param tableName
      * @param  cls   HBase表对应的实体类，属性只包含对应表的 列 ， 不区分大小写
      * @return     如果 查询不到  则 返回  null
      * @throws IOException
      * @throws InstantiationException
      * List&lt;T&gt;
     */
    &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException;

}</code></pre><p><strong>com/hsiehchou/hbase/search/HBaseSearchServiceImpl.java</strong></p>
<pre><code>package com.hsiehchou.hbase.search;

import com.hsiehchou.hbase.config.HBaseTableFactory;
import com.hsiehchou.hbase.extractor.RowExtractor;
import org.apache.hadoop.hbase.client.*;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;


public class HBaseSearchServiceImpl implements HBaseSearchService,Serializable{

    private static final long serialVersionUID = -8657479861137115645L;

    private static final Logger LOG = LoggerFactory.getLogger(HBaseSearchServiceImpl.class);

    private HBaseTableFactory factory = new HBaseTableFactory();
    private int poolCapacity = 6;


    @Override
    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, RowExtractor&lt;T&gt; extractor) throws IOException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, RowExtractor&lt;T&gt; extractor) throws IOException {
        return null;
    }

    @Override
    public &lt;T&gt; Map&lt;String, List&lt;T&gt;&gt; searchMore(List&lt;SearchMoreTable&lt;T&gt;&gt; more) throws IOException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; search(String tableName, Scan scan, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Scan&gt; scans, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException {
        List&lt;T&gt; data = new ArrayList&lt;T&gt;();
        search(tableName, gets, extractor,data);
        return data;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, RowExtractor&lt;T&gt; extractor) throws IOException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {
        return null;
    }

    @Override
    public &lt;T&gt; List&lt;T&gt; searchMore(String tableName, List&lt;Get&gt; gets, int perThreadExtractorGetNum, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {
        return null;
    }

    @Override
    public &lt;T&gt; T search(String tableName, Get get, RowExtractor&lt;T&gt; extractor) throws IOException {

        T obj = null;
        List&lt;T&gt; res = search(tableName,Arrays.asList(get),extractor);
        if( !res.isEmpty()){
            obj = res.get(0);
        }

        return obj;
    }

    @Override
    public &lt;T&gt; T search(String tableName, Get get, Class&lt;T&gt; cls) throws IOException, InstantiationException, IllegalAccessException {
        return null;
    }

    private &lt;T&gt; void search(String tableName, List&lt;Get&gt; gets,
                            RowExtractor&lt;T&gt; extractor , List&lt;T&gt; data ) throws IOException {

        //根据table名获取表连接
        Table table = factory.getHBaseTableInstance(tableName);
        if(table != null ){
            Result[] results = table.get(gets);
            int n = 0;
            T row = null;
            for( Result result : results){
                if( !result.isEmpty() ){
                    row = extractor.extractRowData(result, n);
                    if(row != null )data.add(row);
                    n++;
                }
            }
            close( table, null);
        }else{
            throw new IOException(&quot; table  &quot; + tableName + &quot; is not exists ..&quot;);
        }
    }

    public static boolean  existsRowkey( Table table, String rowkey){
        boolean exists =true;
        try {
            exists = table.exists(new Get(rowkey.getBytes()));
        } catch (IOException e) {
            LOG.error(&quot;失败。&quot;, e );
        }
        return exists;
    }

    public static void  close( Table table, ResultScanner scanner ){

        try {
            if( table != null ){
                table.close();
                table = null;
            }
            if( scanner != null ){
                scanner.close();
                scanner = null;
            }
        } catch (IOException e) {
            LOG.error(&quot;关闭 HBase的表  &quot; + table.getName().toString() + &quot; 失败。&quot;, e );
        }

    }
}</code></pre><p><strong>com/hsiehchou/hbase/search/SearchMoreTable.java</strong></p>
<pre><code>package com.hsiehchou.hbase.search;

import com.hsiehchou.hbase.extractor.RowExtractor;
import org.apache.hadoop.hbase.client.Scan;

public class SearchMoreTable&lt;T&gt; {

    private String tableName;
    private Scan scan;
    private RowExtractor&lt;T&gt; extractor;

    public SearchMoreTable() {
        super();
    }

    public SearchMoreTable(String tableName, Scan scan,
            RowExtractor&lt;T&gt; extractor) {
        super();
        this.tableName = tableName;
        this.scan = scan;
        this.extractor = extractor;
    }

    public String getTableName() {
        return tableName;
    }
    public void setTableName(String tableName) {
        this.tableName = tableName;
    }
    public Scan getScan() {
        return scan;
    }
    public void setScan(Scan scan) {
        this.scan = scan;
    }
    public RowExtractor&lt;T&gt; getExtractor() {
        return extractor;
    }
    public void setExtractor(RowExtractor&lt;T&gt; extractor) {
        this.extractor = extractor;
    }
}</code></pre><p><strong>com/hsiehchou/hbase/spilt/SpiltRegionUtil.java</strong></p>
<pre><code>package com.hsiehchou.hbase.spilt;

import org.apache.hadoop.hbase.util.Bytes;

import java.util.Iterator;
import java.util.TreeSet;

/**
 * hbase 预分区
 */
public class SpiltRegionUtil {

    /**
     * 定义分区
     * @return
     */
    public static byte[][] getSplitKeysBydinct() {

        String[] keys = new String[]{&quot;1&quot;,&quot;2&quot;, &quot;3&quot;,&quot;4&quot;, &quot;5&quot;,&quot;6&quot;, &quot;7&quot;,&quot;8&quot;, &quot;9&quot;,&quot;a&quot;,&quot;b&quot;, &quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;};
        //String[] keys = new String[]{&quot;10|&quot;, &quot;20|&quot;, &quot;30|&quot;, &quot;40|&quot;, &quot;50|&quot;, &quot;60|&quot;, &quot;70|&quot;, &quot;80|&quot;, &quot;90|&quot;};
        byte[][] splitKeys = new byte[keys.length][];

        //通过treeset排序
        TreeSet&lt;byte[]&gt; rows = new TreeSet&lt;byte[]&gt;(Bytes.BYTES_COMPARATOR);//升序排序
        for (int i = 0; i &lt; keys.length; i++) {
            rows.add(Bytes.toBytes(keys[i]));
        }
        Iterator&lt;byte[]&gt; rowKeyIter = rows.iterator();
        int i = 0;
        while (rowKeyIter.hasNext()) {
            byte[] tempRow = rowKeyIter.next();
            rowKeyIter.remove();
            splitKeys[i] = tempRow;
            i++;
        }
        return splitKeys;
    }
}</code></pre><h4 id="6、执行"><a href="#6、执行" class="headerlink" title="6、执行"></a>6、执行</h4><p>spark-submit <code>--</code>master local[1] <code>--</code>num-executors 1 <code>--</code>driver-memory 300m <code>--</code>executor-memory 500m <code>--</code>executor-cores 1 <code>--</code>jars $(echo /usr/chl/spark7/jars/*.jar | tr ‘ ‘ ‘,’) <code>--</code>class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</p>
<h4 id="7、执行截图"><a href="#7、执行截图" class="headerlink" title="7、执行截图"></a>7、执行截图</h4><p><img src="/medias/hbase_list.PNG" alt="hbase_list"></p>
<p><img src="/medias/hbase_scan.PNG" alt="hbase_scan"></p>
<p><img src="/medias/hbase%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE.PNG" alt="hbase写入数据"></p>
<h3 id="十二、SpringCloud-项目构建"><a href="#十二、SpringCloud-项目构建" class="headerlink" title="十二、SpringCloud 项目构建"></a>十二、SpringCloud 项目构建</h3><p><img src="/medias/SpringCloud%E5%BE%AE%E6%9C%8D%E5%8A%A1.PNG" alt="SpringCloud微服务"></p>
<p><img src="/medias/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C.PNG" alt="服务注册"></p>
<p><strong>解决IntelliJ IDEA 创建Maven项目速度慢问题</strong><br>add Maven Property<br>Name:archetypeCatalog<br>Value:internal</p>
<h4 id="1、构建SpringCloud父项目"><a href="#1、构建SpringCloud父项目" class="headerlink" title="1、构建SpringCloud父项目"></a>1、构建SpringCloud父项目</h4><p>在原项目下新建 xz_bigdata_springcloud_dir目录</p>
<p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_dir%E7%9B%AE%E5%BD%95.PNG" alt="新建 xz_bigdata_springcloud_dir目录"></p>
<h4 id="2、在此目录下新建-xz-bigdata-springclod-root项目"><a href="#2、在此目录下新建-xz-bigdata-springclod-root项目" class="headerlink" title="2、在此目录下新建 xz_bigdata_springclod_root项目"></a>2、在此目录下新建 xz_bigdata_springclod_root项目</h4><p><img src="/medias/%E6%96%B0%E5%BB%BA%20xz_bigdata_springcloud_root%E9%A1%B9%E7%9B%AE.PNG" alt="新建 xz_bigdata_springcloud_root项目"></p>
<h4 id="3、-引入SpringCloud依赖"><a href="#3、-引入SpringCloud依赖" class="headerlink" title="3、    引入SpringCloud依赖"></a>3、    引入SpringCloud依赖</h4><p><strong>父pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;modules&gt;
    &lt;module&gt;xz_bigdata_springcloud_common&lt;/module&gt;
    &lt;module&gt;xz_bigdata_springcloud_esquery&lt;/module&gt;
    &lt;module&gt;xz_bigdata_springcloud_eureka&lt;/module&gt;
    &lt;module&gt;xz_bigdata_springcloud_hbasequery&lt;/module&gt;
  &lt;/modules&gt;

  &lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;2.0.9.RELEASE&lt;/version&gt;
  &lt;/parent&gt;

  &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
  &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
  &lt;packaging&gt;pom&lt;/packaging&gt;

  &lt;name&gt;xz_bigdata_springcloud_root&lt;/name&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;

  &lt;!--CDH源--&gt;
  &lt;repositories&gt;
    &lt;repository&gt;
      &lt;id&gt;cloudera&lt;/id&gt;
      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;/repository&gt;
  &lt;/repositories&gt;
  &lt;!--依赖管理，用于管理spring-cloud的依赖--&gt;
  &lt;dependencyManagement&gt;
    &lt;dependencies&gt;
      &lt;!--spring-cloud-dependencies--&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
        &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;
        &lt;version&gt;Finchley.SR1&lt;/version&gt;
        &lt;type&gt;pom&lt;/type&gt;
        &lt;scope&gt;import&lt;/scope&gt;
      &lt;/dependency&gt;
    &lt;/dependencies&gt;
  &lt;/dependencyManagement&gt;
  &lt;!--打包插件--&gt;
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;version&gt;3.1&lt;/version&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.8&lt;/source&gt;
          &lt;target&gt;1.8&lt;/target&gt;
          &lt;encoding&gt;UTF-8&lt;/encoding&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
&lt;/project&gt;</code></pre><p><strong>删除父项目src目录。因为这个项目主要是管理子项目不做任何逻辑业务</strong></p>
<h4 id="4、构建SpringCloud-Common子项目"><a href="#4、构建SpringCloud-Common子项目" class="headerlink" title="4、构建SpringCloud Common子项目"></a>4、构建SpringCloud Common子项目</h4><p><strong>新建子模块</strong><br>xz_bigdata_springcloud_common</p>
<p><strong>引入依赖</strong></p>
<p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_springcloud_common&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!--eureka-server--&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-starter-eureka-server --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;
            &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;
                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
            &lt;version&gt;1.2.24&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;</code></pre><h4 id="5、构建Eureka服务注册中心"><a href="#5、构建Eureka服务注册中心" class="headerlink" title="5、构建Eureka服务注册中心"></a>5、构建Eureka服务注册中心</h4><p><strong>新建xz_bigdata_springcloud_eureka子模块</strong></p>
<p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_eureka%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_eureka子模块"></p>
<p><strong>引入依赖</strong></p>
<p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_springcloud_eureka&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_springcloud_eureka&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!--用户验证--&gt;
  &lt;!--      &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;
            &lt;version&gt;1.4.1.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;--&gt;
    &lt;/dependencies&gt;


    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;
                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;
                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;
                &lt;/configuration&gt;

                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;
                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;
                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;
                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;!-- 打成jar包插件 --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;!--
                        生成的jar中，不要包含pom.xml和pom.properties这两个文件
                    --&gt;
                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;
                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;
                        &lt;manifest&gt;
                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;
                            &lt;!-- jar启动入口类--&gt;
                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                        &lt;!--       &lt;manifestEntries&gt;
                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;
                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;
                               &lt;/manifestEntries&gt;--&gt;
                    &lt;/archive&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;
                    &lt;includes&gt;
                        &lt;!-- 打jar包时，只打包class文件 --&gt;
                        &lt;include&gt;**/*.class&lt;/include&gt;
                        &lt;include&gt;**/*.properties&lt;/include&gt;
                        &lt;include&gt;**/*.yml&lt;/include&gt;
                    &lt;/includes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;</code></pre><p>新建resources配置文件目录，添加application.yml配置文件或者 application.properties</p>
<p><strong>application.yml</strong></p>
<pre><code>server:
  port: 8761
eureka:
  client:
    register-with-eureka: false
    fetch-registry: false
    service-url:
      defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p><img src="/medias/xz_bigdata_springcloud_eureka%E7%BB%93%E6%9E%84.PNG" alt="xz_bigdata_springcloud_eureka结构"></p>
<p><strong>新建EurekaApplication 启动类</strong></p>
<p><strong>EurekaApplication.java</strong></p>
<pre><code>package com.hsiehchou.springcloud.eureka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;

/**
 * 注册中心
 */
@SpringBootApplication
@EnableEurekaServer
public class EurekaApplication
{
    public static void main( String[] args )
    {
        SpringApplication.run(EurekaApplication.class, args);
    }
}</code></pre><p><strong>执行EurekaApplication 启动</strong></p>
<p><strong>访问localhost:8761</strong></p>
<p><img src="/medias/%E8%AE%BF%E9%97%AEhadoop38761.PNG" alt="访问hadoop3:8761"></p>
<h4 id="6、构建HBase查询服务模块"><a href="#6、构建HBase查询服务模块" class="headerlink" title="6、构建HBase查询服务模块"></a>6、构建HBase查询服务模块</h4><p><strong>新建xz_bigdata_springcloud_root子模块</strong></p>
<p><img src="/medias/%E6%96%B0%E5%BB%BAxz_bigdata_springcloud_root%E5%AD%90%E6%A8%A1%E5%9D%97.PNG" alt="新建xz_bigdata_springcloud_root子模块"></p>
<p><strong>添加依赖</strong></p>
<p><strong>pom.xml</strong></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;

&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;xz_bigdata_springcloud_root&lt;/artifactId&gt;
        &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;xz_bigdata_springcloud_hbasequery&lt;/artifactId&gt;

    &lt;name&gt;xz_bigdata_springcloud_hbasequery&lt;/name&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!--spring common依赖--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou.springcloud&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_springcloud_common&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;HdrHistogram&lt;/artifactId&gt;
                    &lt;groupId&gt;org.hdrhistogram&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;!--基础服务hbase依赖--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.hsiehchou&lt;/groupId&gt;
            &lt;artifactId&gt;xz_bigdata_hbase&lt;/artifactId&gt;
            &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;artifactId&gt;fastjson&lt;/artifactId&gt;
                    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;
                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;
                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;
                &lt;/configuration&gt;

                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;
                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;
                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;
                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;!-- 打成jar包插件 --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;!--
                        生成的jar中，不要包含pom.xml和pom.properties这两个文件
                    --&gt;
                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;
                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;
                        &lt;manifest&gt;
                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;
                            &lt;!-- jar启动入口类--&gt;
                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                        &lt;!--       &lt;manifestEntries&gt;
                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;
                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;
                               &lt;/manifestEntries&gt;--&gt;
                    &lt;/archive&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;
                    &lt;includes&gt;
                        &lt;!-- 打jar包时，只打包class文件 --&gt;
                        &lt;include&gt;**/*.class&lt;/include&gt;
                        &lt;include&gt;**/*.properties&lt;/include&gt;
                        &lt;include&gt;**/*.yml&lt;/include&gt;
                    &lt;/includes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;</code></pre><p><strong>添加配置文件</strong></p>
<p><strong>新建 resources 目录</strong><br>添加 <strong>application.properties</strong> 文件</p>
<pre><code>server.port=8002

logging.level.root=INFO
logging.level.org.hibernate=INFO
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE
logging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACE
logging.level.com.itmuch=DEBUG
spring.http.encoding.charset=UTF-8
spring.http.encoding.enable=true
spring.http.encoding.force=true

eureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/

spring.application.name=xz-bigdata-springcloud-hbasequery
eureka.instance.prefer-ip-address=true</code></pre><p><strong>构建启动类</strong></p>
<p>新建 <strong>com.hsiehchou.springcloud.hbase</strong>包<br>构建 <strong>HbaseApplication</strong> 启动类</p>
<pre><code>package com.hsiehchou.springcloud;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;

@SpringBootApplication
@EnableEurekaServer
public class HbaseQueryApplication
{
    public static void main( String[] args )
    {
        SpringApplication.run(HbaseQueryApplication.class, args);
    }
}</code></pre><p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p>
<p><strong>构建服务</strong></p>
<p><img src="/medias/%E6%9E%84%E5%BB%BAHbase%E6%9C%8D%E5%8A%A1.PNG" alt="构建Hbase服务"></p>
<p>构建 <strong>com.hsiehchou.springcloud.hbase.controller</strong></p>
<p>创建 <strong>HbaseBaseController</strong></p>
<p><strong>HbaseBaseController.java</strong></p>
<pre><code>package com.hsiehchou.springcloud.hbase.controller;

import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;
import com.hsiehchou.hbase.search.HBaseSearchService;
import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;
import com.hsiehchou.springcloud.hbase.service.HbaseBaseService;
import org.apache.hadoop.hbase.client.Get;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.*;

import javax.annotation.Resource;
import java.io.IOException;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

@Controller
@RequestMapping(value=&quot;/hbase&quot;)
public class HbaseBaseController {

    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseController.class);


    //注入 通过这个注解可以直接拿到HbaseBaseService这个的实例
    @Resource
    private HbaseBaseService hbaseBaseService;

    @ResponseBody
    @RequestMapping(value=&quot;/search/{table}/{rowkey}&quot;, method={RequestMethod.GET,RequestMethod.POST})
    public Set&lt;String&gt; search(@PathVariable(value = &quot;table&quot;) String table,
                              @PathVariable(value = &quot;rowkey&quot;) String rowkey){
        return hbaseBaseService.getSingleColumn(table,rowkey);
    }

    @ResponseBody
    @RequestMapping(value=&quot;/search1&quot;, method={RequestMethod.GET,RequestMethod.POST})
    public Set&lt;String&gt; search1( @RequestParam(name = &quot;table&quot;) String table,
                                @RequestParam(name = &quot;rowkey&quot;) String rowkey){
        //通过二级索引去找主关联表的rowkey 这个rowkey就是MAC
        return hbaseBaseService.getSingleColumn(table,rowkey);
    }

    @ResponseBody
    @RequestMapping(value = &quot;/getHbase&quot;,method = {RequestMethod.GET,RequestMethod.POST})
    public Set&lt;String&gt; getHbase(@RequestParam(name=&quot;table&quot;) String table,
                                @RequestParam(name=&quot;rowkey&quot;) String rowkey){
        return hbaseBaseService.getSingleColumn(table, rowkey);
    }

    @ResponseBody
    @RequestMapping(value = &quot;/getRelation&quot;,method = {RequestMethod.GET,RequestMethod.POST})
    public Map&lt;String,List&lt;String&gt;&gt; getRelation(@RequestParam(name = &quot;field&quot;) String field,
                                                @RequestParam(name = &quot;fieldValue&quot;) String fieldValue){
        return hbaseBaseService.getRealtion(field,fieldValue);
    }

    public static void main(String[] args) {
        HbaseBaseController hbaseBaseController = new HbaseBaseController();
        hbaseBaseController.getHbase(&quot;send_mail&quot;, &quot;65497873@qq.com&quot;);
    }
}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.hbase.service</strong></p>
<p>创建 <strong>HbaseBaseService</strong></p>
<p><strong>HbaseBaseService.java</strong></p>
<pre><code>package com.hsiehchou.springcloud.hbase.service;

import com.hsiehchou.hbase.entity.HBaseCell;
import com.hsiehchou.hbase.entity.HBaseRow;
import com.hsiehchou.hbase.extractor.MultiVersionRowExtrator;
import com.hsiehchou.hbase.extractor.SingleColumnMultiVersionRowExtrator;
import com.hsiehchou.hbase.search.HBaseSearchService;
import com.hsiehchou.hbase.search.HBaseSearchServiceImpl;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Put;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;

import javax.annotation.Resource;
import java.io.IOException;
import java.util.*;

@Service
public class HbaseBaseService {
    private static Logger LOG = LoggerFactory.getLogger(HbaseBaseService.class);

    @Resource
    private HbaseBaseService hbaseBaseService;

    /**
     * 获取hbase单列数据的多版本信息
     * @param field
     * @param rowkey
     * @return
     */
    public Set&lt;String&gt; getSingleColumn(String field,String rowkey){
        //从索引表中获取总关联表的rowkey,获取phone对应的多版本MAC
        Set&lt;String&gt; search = null;
        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();
        String table = &quot;test:&quot;+field;
        Get get = new Get(rowkey.getBytes());
        try {
            get.setMaxVersions(100);
        } catch (IOException e) {
            e.printStackTrace();
        }
        Set set = new HashSet&lt;String&gt;();
        SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);

        try {
            search = hBaseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);
            System.out.println(search.toString());
        } catch (IOException e) {
            e.printStackTrace();
        }
        return search;
    }

    /**
     *  获取单列多版本
     * @param table
     * @param rowkey
     * @param versions
     * @return
     */
    public Set&lt;String&gt; getSingleColumn(String table,String rowkey,int versions){
        Set&lt;String&gt; search = null;
        try {
            HBaseSearchService baseSearchService = new HBaseSearchServiceImpl();
            Get get = new Get(rowkey.getBytes());
            get.setMaxVersions(versions);
            Set set = new HashSet&lt;String&gt;();
            SingleColumnMultiVersionRowExtrator singleColumnMultiVersionRowExtrator = new SingleColumnMultiVersionRowExtrator(&quot;cf&quot;.getBytes(), &quot;phone_mac&quot;.getBytes(), set);
            search = baseSearchService.search(table, get, singleColumnMultiVersionRowExtrator);
        } catch (IOException e) {
            LOG.error(null,e);
        }
        System.out.println(search);
        return search;
    }

    /**
     * 直接通过关联表字段值获取整条记录
     * hbase 二级查找
     * @param field
     * @param fieldValue
     * @return
     */
    public Map&lt;String,List&lt;String&gt;&gt; getRealtion(String field,String fieldValue){

        //第一步 从二级索引表中找到多版本的rowkey
        Map&lt;String,List&lt;String&gt;&gt; map = new HashMap&lt;&gt;();

        //首先查找索引表
        //查找的表名
        String table = &quot;test:&quot; + field;
        String indexRowkey = fieldValue;
        HbaseBaseService hbaseBaseService = new HbaseBaseService();
        Set&lt;String&gt; relationRowkeys = hbaseBaseService.getSingleColumn(table, indexRowkey, 100);

        //第二步 拿到二级索引表中得到的 主关联表的rowkey
        //对这些rowkey进行遍历 获取主关联表中rowkey对应的所有多版本数据

        //遍历relationRowkeys，将其封装成List&lt;Get&gt;
        List&lt;Get&gt; list = new ArrayList&lt;&gt;();
        relationRowkeys.forEach(relationRowkey-&gt;{
            //通过relationRowkey去找relation表中的所有信息
            Get get = new Get(relationRowkey.getBytes());
            try {
                get.setMaxVersions(100);
            } catch (IOException e) {
                e.printStackTrace();
            }
            list.add(get);
        });

        MultiVersionRowExtrator multiVersionRowExtrator = new MultiVersionRowExtrator();
        HBaseSearchService hBaseSearchService = new HBaseSearchServiceImpl();

        try {
            //&lt;T&gt; List&lt;T&gt; search(String tableName, List&lt;Get&gt; gets, RowExtractor&lt;T&gt; extractor) throws IOException;

            List&lt;HBaseRow&gt; search = hBaseSearchService.search(&quot;test:relation&quot;, list, multiVersionRowExtrator);
            search.forEach(hbaseRow-&gt;{
                Map&lt;String, Collection&lt;HBaseCell&gt;&gt; cellMap = hbaseRow.getCell();
                cellMap.forEach((key,value)-&gt;{
                    //把Map&lt;String,Collection&lt;HBaseCell&gt;&gt;转为Map&lt;String,List&lt;String&gt;&gt;
                    List&lt;String&gt; listValue = new ArrayList&lt;&gt;();
                    value.forEach(x-&gt;{
                        listValue.add(x.toString());
                    });
                    map.put(key,listValue);
                });
            });
        } catch (IOException e) {
            e.printStackTrace();
        }
        System.out.println(map.toString());
     return map;
    }

    public static void main(String[] args) {
        HbaseBaseService hbaseBaseService = new HbaseBaseService();
//        hbaseBaseService.getRealtion(&quot;send_mail&quot;,&quot;65494533@qq.com&quot;);
        hbaseBaseService.getSingleColumn(&quot;phone&quot;,&quot;18609765012&quot;);
    }
}</code></pre><h4 id="7、构建ES查询服务"><a href="#7、构建ES查询服务" class="headerlink" title="7、构建ES查询服务"></a>7、构建ES查询服务</h4><p>使用jest API 是走的 <strong>HTTP 请求</strong>  <strong>9200端口</strong><br>依赖如下:</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.searchbox&lt;/groupId&gt;
    &lt;artifactId&gt;jest&lt;/artifactId&gt;
    &lt;version&gt;6.3.1&lt;/version&gt;
&lt;/dependency&gt;</code></pre><p>9200作为Http协议，<strong>主要用于外部通讯</strong></p>
<p>9300作为Tcp协议，jar之间就是通过 <strong>tcp协议通讯</strong></p>
<p><strong>ES集群之间是通过9300进行通讯</strong></p>
<p><strong>新建xz_bigdata_springcloud_esquery</strong></p>
<p><strong>新建xz_bigdata_springcloud_esquery子项目</strong></p>
<p><strong>准备</strong></p>
<p>新建 <strong>resources</strong> 配置文件目录</p>
<p><strong>增加配置文件</strong></p>
<p><strong>application.properties</strong></p>
<pre><code>server.port=8003

logging.level.root=INFO
logging.level.org.hibernate=INFO
logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE
logging.level.org.hibernate.type.descriptor.sql.BasicExtractor= TRACE
logging.level.com.itmuch=DEBUG
spring.http.encoding.charset=UTF-8
spring.http.encoding.enable=true
spring.http.encoding.force=true

eureka.client.serviceUrl.defaultZone=http://root:root@hadoop3:8761/eureka/

spring.application.name=xz-bigdata-springcloud-esquery
eureka.instance.prefer-ip-address=true


#关闭EDES检测
management.health.elasticsearch.enabled=false

spring.elasticsearch.jest.uris=[&quot;http://192.168.116.201:9200&quot;]


#全部索引
esIndexs=wechat,mail,qq</code></pre><p><strong>新建ES微服务启动类</strong></p>
<p><strong>ESqueryApplication.java</strong></p>
<pre><code>package com.hsiehchou.springcloud.es;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cloud.client.discovery.EnableDiscoveryClient;
import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;
import org.springframework.cloud.openfeign.EnableFeignClients;

@SpringBootApplication
@EnableEurekaServer
@EnableDiscoveryClient
@EnableFeignClients
public class ESqueryApplication {
    public static void main(String[] args) {
        SpringApplication.run(ESqueryApplication.class,args);
    }
}</code></pre><p><strong>启动 Eureka  ES 微服务</strong></p>
<p><img src="/medias/%E6%B3%A8%E5%86%8C%E6%88%90%E5%8A%9F.PNG" alt="注册成功"><br>说明注册成功</p>
<p><img src="/medias/ES%E8%B0%83%E7%94%A8Hbase.PNG" alt="ES调用Hbase"></p>
<p>构建 <strong>com.hsiehchou.springcloud.es.controller</strong></p>
<p>创建 <strong>EsBaseController</strong></p>
<pre><code>package com.hsiehchou.springcloud.es.controller;

import com.hsiehchou.springcloud.es.feign.HbaseFeign;
import com.hsiehchou.springcloud.es.service.EsBaseService;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.ResponseBody;

import javax.annotation.Resource;
import java.util.List;
import java.util.Map;
import java.util.Set;

@Controller
@RequestMapping(value = &quot;/es&quot;)
public class EsBaseController {


    @Value(&quot;${esIndexs}&quot;)
    private String esIndexs;

    @Resource
    private EsBaseService esBaseService;

    @Resource
    private HbaseFeign hbaseFeign;

    /**
     * 基础查询
     * @param indexName
     * @param typeName
     * @param sortField
     * @param sortValue
     * @param pageNumber
     * @param pageSize
     * @return
     */
    @ResponseBody
    @RequestMapping(value = &quot;/getBaseInfo&quot;, method = {RequestMethod.GET, RequestMethod.POST})
    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(@RequestParam(name = &quot;indexName&quot;) String indexName,
                                                 @RequestParam(name = &quot;typeName&quot;) String typeName,
                                                 @RequestParam(name = &quot;sortField&quot;) String sortField,
                                                 @RequestParam(name = &quot;sortValue&quot;) String sortValue,
                                                 @RequestParam(name = &quot;pageNumber&quot;) int pageNumber,
                                                 @RequestParam(name = &quot;pageSize&quot;) int pageSize) {
        // 根据数据类型, 排序，分页
        // indexName typeName
        // sortField sortValue
        // pageNumber  pageSize
        return  esBaseService.getBaseInfo(indexName,typeName,sortField,sortValue,pageNumber,pageSize);
    }


    /**
     * 根据任意条件查找轨迹数据
     * @param field
     * @param fieldValue
     * @return
     */
    @ResponseBody
    @RequestMapping(value = &quot;/getLocus&quot;, method = {RequestMethod.GET, RequestMethod.POST})
    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(@RequestParam(name = &quot;field&quot;) String field,
                                                 @RequestParam(name = &quot;fieldValue&quot;) String fieldValue) {

        Set&lt;String&gt; macs = hbaseFeign.search1(field, fieldValue);
        System.out.println(macs.toString());
        // 根据数据类型, 排序，分页
        // indexName typeName
        // sortField sortValue
        // pageNumber  pageSize
        String mac = macs.iterator().next();

        return  esBaseService.getLocus(mac);
    }

    /**
     * 所有表数据总量
     * @return
     */
    @ResponseBody
    @RequestMapping(value=&quot;/getAllCount&quot;, method={RequestMethod.GET,RequestMethod.POST})
    public Map&lt;String,Long&gt; getAllCount(){
        Map&lt;String, Long&gt; allCount = esBaseService.getAllCount(esIndexs);
        System.out.println(allCount);
        return allCount;
    }

    @ResponseBody
    @RequestMapping(value=&quot;/group&quot;, method={RequestMethod.GET,RequestMethod.POST})
    public Map&lt;String,Long&gt; group(@RequestParam(name = &quot;indexName&quot;) String indexName,
                                  @RequestParam(name = &quot;typeName&quot;) String typeName,
                                  @RequestParam(name = &quot;field&quot;) String field){
        return esBaseService.aggregation(indexName,typeName,field);
    }


    public static void main(String[] args){
        EsBaseController esBaseController = new EsBaseController();
        esBaseController.getLocus(&quot;phone&quot;,&quot;18609765432&quot;);
    }
}</code></pre><p>构建 <strong>com.hsiehchou.springcloud.es.service</strong></p>
<p>创建 <strong>EsBaseService</strong></p>
<pre><code>package com.hsiehchou.springcloud.es.service;

import com.hsiehchou.es.jest.service.JestService;
import com.hsiehchou.es.jest.service.ResultParse;
import io.searchbox.client.JestClient;
import io.searchbox.core.SearchResult;
import org.springframework.stereotype.Service;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

@Service
public class EsBaseService {

    // 根据数据类型, 排序，分页
    // indexName typeName
    // sortField sortValue
    // pageNumber  pageSize
    public List&lt;Map&lt;String, Object&gt;&gt; getBaseInfo(String indexName,
                                                 String typeName,
                                                 String sortField,
                                                 String sortValue,
                                                 int pageNumber,
                                                 int pageSize) {
        //实现查询
        JestClient jestClient = null;
        List&lt;Map&lt;String, Object&gt;&gt; maps = null;
        try {
            jestClient = JestService.getJestClient();
            SearchResult search = JestService.search(jestClient,
                    indexName,
                    typeName,
                    &quot;&quot;,
                    &quot;&quot;,
                    sortField,
                    sortValue,
                    pageNumber,
                    pageSize);
            maps = ResultParse.parseSearchResultOnly(search);
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            JestService.closeJestClient(jestClient);
        }
        return maps;
    }


    // 传时间范围   比如你要查3天之内的轨迹
    // es中text的类型的可以直接查询，而keyword类型的必须带.keyword，例如，phone_mac.keyword
    public List&lt;Map&lt;String, Object&gt;&gt; getLocus(String mac){
        //实现查询
        JestClient jestClient = null;
        List&lt;Map&lt;String, Object&gt;&gt; maps = null;
        String[] includes = new String[]{&quot;latitude&quot;,&quot;longitude&quot;,&quot;collect_time&quot;};
        try {
            jestClient = JestService.getJestClient();
            SearchResult search = JestService.search(jestClient,
                    &quot;&quot;,
                    &quot;&quot;,
                    &quot;phone_mac.keyword&quot;,
                    mac,
                    &quot;collect_time&quot;,
                    &quot;asc&quot;,
                    1,
                    2000,
                    includes);
            maps = ResultParse.parseSearchResultOnly(search);
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            JestService.closeJestClient(jestClient);
        }
        return maps;
    }


     public Map&lt;String,Long&gt; getAllCount(String esIndexs){

        Map&lt;String,Long&gt; countMap = new HashMap&lt;&gt;();
        JestClient jestClient = null;
        try {
            jestClient = JestService.getJestClient();
            String[] split = esIndexs.split(&quot;,&quot;);
            for (int i = 0; i &lt; split.length; i++) {
                String index = split[i];
                Long count = JestService.count(jestClient, index, index);
                countMap.put(index,count);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }finally {
            JestService.closeJestClient(jestClient);
        }
        return countMap;
    }

    public Map&lt;String,Long&gt; aggregation(String indexName,String typeName,String field){

        JestClient jestClient = null;
        Map&lt;String, Long&gt; stringLongMap = null;
        try {
            jestClient = JestService.getJestClient();
            SearchResult aggregation = JestService.aggregation(jestClient, indexName, typeName, field);
            stringLongMap = ResultParse.parseAggregation(aggregation);
        } catch (Exception e) {
            e.printStackTrace();
        }finally {
            JestService.closeJestClient(jestClient);
        }
        return stringLongMap;
    }
}</code></pre><p><strong>这里用到了ES的大数据基础服务</strong></p>
<p><strong>轨迹查询</strong></p>
<p>用到了 <strong>HBase</strong> 的服务，使用 <strong>Fegin</strong><br><strong>SpringCloud Feign</strong></p>
<p><strong>Feign</strong> 是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用 <strong>Feign</strong> ，只需要创建一个接口并用注解的方式来配置它，即可完成对服务提供方的接口绑定服务调用客户端的开发量。</p>
<p>构建 <strong>com.hsiehchou.springcloud.es.fegin</strong></p>
<p>创建 <strong>HbaseFeign</strong></p>
<pre><code>package com.hsiehchou.springcloud.es.feign;

import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.ResponseBody;

import java.util.Set;

@FeignClient(name = &quot;xz-bigdata-springcloud-hbasequery&quot;)
public interface HbaseFeign {

    @ResponseBody
    @RequestMapping(value=&quot;/hbase/search1&quot;, method=RequestMethod.GET)
    public Set&lt;String&gt; search1(@RequestParam(name = &quot;table&quot;) String table,
                               @RequestParam(name = &quot;rowkey&quot;) String rowkey);
}</code></pre><h4 id="8、微服务手动部署"><a href="#8、微服务手动部署" class="headerlink" title="8、微服务手动部署"></a>8、微服务手动部署</h4><p><strong>Maven添加打包插件</strong></p>
<pre><code> &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;&lt;!--打包依赖的jar包--&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;
                    &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt; &lt;!-- 表示是否不包含间接依赖的包 --&gt;
                    &lt;stripVersion&gt;false&lt;/stripVersion&gt; &lt;!-- 去除版本信息 --&gt;
                &lt;/configuration&gt;

                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;!-- 拷贝项目依赖包到lib/目录下 --&gt;
                            &lt;outputDirectory&gt;${project.build.directory}/jars&lt;/outputDirectory&gt;
                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;
                            &lt;stripVersion&gt;false&lt;/stripVersion&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;


            &lt;!-- 打成jar包插件 --&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.4&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;!--
                        生成的jar中，不要包含pom.xml和pom.properties这两个文件
                    --&gt;
                        &lt;addMavenDescriptor&gt;false&lt;/addMavenDescriptor&gt;
                        &lt;!-- 生成MANIFEST.MF的设置 --&gt;
                        &lt;manifest&gt;
                            &lt;!-- 为依赖包添加路径, 这些路径会写在MANIFEST文件的Class-Path下 --&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;classpathPrefix&gt;jars/&lt;/classpathPrefix&gt;
                            &lt;!-- jar启动入口类--&gt;
                            &lt;mainClass&gt;com.cn.hbase.mr.HbaseMr&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                        &lt;!--       &lt;manifestEntries&gt;
                                   &amp;lt;!&amp;ndash; 在Class-Path下添加配置文件的路径 &amp;ndash;&amp;gt;
                                   &lt;Class-Path&gt;&lt;/Class-Path&gt;
                               &lt;/manifestEntries&gt;--&gt;
                    &lt;/archive&gt;
                    &lt;outputDirectory&gt;${project.build.directory}/&lt;/outputDirectory&gt;
                    &lt;includes&gt;
                        &lt;!-- 打jar包时，只打包class文件 --&gt;
                        &lt;include&gt;**/*.class&lt;/include&gt;
                        &lt;include&gt;**/*.properties&lt;/include&gt;
                        &lt;include&gt;**/*.yml&lt;/include&gt;
                    &lt;/includes&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;</code></pre><p>因为微服务<strong>依赖 xz_bigdata2</strong> 所以<strong>先打包xz_bigdata2</strong> </p>
<p><strong>修改配置文件</strong></p>
<pre><code>defaultZone: http://root:root@hadoop3:8761/eureka/</code></pre><p>将注册中心 IP 改为部署服务器的IP<br>微服务同理</p>
<p>上面给出的配置文件已经修改好了</p>
<p><strong>部署</strong></p>
<ol>
<li><strong>先部署Erueka服务中心</strong><br>新建<strong>/usr/chl/springcloud/eureka</strong></li>
</ol>
<p><img src="/medias/%E9%83%A8%E7%BD%B2%E5%9C%B0%E6%96%B9.PNG" alt="部署地方"></p>
<p>上传jars 和jar</p>
<p><img src="/medias/eureka.PNG" alt="eureka"></p>
<ol start="2">
<li><strong>启动服务中心</strong><br>eureka服务注册中心启动</li>
</ol>
<pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p>查看日志</p>
<pre><code>tail -f nohup.out</code></pre><ol start="3">
<li><strong>部署esquery</strong><br>esquery微服务启动</li>
</ol>
<pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><ol start="4">
<li><strong>部署hbasequery</strong><br>hbasequery微服务启动</li>
</ol>
<pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre><h4 id="9、执行-1"><a href="#9、执行-1" class="headerlink" title="9、执行"></a>9、执行</h4><ol>
<li><p>hadoop3:8002/hbase/getRelation?field=phone&amp;fieldValue=18609765012<br><img src="/medias/10.PNG" alt="1"></p>
</li>
<li><p>hadoop3:8002/hbase/search1?table=phone&amp;rowkey=18609765012<br><img src="/medias/20.PNG" alt="2"></p>
</li>
<li><p>hadoop3:8002/hbase/getHbase?table=send_mail&amp;rowkey=65497873@qq.com<br><img src="/medias/30.PNG" alt="3"></p>
</li>
<li><p>hadoop3:8002/hbase/getHbase?table=phone&amp;rowkey=18609765012<br><img src="/medias/40.PNG" alt="4"></p>
</li>
<li><p>hadoop3:8002/hbase/search/phone/18609765012<br><img src="/medias/5.PNG" alt="5"></p>
</li>
<li><p>hadoop3:8003/es/getAllCount<br><img src="/medias/6.PNG" alt="6"></p>
</li>
<li><p>hadoop3:8003/es/getBaseInfo<br><img src="/medias/7.PNG" alt="7"></p>
</li>
<li><p>hadoop3:8003/es/getLocus<br><img src="/medias/8.PNG" alt="8"></p>
</li>
<li><p>hadoop3:8003/es/group<br><img src="/medias/9.PNG" alt="9"></p>
</li>
</ol>
<h3 id="十三、附录"><a href="#十三、附录" class="headerlink" title="十三、附录"></a>十三、附录</h3><h4 id="1、测试数据"><a href="#1、测试数据" class="headerlink" title="1、测试数据"></a>1、测试数据</h4><p><strong>mail_source1_1111101.txt</strong></p>
<pre><code>000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    11111111@qq.com    1789097863    今天出去打球吗    send
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    22222222@qq.com    1789097864    今天出去打球吗    send
000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300088    65497873@qq.com    1789090763    33333333@qq.com    1789097863    今天出去打球吗    send
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300085    65497873@qq.com    1789090764    44444444@qq.com    1789097864    今天出去打球吗    send
000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send
000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send
000000000000000    000000000000000    23.000001    24.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    1323243@qq.com    1789098763    43432543@qq.com    1789098863    今天出去打球吗    send
000000000000000    000000000000000    24.000001    25.000001    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    1323243@qq.com    1789098764    43432543@qq.com    1789098864    今天出去打球吗    send</code></pre><p><strong>qq_source1_1111101.txt</strong></p>
<pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762
000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762
000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343
000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263
000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653
000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343
000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542
000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263
000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653
000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343
000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542
000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><p><strong>wechat_source1_1111101.txt</strong></p>
<pre><code>000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762
000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
000000000000000    000000000000000    23.000000    24.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305988    andiy    18609765432    judy            1789098762
000000000000000    000000000000000    24.000000    25.000000    aa-aa-aa-aa-aa-aa    bb-bb-bb-bb-bb-bb    32109231    1557305985    andiy    18609765432    judy            1789098763
000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300388    xz    18609765012    ls            1789000653
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300545    xz    18609765012    ls            1789000343
000000000000011    000000000000011    23.000011    24.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300658    xz    18609765012    ls            1789000542
000000000000011    000000000000011    24.000011    25.000011    1c-41-cd-b1-df-3f    1b-3d-zg-fg-ef-1b    32109246    1557300835    xz    18609765012    ls            1789000263
000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557300388    xz    18609765016    ls            1789001653
000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557302235    xz    18609765016    ls            1789001343
000000000000011    000000000000011    23.000021    24.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303658    xz    18609765016    ls            1789001542
000000000000011    000000000000011    24.000021    25.000031    1c-31-5d-b1-6f-3f    3y-5g-g6-du-bv-2f    32109246    1557303835    xz    18609765016    ls            1789001263
000000000000011    000000000000011    23.000031    24.000041    4c-6f-c7-3d-a4-3d    9g-gd-3h-3k-ld-3f    32109246    1557300001    xz    18609765014    ls            1789050653
000000000000011    000000000000011    24.000031    25.000051    7c-8e-d4-a6-3d-5c    54-hg-gi-yx-ef-ge    32109246    1557300005    xz    18609765015    ls            1789070343
000000000000011    000000000000011    23.000031    24.000061    8c-g1-ed-7b-5f-1b    47-fy-vv-hs-ue-fd    32109246    1557300008    xz    18609765017    ls            1789080542
000000000000011    000000000000011    24.000031    25.000071    0c-76-2a-b1-3c-1a    f5-nw-hf-ud-ht-ea    32109246    1557300115    xz    18609765010    ls            1789082263</code></pre><h4 id="2、Kafka"><a href="#2、Kafka" class="headerlink" title="2、Kafka"></a>2、Kafka</h4><p>创建topic，1个副本3个分区<br>kafka-topics –zookeeper hadoop1:2181 –topic chl_test7 –create –replication-factor 1 –partitions 3</p>
<p><strong>删除topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –delete –topic chl_test7</p>
<p><strong>列出所有的topic</strong><br>kafka-topics –zookeeper hadoop1:2181 –list</p>
<p><strong>消费</strong><br>kafka-console-consumer –bootstrap-server hadoop1:9092 –topic chl_test7 –from-beginning</p>
<h4 id="3、kafka2es"><a href="#3、kafka2es" class="headerlink" title="3、kafka2es"></a>3、kafka2es</h4><p><strong>启动sparkstreaming任务</strong></p>
<pre><code>spark-submit --master yarn-cluster --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar chl_test7 chl_test7</code></pre><pre><code>spark-submit 
--master yarn-cluster    //集群启动
--num-executors 1        //分配多少个进程
--driver-memory 500m  //driver内存
--executor-memory 1g //进程内存
--executor-cores 1       //开多少个核，线程
--jars $(echo /usr/chl/spark8/jars/*.jar | tr &#39; &#39; &#39;,&#39;) //加载jar
--class com.hsiehchou.spark.streaming.kafka.kafka2es.Kafka2esStreaming //执行类 /usr/chl/spark8/xz_bigdata_spark-1.0-SNAPSHOT.jar //包的位置</code></pre><h4 id="4、Yarn"><a href="#4、Yarn" class="headerlink" title="4、Yarn"></a>4、Yarn</h4><p><strong>将yarn的执行日志输出</strong><br>yarn logs -applicationId application_1561627166793_0002 &gt; log.log</p>
<p><strong>查看日志</strong><br>more log.log</p>
<p>cat log.log</p>
<h4 id="5、CDH的7180打不开"><a href="#5、CDH的7180打不开" class="headerlink" title="5、CDH的7180打不开"></a>5、CDH的7180打不开</h4><p><strong>查看cloudera-scm-server状态</strong><br>service cloudera-scm-server status</p>
<p><strong>查看cloudera-scm-server 日志</strong><br>cat /var/log/cloudera-scm-server/cloudera-scm-server.log</p>
<p><strong>重启cloudera-scm-server</strong><br>service cloudera-scm-server restart</p>
<h4 id="6、CDH的jdk设置—重要"><a href="#6、CDH的jdk设置—重要" class="headerlink" title="6、CDH的jdk设置—重要"></a>6、CDH的jdk设置—重要</h4><p><strong>/usr/local/jdk1.8</strong></p>
<h4 id="7、预警"><a href="#7、预警" class="headerlink" title="7、预警"></a>7、预警</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.warn.WarningStreamingTask /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><h4 id="8、Kibana的DEV-Tools"><a href="#8、Kibana的DEV-Tools" class="headerlink" title="8、Kibana的DEV Tools"></a>8、Kibana的DEV Tools</h4><pre><code>GET _search
{
  &quot;query&quot;: {
    &quot;match_all&quot;: {}
  }
}

GET  _cat/indices

DELETE tanslator_test1111

DELETE qq
DELETE wechat
DELETE mail

GET wechat

GET mail

GET _search

GET mail/_search

GET mail/_mapping

PUT mail

PUT mail/mail/_mapping
{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;send_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;accept_mail&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;mail_content&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;mail_type&quot;:{&quot;type&quot;: &quot;keyword&quot;},
     &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}

GET qq/_search

GET qq/_mapping

PUT qq

PUT qq/qq/_mapping
{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}

GET wechat/_search

GET wechat/_mapping

PUT wechat

PUT wechat/wechat/_mapping
{
  &quot;_source&quot;: {
    &quot;enabled&quot;: true
  },
  &quot;properties&quot;: {
    &quot;imei&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;imsi&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;longitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;latitude&quot;:{&quot;type&quot;: &quot;double&quot;},
    &quot;phone_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_mac&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;device_number&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;collect_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;phone&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;object_username&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;send_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;accept_message&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;message_time&quot;:{&quot;type&quot;: &quot;long&quot;},
    &quot;id&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;table&quot;:{&quot;type&quot;: &quot;keyword&quot;},
    &quot;filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}},
    &quot;absolute_filename&quot;:{&quot;type&quot;: &quot;text&quot;,&quot;fields&quot;: {&quot;keyword&quot;: {&quot;ignore_above&quot;: 256,&quot;type&quot;: &quot;keyword&quot;}}}
  }
}</code></pre><h4 id="9、Hive"><a href="#9、Hive" class="headerlink" title="9、Hive"></a>9、Hive</h4><p><strong>kafka写入hive</strong></p>
<pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.Kafka2HiveTest /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>show tables;

hdfs dfs -ls /apps/hive/warehouse/external

hdfs dfs -rm -r /apps/hive/warehouse/external/mail

drop table mail;

desc qq;

select * from qq limit 1;

注意了：cdh的hive版本跟其对应的spark版本不一致的话此处执行不了
select count(*) from qq;
</code></pre><p><strong>合并小文件</strong></p>
<pre><code>crontab -e

0 1 * * * spark-submit --master local[1] --num-executors 1 --driver-memory 300m --executor-memory 500m --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hdfs.CombineHdfs /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><p><img src="/medias/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1crontab.PNG" alt="定时任务crontab"></p>
<h4 id="10、Zookeeper"><a href="#10、Zookeeper" class="headerlink" title="10、Zookeeper"></a>10、Zookeeper</h4><p><strong>启动zookeeper客户端</strong><br>zookeeper-client</p>
<p><strong>清除消费者</strong><br>rmr /consumers/WarningStreamingTask2/offsets</p>
<p>rmr /consumers/Kafka2HiveTest/offsets</p>
<p>rmr /consumers/DataRelationStreaming1/offsets</p>
<h4 id="11、Hbase"><a href="#11、Hbase" class="headerlink" title="11、Hbase"></a>11、Hbase</h4><pre><code>spark-submit --master local[1] --num-executors 1 --driver-memory 500m --executor-memory 1g --executor-cores 1 --jars $(echo /usr/chl/spark7/jars/*.jar | tr &#39; &#39; &#39;,&#39;) --class com.hsiehchou.spark.streaming.kafka.kafka2hbase.DataRelationStreaming /usr/chl/spark7/xz_bigdata_spark-1.0-SNAPSHOT.jar</code></pre><pre><code>hbase shell

list

create &#39;t1&#39;,&#39;cf&#39;

desc &#39;t1&#39;

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;66666666&#39;

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:weixin&#39;,&#39;weixin1&#39;

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:mail&#39;,&#39;66666@qq.com&#39;

scan &#39;t1&#39;

将表变成多版本
alter &#39;t1&#39;,{NAME=&gt;&#39;cf&#39;,VERSIONS=&gt;50}

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;77777777&#39;

get &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;55555555&#39;

put &#39;t1&#39;,&#39;aa-aa-aa-aa-aa-aa&#39;,&#39;cf:qq&#39;,&#39;88888888&#39;,1290300544

执行DataRelationStreaming
scan &#39;test:relation&#39;

get &#39;test:username&#39;,&#39;andiy&#39;

scan &#39;test:relation&#39;

mail 
改mac 邮箱

get  &#39;test:relation&#39;,&#39;&#39;,{COLUMN=&gt;&#39;cf&#39;,VERSIONS=&gt;10}

disable &#39;test:imei&#39;
drop &#39;test:imei&#39;

disable &#39;test:imsi&#39;
drop &#39;test:imsi&#39;

disable &#39;test:phone&#39;
drop &#39;test:phone&#39;

disable &#39;test:phone_mac&#39;
drop &#39;test:phone_mac&#39;

disable &#39;test:relation&#39;
drop &#39;test:relation&#39;

disable &#39;test:send_mail&#39;
drop &#39;test:send_mail&#39;

disable &#39;test:username&#39;
drop &#39;test:username&#39;</code></pre><h4 id="12、SpringCloud"><a href="#12、SpringCloud" class="headerlink" title="12、SpringCloud"></a>12、SpringCloud</h4><p><strong>eureka服务注册中心启动</strong></p>
<pre><code>nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.eureka.EurekaApplication &amp;</code></pre><p><strong>查看日志</strong></p>
<pre><code>tail -f nohup.out</code></pre><p><strong>esquery微服务启动</strong></p>
<pre><code>nohup java -cp /usr/chl/springcloud/esquery/xz_bigdata_springcloud_esquery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.es.ESqueryApplication &amp;</code></pre><p><strong>hbasequery微服务启动</strong></p>
<pre><code>nohup java -cp /usr/chl/springcloud/hbasequery/xz_bigdata_springcloud_hbasequery-1.0-SNAPSHOT.jar com.hsiehchou.springcloud.HbaseQueryApplication &amp;</code></pre>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://blog.hsiehchou.com" rel="external nofollow noreferrer">谢舟</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://blog.hsiehchou.com/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/">https://blog.hsiehchou.com/2019/07/27/qi-ye-wang-luo-ri-zhi-fen-xi/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="https://blog.hsiehchou.com" target="_blank">谢舟</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/">
                                    <span class="chip bg-color">大数据项目</span>
                                </a>
                            
                                <a href="/tags/%E7%BD%91%E7%BB%9C%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/">
                                    <span class="chip bg-color">网络日志分析</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/share/css/share.min.css">

<div id="article-share">
    
    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            
        </div>
    </div>

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2019/07/27/idea-zhong-jar-chong-tu-cha-zhao-kuai-jie-jian-kuai-su-ding-wei/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/12.jpg" class="responsive-img" alt="IDEA中jar冲突查找快捷键快速定位">
                        
                        <span class="card-title">IDEA中jar冲突查找快捷键快速定位</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Ctrl+Alt+Shift+N

                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2019-07-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%B7%A5%E5%85%B7/" class="post-category">
                                    工具
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/IDEA/">
                        <span class="chip bg-color">IDEA</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/07/25/eureka-fu-wu-zhu-ce-zhong-xin-qi-dong/">
                    <div class="card-image">
                        
                        
                        <img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/featureimages/0.jpg" class="responsive-img" alt="Eureka服务注册中心启动">
                        
                        <span class="card-title">Eureka服务注册中心启动</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            nohup启动（nohup不间断运行）
nohup java -cp /usr/chl/springcloud/eureka/xz_bigdata_springcloud_eureka-1.0-SNAPSHOT.jar com.hsiehc
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2019-07-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/SpringCloud/" class="post-category">
                                    SpringCloud
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/SpringCloud/">
                        <span class="chip bg-color">SpringCloud</span>
                    </a>
                    
                    <a href="/tags/Eureka/">
                        <span class="chip bg-color">Eureka</span>
                    </a>
                    
                    <a href="/tags/nohup/">
                        <span class="chip bg-color">nohup</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('60')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 谢舟的博客<br />'
            + '文章作者: 谢舟<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4, h5'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4, h5').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>



    <footer class="page-footer bg-color">
    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019-2020</span>
            <a href="https://blog.hsiehchou.com" target="_blank">谢舟</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">492.5k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            
			<br>
            
            <span id="icp"><img src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/medias/icp.png" style="vertical-align: text-bottom;" />
                <a href="http://beian.miit.gov.cn/" target="_blank">苏ICP备17042062号 苏公网安备 32062102000231号</a>
            </span>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">


    <a href="mailto:babbyxie@foxmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=417952939" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 417952939" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/materialize/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/aos/aos.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/js/matery.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>


    <!-- Baidu Analytics -->

<script>
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?1fdd6e11866c1fe7b815d69a4a4206ea";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    

    
    
    
    <script src="https://cdn.jsdelivr.net/gh/hsiehchou/hsiehchou.github.io/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
